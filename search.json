[
  {
    "objectID": "readings/r-basics.html",
    "href": "readings/r-basics.html",
    "title": "R Basics",
    "section": "",
    "text": "For example, the data used in our textbook problems is included in the {resampledata3} R package. It is already installed on maize, but if you’re using a local version of R, you’ll need to install it by going to tools –&gt; install packages and then typing the name of the package in.\n\nlibrary(resampledata3)\n\n\nAttaching package: 'resampledata3'\n\n\nThe following object is masked from 'package:datasets':\n\n    Titanic\n\n\nIn this course, we’ll use a lot of tools found in the tidyverse of R packages. To load many of these packages at once, you can use the library(&lt;package_name&gt;) command. So to load the tidyverse we run:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we see a lot of extra info printed when we load the tidyverse. These messages are just telling you what packages are now available to you and warning you that a few functions (e.g., filter) has been replaced by the tidyverse version. We’ll see how to suppress these messages later."
  },
  {
    "objectID": "readings/r-basics.html#loading-r-packages",
    "href": "readings/r-basics.html#loading-r-packages",
    "title": "R Basics",
    "section": "",
    "text": "For example, the data used in our textbook problems is included in the {resampledata3} R package. It is already installed on maize, but if you’re using a local version of R, you’ll need to install it by going to tools –&gt; install packages and then typing the name of the package in.\n\nlibrary(resampledata3)\n\n\nAttaching package: 'resampledata3'\n\n\nThe following object is masked from 'package:datasets':\n\n    Titanic\n\n\nIn this course, we’ll use a lot of tools found in the tidyverse of R packages. To load many of these packages at once, you can use the library(&lt;package_name&gt;) command. So to load the tidyverse we run:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we see a lot of extra info printed when we load the tidyverse. These messages are just telling you what packages are now available to you and warning you that a few functions (e.g., filter) has been replaced by the tidyverse version. We’ll see how to suppress these messages later."
  },
  {
    "objectID": "readings/r-basics.html#creating-and-naming-objects",
    "href": "readings/r-basics.html#creating-and-naming-objects",
    "title": "R Basics",
    "section": "Creating and naming objects",
    "text": "Creating and naming objects\nAll R statements where you create objects have the form:\n\nobject_name &lt;- value\n\n\n\n\n\n\n\nNote\n\n\n\nCan’t I use the equal operator (=) to assign objects? Of course you can! The assignment operator (&lt;-) is unique to R and I try to use it when assigning objects. If the = sign comes more naturally to you, that’s also just fine\n\n\nAt first, we’ll be creating a lot of data objects. For example, we an load a data set containing the ratings for each episode of The Office using the code\n\noffice_ratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv\")\n\nIn this class you will be creating a lot of objects, so you’ll need to come up with names for those objects. Trying to think of informative/meaningful names for objects is hard, but necessary work! Below are the fundamental rules for naming objects in R:\n\nnames can’t start with a number\nnames are case-sensitive\nsome common letters are used internally by R and should be avoided as variable names (c, q, t, C, D, F, T, I)\nThere are reserved words that R won’t let you use for variable names (for, in, while, if, else, repeat, break, next)\nR will let you use the name of a predefined function—but don’t do it!\n\nYou can always check to see if you the name you want to use is already taken via exists():\nFor example lm exists\n\nexists(\"lm\")\n\n[1] TRUE\n\n\nbut carleton_college doesn’t.\n\nexists(\"carleton_college\")\n\n[1] FALSE\n\n\nThere are also a lot of naming styles out there, and if you have coded in another language, you may have already developed a preference. Below is an illustration by Allison Horst\n\n\n\n\n\n\n\n\n\nI generally following the tidyverse style guide, so you’ll see that I use only lowercase letters, numbers, and _ (snake case)."
  },
  {
    "objectID": "readings/r-basics.html#overviews-of-data-frames",
    "href": "readings/r-basics.html#overviews-of-data-frames",
    "title": "R Basics",
    "section": "Overviews of data frames",
    "text": "Overviews of data frames\nAbove, you loaded in a data set called office_ratings. Data sets are stored as a special data structure called a data frame. Data frames are the most-commonly used data structure for data analysis in R. For now, think of them like spreadsheets.\nOnce you have your data frame, you can get a quick overview of it using a few commands (below I use data_set as a generic placeholder for the data frame’s name):\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nhead(data_set)\nprint the first 6 rows\n\n\ntail(data_set)\nprint the last 6 rows\n\n\nglimpse(data_set)\na quick overview where columns run down the screen and the data values run across. This allows you to see every column in the data frame.\n\n\nstr(data_set)\na quick overview like glimpse(), but without some of the formatting\n\n\nsummary(data_set)\nquick summary statistics for each column\n\n\ndim(data_set)\nthe number of rows and columns\n\n\nnrow(data_set)\nthe number of rows\n\n\nncol(data_set)\nthe number of columns\n\n\n\n## Tibbles\nA tibble, or a tbl_df is another version of a data frame which is used by default in a lot of the tidyverse packages that we’ll use.\n\nTibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\n\n\n\n\n\n\n\n Check point\n\n\n\nRun the above commands on the office_ratings data set. Compare and contrast the information returned by each command.\n\n\n\n\n\n\n\n\nGetting a spreadsheet\n\n\n\nIn RStudio, you can run the command View(data_set) to pull up a spreadsheet representation of a data frame. You can also click on the name of the data frame in the Environment pane. This can be a great way help you think about the data, and even has some interactive functions (e.g., filtering and searching); however, never include View(data_set) in an .Rmd file!!\n\n\n\n\n\n\n\n\nReview from intro stats\n\n\n\nIn intro stats we used the terms cases (or observations) and variables to describe the rows and columns of a data frame, respectively."
  },
  {
    "objectID": "readings/r-basics.html#extracting-pieces-of-data-frames",
    "href": "readings/r-basics.html#extracting-pieces-of-data-frames",
    "title": "R Basics",
    "section": "Extracting pieces of data frames",
    "text": "Extracting pieces of data frames\nSince data frames are the fundamental data structure for most analyses in R, it’s important to know how to work with them. You already know how to get an overview of a data frame, but that isn’t always very informative. Often, you want to extract pieces of a data frame, such as a specific column or row.\n\nExtracting rows\nData frames can be indexed by their row/column numbers. To extract elements of a data frame, the basic syntax is data_set[row.index, column.index]. So, to extract the 10th row of office_ratings we run\n\noffice_ratings[10, ]\n\n# A tibble: 1 × 6\n  season episode title    imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      2       4 The Fire         8.4        2713 2005-10-11\n\n\nNotice that to extract an entire row, we leave the column index position blank.\nWe can also extract multiple rows by creating a vector of row indices. For example, we can extract the first 5 rows via\n\noffice_ratings[1:5, ]\n\n# A tibble: 5 × 6\n  season episode title         imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      1       1 Pilot                 7.6        3706 2005-03-24\n2      1       2 Diversity Day         8.3        3566 2005-03-29\n3      1       3 Health Care           7.9        2983 2005-04-05\n4      1       4 The Alliance          8.1        2886 2005-04-12\n5      1       5 Basketball            8.4        3179 2005-04-19\n\n\nHere, 1:5 create a sequence of integers from 1 to 5.\nWe could also specify arbitrary row index values by combing the values into a vector. For example, we could extract the 1st, 13th, 64th, and 128th rows via\n\noffice_ratings[c(1, 13, 64, 128), ]\n\n# A tibble: 4 × 6\n  season episode title            imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      1       1 Pilot                    7.6        3706 2005-03-24\n2      2       7 The Client               8.6        2631 2005-11-08\n3      4      13 Job Fair                 7.9        1977 2008-05-08\n4      7      11 Classy Christmas         8.9        2138 2010-12-09\n\n\n\n\nExtracting columns\nSimilar to extracting rows, we can use a numeric index to extract the columns of a data frame. For example, to extract the 3rd column, we can run\n\noffice_ratings[,3]\n\n# A tibble: 188 × 1\n   title            \n   &lt;chr&gt;            \n 1 Pilot            \n 2 Diversity Day    \n 3 Health Care      \n 4 The Alliance     \n 5 Basketball       \n 6 Hot Girl         \n 7 The Dundies      \n 8 Sexual Harassment\n 9 Office Olympics  \n10 The Fire         \n# ℹ 178 more rows\n\n\nAlternatively, we can pass in the column name in quotes instead of the column number\n\noffice_ratings[,\"title\"]\n\n# A tibble: 188 × 1\n   title            \n   &lt;chr&gt;            \n 1 Pilot            \n 2 Diversity Day    \n 3 Health Care      \n 4 The Alliance     \n 5 Basketball       \n 6 Hot Girl         \n 7 The Dundies      \n 8 Sexual Harassment\n 9 Office Olympics  \n10 The Fire         \n# ℹ 178 more rows\n\n\nNotice that the extracted column is still formatted as a data frame (or tibble). If you want to extract the contents of the column and just have a vector of titles, you have a few options.\n\nYou could use double brackets with the column number:\n\n\noffice_ratings[[3]]\n\n\nYou could use double brackets with the column name in quotes:\n\n\noffice_ratings[[\"title\"]]\n\n\nYou could use the $ extractor with the column name (not in quotes):\n\n\noffice_ratings$title\n\n\n\n\n\n\n\n Check point\n\n\n\n\nExtract the 35th row of office_ratings.\nExtract rows 35, 36, 37, and 38 of office_ratings.\nExtract the imdb_rating column from office ratings using the column index number.\nExtract the imdb_rating column from office ratings using the column name."
  },
  {
    "objectID": "readings/r-basics.html#lists-very-optional-for-stat250",
    "href": "readings/r-basics.html#lists-very-optional-for-stat250",
    "title": "R Basics",
    "section": "Lists very optional for Stat250",
    "text": "Lists very optional for Stat250\nIt turns out that data frames are special cases of lists, a more general data structure. In a data frame, each column is an element of the data list and each column must be of the same length. In general, lists can be comprised of elements of vastly different lengths and data types.\nAs an example, let’s construct a list of the faculty in the MAST department and what is being taught this winter.\n\nstat_faculty &lt;- c(\"Kelling\", \"Loy\", \"Luby\", \"Poppick\", \"St. Clair\", \"Wadsworth\")\nstat_courses &lt;- c(120, 220, 230, 250, 285, 330)\nmath_faculty &lt;- c(\"Brooke\", \"Davis\", \"Egge\", \"Gomez-Gonzales\", \"Haunsperger\", \"Johnson\", \n                  \"Meyer\", \"Montee\", \"Shrestha\",\"Terry\", \"Thompson\", \"Turnage-Butterbaugh\")\nmath_courses &lt;- c(101, 106, 111, 120, 210, 211, 232, 236, 240, 241, 251, 321, 333, 395)\n\nmast &lt;- list(stat_faculty = stat_faculty, stat_courses = stat_courses, \n             math_faculty = math_faculty, math_courses = math_courses)\n\n\nOverview of a list\nYou can get an overview of a list a few ways:\n\nglimpse(list_name) and str(list_name) list the elements of the list and the first few entries of each element.\n\n\nglimpse(mast)\n\nList of 4\n $ stat_faculty: chr [1:6] \"Kelling\" \"Loy\" \"Luby\" \"Poppick\" ...\n $ stat_courses: num [1:6] 120 220 230 250 285 330\n $ math_faculty: chr [1:12] \"Brooke\" \"Davis\" \"Egge\" \"Gomez-Gonzales\" ...\n $ math_courses: num [1:14] 101 106 111 120 210 211 232 236 240 241 ...\n\n\n\nlength(list_name) will tell you how many elements are in the list\n\n\nlength(mast)\n\n[1] 4\n\n\n\n\nExtracting elements of a list\nSince data frames are lists, you’ve already seen how to extract elements of a list. For example, to extract the stat_faculty you could run\n\nmast[[1]]\n\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\nor\n\nmast[[\"stat_faculty\"]]\n\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you had only used a single bracket above, the returned object would still be a list, which is typically not what we would want.\n\nmast[1]\n\n$stat_faculty\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\n\n\n\n\n\n\n\n\n Check point\n\n\n\nExtract the statistics courses offered this term."
  },
  {
    "objectID": "readings/r-basics.html#vectors",
    "href": "readings/r-basics.html#vectors",
    "title": "R Basics",
    "section": "Vectors",
    "text": "Vectors\nThe columns of the office_ratings data frame and the elements of the mast list were comprised of (atomic) vectors. Unlike lists, all elements within a vector share the same type. For example, all names in the stat_faculty vector were character strings and all ratings in the imdb_rating column were numeric. We’ll deal with a variety of types of vectors in this course, including:\n\nnumeric\ncharacter (text)\nlogical (TRUE/FALSE)\n\n\nExtracting elements of a vector\nJust like with lists (and therefore data frames), we use brackets to extract elements from a vector. As an example, let’s work with the title column from office_ratings.\n\ntitle &lt;- office_ratings$title # vector of titles\n\nTo extract the 111th title, we run\n\ntitle[111]\n\n[1] \"New Leads\"\n\n\nor two extract the 100th through 111th titles, we run\n\ntitle[100:111]\n\n [1] \"Double Date\"          \"Murder\"               \"Shareholder Meeting\" \n [4] \"Scott's Tots\"         \"Secret Santa\"         \"The Banker\"          \n [7] \"Sabre\"                \"Manager and Salesman\" \"The Delivery: Part 1\"\n[10] \"The Delivery: Part 2\" \"St. Patrick's Day\"    \"New Leads\"           \n\n\n\n\nNegative indices\nSometimes, we want to “kick out” elements of our vector. To do this, we can use a negative index value. For example,\n\ntitle[-1]\n\nreturns all but the first title—that is, it kicks out the first title. To kick out multiple elements, we need to negate a vector of indices. For example, below we kick out the first 10 titles\n\ntitle[-c(1:10)]\n\nAnd now we kick out the 5th, 50th, and 150th titles\n\ntitle[-c(5, 50, 150)]\n\nThis idea can be adapted to lists and data frames. For example, to kick out the first row of office_ratings, we run\n\noffice_ratings[-1,]\n\n# A tibble: 187 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       2 Diversity Day             8.3        3566 2005-03-29\n 2      1       3 Health Care               7.9        2983 2005-04-05\n 3      1       4 The Alliance              8.1        2886 2005-04-12\n 4      1       5 Basketball                8.4        3179 2005-04-19\n 5      1       6 Hot Girl                  7.8        2852 2005-04-26\n 6      2       1 The Dundies               8.7        3213 2005-09-20\n 7      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 8      2       3 Office Olympics           8.4        2742 2005-10-04\n 9      2       4 The Fire                  8.4        2713 2005-10-11\n10      2       5 Halloween                 8.2        2561 2005-10-18\n# ℹ 177 more rows\n\n\nor to kick out the math courses from the mast list we run\n\nmast[-4]\n\n$stat_faculty\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n$stat_courses\n[1] 120 220 230 250 285 330\n\n$math_faculty\n [1] \"Brooke\"              \"Davis\"               \"Egge\"               \n [4] \"Gomez-Gonzales\"      \"Haunsperger\"         \"Johnson\"            \n [7] \"Meyer\"               \"Montee\"              \"Shrestha\"           \n[10] \"Terry\"               \"Thompson\"            \"Turnage-Butterbaugh\"\n\n\n\n\nLogical indices\nIt’s great to be able to extract (or omit) elements using indices, but sometimes we don’t know what index value we should use. For example, if you wanted to extract all of the 300-level statistics courses from the stat_courses vector, you would need to manually determine that positions 2:5 meet that requirement. That’s a lot of work! A better alternative is to allow R to find the elements meeting that requirement using logical operators. Below is a table summarizing common logical operators in R.\n\n\n\nComparison\nMeaning\n\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nis equal to\n\n\n!=\nnot equal to\n\n\n\nIn order to extract the 300-level statistics courses, we’ll take two steps:\n\nWe’ll determine whether each course is numbered at least 300,\nthen we’ll use that sequence of TRUEs/FALSEs to extract the course.\n\nSo, first we use the logical operator &gt;= to compare stat_courses and 300. This returns TRUE if the element meets the specification and FALSE otherwise.\n\nstat_courses &gt;= 300\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nNow, we can use this vector as our index. Only the TRUE elements will be extracted:\n\nstat_courses[stat_courses &gt;= 300]\n\n[1] 330\n\n\nThe same idea can be used with data frames and lists, just remember how to format the brackets and indices!\n\n\n\n\n\n\n Check point\n\n\n\n\nExtract all statistics courses below 250 from stat_courses.\nExtract all math courses except for 240 (probability) from math_courses.\nExtract all rows from season 3 of The Office."
  },
  {
    "objectID": "notes/17-two-sample-ci/17-activity.html",
    "href": "notes/17-two-sample-ci/17-activity.html",
    "title": "17: Comparing two sample confidence interval procedures",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nRecall the coverage simulation results that we looked at for the 1-sample t procedure:\nYour task for today is to replicate the simulation study for the following settings:\nYou should also assess how the sample size impacts the coverage:"
  },
  {
    "objectID": "notes/17-two-sample-ci/17-activity.html#setup",
    "href": "notes/17-two-sample-ci/17-activity.html#setup",
    "title": "17: Comparing two sample confidence interval procedures",
    "section": "Setup",
    "text": "Setup\n\n# Sample sizes for x and y\nn1 &lt;- 10\nn2 &lt;- 10\n\nN_sim &lt;- 100    # keep this small-ish for efficiency. This is the number of times you will draw new samples\nN_boot &lt;- 1000  # keep this smaller than usual as well. This is the number of bootstrap simulations you will run for each pair of samples\n\n# Storage for results\nresults &lt;- data.frame( \n  t_lower = numeric(N_sim),\n  t_upper = numeric(N_sim), \n  boott_lower = numeric(N_sim), \n  boott_upper = numeric(N_sim), \n  boot_lower = numeric(N_sim),\n  boot_upper = numeric(N_sim)\n)"
  },
  {
    "objectID": "notes/17-two-sample-ci/17-activity.html#run-simulation",
    "href": "notes/17-two-sample-ci/17-activity.html#run-simulation",
    "title": "17: Comparing two sample confidence interval procedures",
    "section": "Run Simulation",
    "text": "Run Simulation\n\nset.seed(050825)\n\nfor(i in 1:N_sim){\n  # Step 1: draw random samples. This should be the only thing you change within this chunk between the simulations\n  x &lt;- rnorm(n1, mean = 0, sd = 1)\n  y &lt;- rnorm(n2, mean = 1.5, sd = 2)\n  \n  true_diff &lt;- -1.5 # this is the true difference in the population means (x - y)\n  \n  # Step 2: compute formula t confidence interval\n  t_ci &lt;- t.test(x, y, conf.level = .95)$conf\n  results$t_lower[i] &lt;- t_ci[1]\n  results$t_upper[i] &lt;- t_ci[2]\n  \n  # Step 3: run bootstrap for each sample; store bootstrap means and bootstrap t's\n  \n  diff_mean &lt;- numeric(N_boot) # storage for bootstrap differences\n  t_boot &lt;- numeric(N_boot) # storage for bootstrap differences\n  \n  for(j in 1:N_boot){\n    x_boot &lt;- sample(x, n1, replace = TRUE)\n    y_boot &lt;- sample(y, n2, replace = TRUE)\n    diff_mean[j] &lt;- mean(x_boot) - mean(y_boot)\n    t_boot[j] &lt;- ((mean(x_boot) - mean(y_boot)) - (mean(x) - mean(y)))/sqrt(var(x_boot)/n1 + var(y_boot)/n2)\n  }\n  \n  results$boot_lower[i] &lt;- quantile(diff_mean, prob = c(.025))\n  results$boot_upper[i] &lt;- quantile(diff_mean, prob = c(.975))\n  \n  results$boott_lower[i] &lt;- (mean(x)- mean(y)) - quantile(t_boot, .975)*sqrt(var(x)/n1 + var(y)/n2)\n  results$boott_upper[i] &lt;- (mean(x)- mean(y)) - quantile(t_boot, .025)*sqrt(var(x)/n1 + var(y)/n2)\n}"
  },
  {
    "objectID": "notes/17-two-sample-ci/17-activity.html#analyze-results-find-coverage-rates",
    "href": "notes/17-two-sample-ci/17-activity.html#analyze-results-find-coverage-rates",
    "title": "17: Comparing two sample confidence interval procedures",
    "section": "Analyze results: find coverage rates",
    "text": "Analyze results: find coverage rates\n\nmean(true_diff &lt; results$t_lower) # percent of times that t CI is too high\n\n[1] 0.01\n\nmean(true_diff &gt; results$t_upper) # percent of times that t CI is too low\n\n[1] 0.05\n\nmean(true_diff &lt; results$boot_lower) # percent of times that bootstrap CI is too high\n\n[1] 0.01\n\nmean(true_diff &gt; results$boot_upper) # percent of times that bootstrap CI is too low\n\n[1] 0.07\n\nmean(true_diff &lt; results$boott_lower) # percent of times that bootstrap T CI is too high\n\n[1] 0.01\n\nmean(true_diff &gt; results$boott_upper) # percent of times that bootstrap T CI is too low\n\n[1] 0.05"
  },
  {
    "objectID": "notes/17-two-sample-ci/17-activity.html#analyze-results-graph",
    "href": "notes/17-two-sample-ci/17-activity.html#analyze-results-graph",
    "title": "17: Comparing two sample confidence interval procedures",
    "section": "Analyze results: graph",
    "text": "Analyze results: graph\nNote: if you use &gt;100 samples, this plot will probably not look very nice!\n\np1 &lt;- ggplot(results, aes(xmin = t_lower, xmax = t_upper, y = 1:N_sim, col = (true_diff &lt; t_lower )| (true_diff &gt; t_upper))) +\n  geom_errorbarh() +\n  labs(title = \"t formula\", \n       col = \"Missed\", \n       y = \"Simulation\", \n       x = \"Diff in means\") + \n  scale_color_viridis_d(end = .75, option = \"plasma\")\n\np2 &lt;- ggplot(results, aes(xmin = boot_lower, xmax = boot_upper, y = 1:N_sim, col = (true_diff &lt; boot_lower )| (true_diff &gt; boot_upper))) +\n  geom_errorbarh() +\n  labs(title = \"Bootstrap\",\n       col = \"Missed\",\n       y = \"Simulation\", \n       x = \"Diff in means\") + \n  scale_color_viridis_d(end = .75, option = \"plasma\")\n\np3 &lt;- ggplot(results, aes(xmin = boott_lower, xmax = boott_upper, y = 1:N_sim, col = (true_diff &lt; boott_lower )| (true_diff &gt; boott_upper))) +\n  geom_errorbarh() +\n  labs(title = \"Bootstrap T\",\n       col = \"Missed\", \n       y = \"Simulation\", \n       x = \"Diff in means\") + \n  scale_color_viridis_d(end = .75, option = \"plasma\")\n\np1 + p2 + p3 + plot_layout(guides = \"collect\") & theme_minimal() & theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "notes/16-bootstrap-t/16-activity.html",
    "href": "notes/16-bootstrap-t/16-activity.html",
    "title": "16: Comparing confidence interval procedures",
    "section": "",
    "text": "The {nycflights23} R package contains a dataset called flights. This dataset contains all flights in and out of NYC-area airports in 2023. Since the dataset has all flights, we can treat it as a population. We’re interested in the average departure delay of flights departing from NYC area airports.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(nycflights23)\n\nflights &lt;- flights |&gt;\n  filter(origin %in% c(\"JFK\", \"EWR\", \"LGA\")) |&gt; # filter to departing flights from NYC\n  drop_na(dep_delay) # drop observations with missing dep_delay variables\n\nglimpse(flights)\n\nRows: 424,614\nColumns: 19\n$ year           &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 1, 18, 31, 33, 36, 503, 520, 524, 537, 547, 549, 551, 5…\n$ sched_dep_time &lt;int&gt; 2038, 2300, 2344, 2140, 2048, 500, 510, 530, 520, 545, …\n$ dep_delay      &lt;dbl&gt; 203, 78, 47, 173, 228, 3, 10, -6, 17, 2, -10, -9, -7, -…\n$ arr_time       &lt;int&gt; 328, 228, 500, 238, 223, 808, 948, 645, 926, 845, 905, …\n$ sched_arr_time &lt;int&gt; 3, 135, 426, 2352, 2252, 815, 949, 710, 818, 852, 901, …\n$ arr_delay      &lt;dbl&gt; 205, 53, 34, 166, 211, -7, -1, -25, 68, -7, 4, -13, -14…\n$ carrier        &lt;chr&gt; \"UA\", \"DL\", \"B6\", \"B6\", \"UA\", \"AA\", \"B6\", \"AA\", \"UA\", \"…\n$ flight         &lt;int&gt; 628, 393, 371, 1053, 219, 499, 996, 981, 206, 225, 800,…\n$ tailnum        &lt;chr&gt; \"N25201\", \"N830DN\", \"N807JB\", \"N265JB\", \"N17730\", \"N925…\n$ origin         &lt;chr&gt; \"EWR\", \"JFK\", \"JFK\", \"JFK\", \"EWR\", \"EWR\", \"JFK\", \"EWR\",…\n$ dest           &lt;chr&gt; \"SMF\", \"ATL\", \"BQN\", \"CHS\", \"DTW\", \"MIA\", \"BQN\", \"ORD\",…\n$ air_time       &lt;dbl&gt; 367, 108, 190, 108, 80, 154, 192, 119, 258, 157, 164, 1…\n$ distance       &lt;dbl&gt; 2500, 760, 1576, 636, 488, 1085, 1576, 719, 1400, 1065,…\n$ hour           &lt;dbl&gt; 20, 23, 23, 21, 20, 5, 5, 5, 5, 5, 5, 6, 5, 6, 6, 6, 6,…\n$ minute         &lt;dbl&gt; 38, 0, 44, 40, 48, 0, 10, 30, 20, 45, 59, 0, 59, 0, 0, …\n$ time_hour      &lt;dttm&gt; 2023-01-01 20:00:00, 2023-01-01 23:00:00, 2023-01-01 2…\n\n\n\n\n\n\n\n\nQ1: EDA\n\n\n\nMake a histogram of dep_delay. This represents our population. What do you notice about the distribution? What is the true \\(\\mu\\) in this case?\n\n\nLet’s explore the performance of our different confidence interval procedures. To do so, you’ll first draw a sample from the flights dataset. You’ll treat this as your data sample throughout the rest of the activity, and compare your confidence intervals to the true value that you found in Q1.\n\n\n\n\n\n\nQ2: Draw your sample\n\n\n\nUse the code below to draw a sample of size 40 from the flights dataset. Make sure to set a seed that you think is different from everyone else in the class! This is the “sample data” that you’ll work with for the rest of this activity.\n\n\n\nset.seed(1) # Replace \"1\" with a random seed\nsample_index &lt;- sample(1:nrow(flights), size = 40)\nflights_sample &lt;- flights[sample_index,]\ndep_delay &lt;- flights_sample$dep_delay\n\n\n\n\n\n\n\nQ3: Construct your confidence intervals\n\n\n\nUsing your sample dep_delay from above, compute three confidence intervals:\n\npercentile bootstrap\nCLT-based t\nbootstrap t-based\n\ntemplate code can be found in the handout for today\n\n\nwhen you’re done, enter your lower and upper bounds in this google sheet\n\n\n\n\n\n\nQ4: (stretch goal, if time) Run coverage simulation\n\n\n\nTry to set up a “simulation of simulations”. Repeat your confidence interval procedure 100 times, on 100 different samples from flights, storing the upper and lower bounds for each confidence interval procedure as you go."
  },
  {
    "objectID": "slides/22/slides22.html#is-there-such-a-thing-as-extrasensory-perception-esp",
    "href": "slides/22/slides22.html#is-there-such-a-thing-as-extrasensory-perception-esp",
    "title": "Multiple Testing",
    "section": "Is there such a thing as “Extrasensory Perception” (ESP)?",
    "text": "Is there such a thing as “Extrasensory Perception” (ESP)?\n\n\nIdea: subjects draw a card at random and telepathically communicate this to someone who then guesses the symbol\n\n\nLet’s say we run this experiment with 14 subjects and 3 get the answer right."
  },
  {
    "objectID": "slides/22/slides22.html#testing-setup",
    "href": "slides/22/slides22.html#testing-setup",
    "title": "Multiple Testing",
    "section": "Testing setup",
    "text": "Testing setup\n\nPopulation Parameter: \\(p\\), the proportion of correctly guessed cards in the population\nSample Statistic: \\(\\widehat{p}\\), the proportion of correctly guessed cards in our sample (3/14)\n\\(H_0: p = .2\\) – if there is no ESP, people choose cards according to random guessing\n\\(H_A: p &gt; .2\\)"
  },
  {
    "objectID": "slides/22/slides22.html#results",
    "href": "slides/22/slides22.html#results",
    "title": "Multiple Testing",
    "section": "Results",
    "text": "Results\n\nprop.test(3, 14, p = .2)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  3 out of 14\nX-squared = 1.5848e-31, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.2\n95 percent confidence interval:\n 0.06806839 0.49043470\nsample estimates:\n        p \n0.2142857 \n\n\n\nbinom.test(3, 14, p = .2)\n\n\n\n\ndata:  3 out of 14\nnumber of successes = 3, number of trials = 14, p-value = 1\nalternative hypothesis: true probability of success is not equal to 0.2\n95 percent confidence interval:\n 0.04657929 0.50797568\nsample estimates:\nprobability of success \n             0.2142857"
  },
  {
    "objectID": "slides/22/slides22.html#oops-i-meant-that-there-were-1400-subjects-and-300-got-the-answer-right",
    "href": "slides/22/slides22.html#oops-i-meant-that-there-were-1400-subjects-and-300-got-the-answer-right",
    "title": "Multiple Testing",
    "section": "Oops! I meant that there were 1400 subjects and 300 got the answer right",
    "text": "Oops! I meant that there were 1400 subjects and 300 got the answer right\n\n\nprop.test(300, 1400, p = .2)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  300 out of 1400\nX-squared = 1.6975, df = 1, p-value = 0.1926\nalternative hypothesis: true p is not equal to 0.2\n95 percent confidence interval:\n 0.1932458 0.2369153\nsample estimates:\n        p \n0.2142857"
  },
  {
    "objectID": "slides/22/slides22.html#oops-i-meant-that-there-were-14000-subjects-and-3000-got-the-answer-right",
    "href": "slides/22/slides22.html#oops-i-meant-that-there-were-14000-subjects-and-3000-got-the-answer-right",
    "title": "Multiple Testing",
    "section": "Oops! I meant that there were 14000 subjects and 3000 got the answer right",
    "text": "Oops! I meant that there were 14000 subjects and 3000 got the answer right\n\n\nprop.test(3000, 14000, p = .2)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  3000 out of 14000\nX-squared = 17.768, df = 1, p-value = 2.495e-05\nalternative hypothesis: true p is not equal to 0.2\n95 percent confidence interval:\n 0.2075323 0.2211967\nsample estimates:\n        p \n0.2142857"
  },
  {
    "objectID": "slides/22/slides22.html#sample-size-has-a-huge-impact-on-significance",
    "href": "slides/22/slides22.html#sample-size-has-a-huge-impact-on-significance",
    "title": "Multiple Testing",
    "section": "Sample Size has a huge impact on “significance”",
    "text": "Sample Size has a huge impact on “significance”\n\nWith small samples, even large effects might not be statistically significant.\n\n\nWith large samples, even very small effects can be statistically significant.\n\n\nThis is one reason why some people prefer “discernible” to “significant”"
  },
  {
    "objectID": "slides/22/slides22.html#section",
    "href": "slides/22/slides22.html#section",
    "title": "Multiple Testing",
    "section": "",
    "text": "Source: xkcd 882"
  },
  {
    "objectID": "slides/22/slides22.html#section-1",
    "href": "slides/22/slides22.html#section-1",
    "title": "Multiple Testing",
    "section": "",
    "text": "Source: xkcd 882"
  },
  {
    "objectID": "slides/22/slides22.html#section-2",
    "href": "slides/22/slides22.html#section-2",
    "title": "Multiple Testing",
    "section": "",
    "text": "Source: xkcd 882"
  },
  {
    "objectID": "slides/22/slides22.html#section-3",
    "href": "slides/22/slides22.html#section-3",
    "title": "Multiple Testing",
    "section": "",
    "text": "Source: xkcd 882"
  },
  {
    "objectID": "slides/22/slides22.html#multiple-testing-1",
    "href": "slides/22/slides22.html#multiple-testing-1",
    "title": "Multiple Testing",
    "section": "Multiple Testing",
    "text": "Multiple Testing\nWhenever you do a single hypothesis test, your Type I Error rate is \\(\\alpha\\). If we do 10 hypothesis tests, what is the probability we make at least one Type I Error?"
  },
  {
    "objectID": "slides/22/slides22.html#section-4",
    "href": "slides/22/slides22.html#section-4",
    "title": "Multiple Testing",
    "section": "",
    "text": "The probability of making at least Type I error among \\(k\\) tests is \\(1-(1-\\alpha)^k\\)\n\nIf you do 10 tests at the \\(\\alpha = .05\\) level, your overall Type I Error rate is over 40%\n\n\nIf you do 100 tests at the \\(\\alpha = .05\\) level, your overall Type I Error rate is over 99%!\n\n\nNeed to do a multiple testing correction in these cases. Plan the number of tests in advance and adjust your original \\(\\alpha\\) accordingly"
  },
  {
    "objectID": "slides/22/slides22.html#family-wise-error-rate",
    "href": "slides/22/slides22.html#family-wise-error-rate",
    "title": "Multiple Testing",
    "section": "Family-Wise Error Rate",
    "text": "Family-Wise Error Rate\n\n\n\n\n\n\nFamily-Wise Error Rate\n\n\n  \n\n\n\nIdea: if we have a desired FWER = \\(\\alpha^*\\), what \\(\\alpha\\) do we need for each hypothesis test?"
  },
  {
    "objectID": "slides/22/slides22.html#sidak-correction",
    "href": "slides/22/slides22.html#sidak-correction",
    "title": "Multiple Testing",
    "section": "Sidak Correction",
    "text": "Sidak Correction"
  },
  {
    "objectID": "slides/22/slides22.html#bonferroni-correction",
    "href": "slides/22/slides22.html#bonferroni-correction",
    "title": "Multiple Testing",
    "section": "Bonferroni Correction",
    "text": "Bonferroni Correction"
  },
  {
    "objectID": "slides/22/slides22.html#section-5",
    "href": "slides/22/slides22.html#section-5",
    "title": "Multiple Testing",
    "section": "",
    "text": "A 2017 paper in the Journal of Clinical Epidemiology looked at published p-values in 120 medical research articles published in top medical journals in 2016.\nTheoretically, the distribution of p-values should be uniform under \\(H_0\\):"
  },
  {
    "objectID": "slides/22/slides22.html#section-6",
    "href": "slides/22/slides22.html#section-6",
    "title": "Multiple Testing",
    "section": "",
    "text": "Under \\(H_A\\), the distribution of p-values should be monotonically increasing:"
  },
  {
    "objectID": "slides/22/slides22.html#section-7",
    "href": "slides/22/slides22.html#section-7",
    "title": "Multiple Testing",
    "section": "",
    "text": "Instead, the distribution looked like:"
  },
  {
    "objectID": "slides/22/slides22.html#six-principles-from-the-asa-statement",
    "href": "slides/22/slides22.html#six-principles-from-the-asa-statement",
    "title": "Multiple Testing",
    "section": "Six principles from the ASA statement",
    "text": "Six principles from the ASA statement\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis."
  },
  {
    "objectID": "slides/22/slides22.html#section-9",
    "href": "slides/22/slides22.html#section-9",
    "title": "Multiple Testing",
    "section": "",
    "text": "43 articles and over 400 pages of opinions and suggestions for ways to move forward"
  },
  {
    "objectID": "slides/13/slides13.html#roadmap",
    "href": "slides/13/slides13.html#roadmap",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Roadmap",
    "text": "Roadmap\n\n\nUnit 1: Estimation\n\nStatistics vs Parameters\nDeveloping an estimator\nEvaluating the estimator’s behavior:\n\nsampling distribution\nbias, variance, consistency\n\n\n\nUnit 2: Inference\n\nOnce we have an estimator, what does it say about the population parameter?"
  },
  {
    "objectID": "slides/13/slides13.html#roadmap-1",
    "href": "slides/13/slides13.html#roadmap-1",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Roadmap",
    "text": "Roadmap\nObserve \\(X_1, ..., X_n \\sim F(\\theta)\\) with \\(\\theta\\) unknown. Estimate \\(\\hat\\theta = g(X_i)\\).\n\nDo we expect \\(\\hat\\theta\\) to be exactly equal to \\(\\theta\\)?\n\n\nWhat are plausible values for \\(\\theta\\) given an observed \\(\\hat\\theta\\)?"
  },
  {
    "objectID": "slides/13/slides13.html#roadmap-2",
    "href": "slides/13/slides13.html#roadmap-2",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Roadmap",
    "text": "Roadmap\nWe want to develop an interval estimate of a population parameter\n\nExact method: Find the sampling distribution in closed form (Chapter 4). Requires knowledge of the distribution of the data!\nBootstrap method: Use the sample to approximate the population and simulate a sampling distribution (Chapter 5).\nAsymptotic method: Use large-sample theory to approximate the sampling distribution (e.g., appeal to CLT; Chapter 7)"
  },
  {
    "objectID": "slides/13/slides13.html#example",
    "href": "slides/13/slides13.html#example",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Example",
    "text": "Example\n\n\n\nDr. Kristen Gorman and the Palmer Station, Antarctica LTER, are studying the bill dimensions of a certain species of penguin\nThey want to estimate the average bill depth and bill length (in mm)\n\n\ndata(\"penguins\", package = \"palmerpenguins\")\ngentoo &lt;- filter(penguins, species == \"Gentoo\")\n\n\n\n\n\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package: https://allisonhorst.github.io/palmerpenguins/"
  },
  {
    "objectID": "slides/13/slides13.html#bill-length",
    "href": "slides/13/slides13.html#bill-length",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Bill length",
    "text": "Bill length\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n40.9\n45.3\n47.3\n49.55\n59.6\n47.5\n3.08\n123\n1"
  },
  {
    "objectID": "slides/13/slides13.html#how-can-we-use-our-one-sample-to-estimate-the-population-mean-bill-length-for-all-gentoo-penguins",
    "href": "slides/13/slides13.html#how-can-we-use-our-one-sample-to-estimate-the-population-mean-bill-length-for-all-gentoo-penguins",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "How can we use our one sample to estimate the population mean bill length for all Gentoo penguins?",
    "text": "How can we use our one sample to estimate the population mean bill length for all Gentoo penguins?\n\n(Theoretical) sampling distribution\nBootstrap distribution"
  },
  {
    "objectID": "slides/13/slides13.html#bootstrap-distribution",
    "href": "slides/13/slides13.html#bootstrap-distribution",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Bootstrap distribution",
    "text": "Bootstrap distribution"
  },
  {
    "objectID": "slides/13/slides13.html#bootstrap-percentile-interval",
    "href": "slides/13/slides13.html#bootstrap-percentile-interval",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Bootstrap percentile interval",
    "text": "Bootstrap percentile interval\nA 95% confidence interval can be constructed from the 2.5 and 97.5th percentiles of the bootstrap distribution"
  },
  {
    "objectID": "slides/13/slides13.html#the-one-sample-bootstrap-algorithm",
    "href": "slides/13/slides13.html#the-one-sample-bootstrap-algorithm",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "The one-sample bootstrap algorithm",
    "text": "The one-sample bootstrap algorithm\nGiven a sample of size n from a population,\n\nDraw a resample of size n, with replacement, from the sample.\nCompute the statistic of interest.\nRepeat this resampling process (steps 1-2) many times, say 10,000.\nConstruct the bootstrap distribution of the statistic."
  },
  {
    "objectID": "slides/13/slides13.html#your-turn",
    "href": "slides/13/slides13.html#your-turn",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Your turn",
    "text": "Your turn\nA sample consists of the following values: 8, 4, 11, 3, 7.\nWhich of the following are possible bootstrap samples from this sample? Why?\n\n8, 3, 7, 11\n4, 11, 4, 3, 3\n3, 4, 5, 7, 8\n7, 8, 8, 3, 4"
  },
  {
    "objectID": "slides/13/slides13.html#population",
    "href": "slides/13/slides13.html#population",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Population",
    "text": "Population\nConsider a Gamma(2, 2) population distribution\n\n\\[E(X) = 1 \\qquad SD(X) = 1/2\\]"
  },
  {
    "objectID": "slides/13/slides13.html#sample",
    "href": "slides/13/slides13.html#sample",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Sample",
    "text": "Sample\nSuppose we draw a random sample of size \\(n=50\\)\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\n\n\n\n\n0.061\n0.445\n0.757\n1.065\n5.088\n0.925\n0.824\n50.000"
  },
  {
    "objectID": "slides/13/slides13.html#bootstrap-distribution-1",
    "href": "slides/13/slides13.html#bootstrap-distribution-1",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Bootstrap distribution",
    "text": "Bootstrap distribution\nWe can bootstrap our sample and obtain the bootstrap distribution\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\n\n\n\n\n0.590\n0.845\n0.918\n0.998\n1.481\n0.926\n0.115\n10,000.000"
  },
  {
    "objectID": "slides/13/slides13.html#sampling-distribution",
    "href": "slides/13/slides13.html#sampling-distribution",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nWe could also draw many different samples and obtain the sampling distribution\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\n\n\n\n\n0.632\n0.933\n0.997\n1.065\n1.443\n1.001\n0.100\n10,000.000"
  },
  {
    "objectID": "slides/13/slides13.html#key-comparisons",
    "href": "slides/13/slides13.html#key-comparisons",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Key comparisons",
    "text": "Key comparisons\n\n\n\n\n\n\n\n\n\n\nMean\nSD\nBias\n\n\n\n\nPopulation\n1\n0.5\n\n\n\nSample\n0.925\n0.925\n\n\n\nSampling distribution\n1.001\n0.1\n0.001\n\n\nBootstrap distribution\n0.926\n0.115\n0.001"
  },
  {
    "objectID": "slides/13/slides13.html#implementation",
    "href": "slides/13/slides13.html#implementation",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": " implementation",
    "text": "implementation\n\n\n# Subsetting to get only one species\ngentoo &lt;- dplyr::filter(penguins, species == \"Gentoo\")\n\n# Bookkeeping\ny &lt;- gentoo$bill_length_mm\nn &lt;- nrow(gentoo)        # sample size\nN &lt;- 10^4                # desired no. resamples\nboot_means &lt;- numeric(N) # a place to store the bootstrap stats\n\n# Resampling from the sample\nfor (i in 1:N) {\n  x &lt;- sample(y, size = n, replace = TRUE)\n  boot_means[i] &lt;- mean(x, na.rm = TRUE)  # you can choose other statistics\n}\n# Calculate a 95% percentile interval\nquantile(boot_means, probs = c(0.025, 0.975))"
  },
  {
    "objectID": "slides/13/slides13.html#section",
    "href": "slides/13/slides13.html#section",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "",
    "text": "First, recall the definition of the CDF:\n\\[F_x(x_0) = P(X \\le x_0)\\]\n\nIn other words, \\(F_x\\) is the probability of the event \\(\\{X \\le x_0\\}\\). If we observe a sample of \\(X_1, ..., X_n \\sim F_x\\), a natural estimator for this probability is the observed proportion of observations where \\(\\{X_i \\le x_0\\}\\).\n\n\n\\[\\widehat F_n = \\frac{\\sum \\mathbb{I}(X_i \\le x_0)}{n}\\] so \\(\\widehat{F}_n\\) is an estimator for \\(F\\)."
  },
  {
    "objectID": "slides/13/slides13.html#how-does-widehat-f-relate-to-the-bootstrap",
    "href": "slides/13/slides13.html#how-does-widehat-f-relate-to-the-bootstrap",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "How does \\(\\widehat F\\) relate to the bootstrap?",
    "text": "How does \\(\\widehat F\\) relate to the bootstrap?"
  },
  {
    "objectID": "slides/13/slides13.html#how-does-widehat-f-relate-to-the-bootstrap-1",
    "href": "slides/13/slides13.html#how-does-widehat-f-relate-to-the-bootstrap-1",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "How does \\(\\widehat F\\) relate to the bootstrap?",
    "text": "How does \\(\\widehat F\\) relate to the bootstrap?\nEach bootstrap sample is drawn from \\(\\widehat F\\):\n\\[\nX_1^{*(1)}, X_2^{*(1)}, ... X_n^{*(1)} \\sim \\widehat F_n\n\\]\n\\[\nX_1^{*(2)}, X_2^{*(2)}, ... X_n^{*(2)} \\sim \\widehat F_n\n\\]\n\\[\nX_1^{*(3)}, X_2^{*(3)}, ... X_n^{*(3)} \\sim \\widehat F_n\n\\]\n\\[\\vdots\\]\n\\[\nX_1^{*(B)}, X_2^{*(B)}, ... X_n^{*(B} \\sim \\widehat F_n\n\\]"
  },
  {
    "objectID": "slides/13/slides13.html#as-n-increases-widehatf_n-gets-closer-to-true-f",
    "href": "slides/13/slides13.html#as-n-increases-widehatf_n-gets-closer-to-true-f",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "As n increases, \\(\\widehat{F}_n\\) gets closer to true \\(F\\)",
    "text": "As n increases, \\(\\widehat{F}_n\\) gets closer to true \\(F\\)"
  },
  {
    "objectID": "slides/13/slides13.html#section-1",
    "href": "slides/13/slides13.html#section-1",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "",
    "text": "It turns out that \\(\\widehat{F}_n\\) is an unbiased and consistent estimator for \\(F\\)!\n\nWhen \\(n\\) is large, \\(\\widehat{F}_n\\) is very close to \\(F\\)\nSo any statistic that is based on \\(\\widehat{F}_n\\) is very similar to the same statistic based on \\(F\\)\nRe-sampling from our original sample results in a sampling distribution that is very similar to the theoretical sampling distribution\nThis is true even if we don’t know what the theoretical sampling distribution is!"
  },
  {
    "objectID": "slides/13/slides13.html#your-turn-theoretical-example",
    "href": "slides/13/slides13.html#your-turn-theoretical-example",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Your turn: theoretical example",
    "text": "Your turn: theoretical example\nConsider a population that has a gamma distribution with parameters r=5, 𝜆=1∕4.\n\n\nUse simulation (with n = 200) to generate an approximate sampling distribution of the mean; plot and describe the distribution.\nNow, draw one random sample of size 200 from this population. Create a histogram of your sample and find the mean and standard deviation.\nCompute the bootstrap distribution of the mean for your sample, plot it, and note the bootstrap mean and standard error.\nCompare the bootstrap distribution to the approximate theoretical sampling distribution by creating a table like slide 17\n\n\nRepeat (a)–(d) for sample sizes of n = 50 and n = 10. Describe carefully your observations about the effects of sample size on the bootstrap distribution."
  },
  {
    "objectID": "slides/13/slides13.html#your-turn-data-example",
    "href": "slides/13/slides13.html#your-turn-data-example",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Your turn: data example",
    "text": "Your turn: data example\nThe Bangladesh data set contains information about arsenic, cobalt, and chlorine concentrations from a sample of 271 water wells in Bangladesh.\n\nlibrary(resampledata3)\nlibrary(tidyverse)\n\n\n\nConduct EDA on the chlorine concentrations and describe the salient features.\nFind the bootstrap distribution of the mean.\nFind and interpret the 95% bootstrap percentile confidence interval.\nWhat is the bootstrap estimate of the bias? What fraction of the bootstrap standard error does it represent?"
  },
  {
    "objectID": "slides/24/slides24.html#can-money-buy-you-happiness",
    "href": "slides/24/slides24.html#can-money-buy-you-happiness",
    "title": "Chi-Squared Tests",
    "section": "Can money buy you happiness?",
    "text": "Can money buy you happiness?\n\nThe General Social Survey (GSS) is a sociological survey used to collect data on demographic characteristics and attitudes of residents of the United States. We’ll consider two questions:\n\nCompared with American families in general, would you say your family income is far below average, below average, average, above average, or far above average?\nTaken all together, how would you say things are these days—would you say that you are very happy, pretty happy, or not too happy?"
  },
  {
    "objectID": "slides/24/slides24.html#can-money-buy-you-happiness-1",
    "href": "slides/24/slides24.html#can-money-buy-you-happiness-1",
    "title": "Chi-Squared Tests",
    "section": "Can money buy you happiness?",
    "text": "Can money buy you happiness?"
  },
  {
    "objectID": "slides/24/slides24.html#happiness-contingency-table",
    "href": "slides/24/slides24.html#happiness-contingency-table",
    "title": "Chi-Squared Tests",
    "section": "Happiness contingency table",
    "text": "Happiness contingency table\n\n\n\n\n\n\n\n\nhappy\nfar below average\nbelow average\naverage\nabove average\nfar above average\nTotal\n\n\n\n\nnot too happy\n50\n123\n120\n33\n4\n330\n\n\npretty happy\n64\n350\n602\n253\n24\n1293\n\n\nvery happy\n39\n121\n319\n190\n25\n694\n\n\nTotal\n153\n594\n1041\n476\n53\n2317\n\n\n\n\n\n\n\nHow can we conclude whether opinion on income and happiness are associated?"
  },
  {
    "objectID": "slides/24/slides24.html#hypotheses",
    "href": "slides/24/slides24.html#hypotheses",
    "title": "Chi-Squared Tests",
    "section": "Hypotheses",
    "text": "Hypotheses\n\\(H_0:\\) the variables are independent\n\nWhat would the contingency table look like under \\(H_0\\)?\n\n\n\n\n\n\n\n\n\nhappy\nfar below average\nbelow average\naverage\nabove average\nfar above average\n\n\n\n\nnot too happy\n21.79111\n84.60078\n148.2650\n67.79456\n7.548554\n\n\npretty happy\n85.38153\n331.48123\n580.9292\n265.63142\n29.576608\n\n\nvery happy\n45.82736\n177.91800\n311.8058\n142.57402\n15.874838"
  },
  {
    "objectID": "slides/24/slides24.html#how-can-we-compare-what-we-observe-to-what-would-be-expected-under-h_0",
    "href": "slides/24/slides24.html#how-can-we-compare-what-we-observe-to-what-would-be-expected-under-h_0",
    "title": "Chi-Squared Tests",
    "section": "How can we compare what we observe to what would be expected under \\(H_0\\)?",
    "text": "How can we compare what we observe to what would be expected under \\(H_0\\)?\nObserved:\n\n\n\n\n\n\n\n\nhappy\nfar below average\nbelow average\naverage\nabove average\nfar above average\nTotal\n\n\n\n\nnot too happy\n50\n123\n120\n33\n4\n330\n\n\npretty happy\n64\n350\n602\n253\n24\n1293\n\n\nvery happy\n39\n121\n319\n190\n25\n694\n\n\nTotal\n153\n594\n1041\n476\n53\n2317\n\n\n\n\n\n\n\nExpected:\n\n\n\n\n\n\n\n\nhappy\nfar below average\nbelow average\naverage\nabove average\nfar above average\nTotal\n\n\n\n\nnot too happy\n21.79111\n84.60078\n148.2650\n67.79456\n7.548554\n330\n\n\npretty happy\n85.38153\n331.48123\n580.9292\n265.63142\n29.576608\n1293\n\n\nvery happy\n45.82736\n177.91800\n311.8058\n142.57402\n15.874838\n694\n\n\nTotal\n153\n594\n1041\n476\n53\n2317"
  },
  {
    "objectID": "slides/24/slides24.html#chi-square-test-stat",
    "href": "slides/24/slides24.html#chi-square-test-stat",
    "title": "Chi-Squared Tests",
    "section": "Chi-square test stat",
    "text": "Chi-square test stat\n\\[C = \\sum_{\\text{all cells}} \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}} = \\sum (O - E)^2/E\\]\n\nBig when differences are big\nSmall when differences are small\nScaled by “bigness” of expected counts\nAll cells contribute"
  },
  {
    "objectID": "slides/24/slides24.html#when-h_0-is-true-c-dotsim-chi2_i-1j-1",
    "href": "slides/24/slides24.html#when-h_0-is-true-c-dotsim-chi2_i-1j-1",
    "title": "Chi-Squared Tests",
    "section": "When \\(H_0\\) is true, C \\(\\dot\\sim \\chi^2_{(I-1)(J-1)}\\)",
    "text": "When \\(H_0\\) is true, C \\(\\dot\\sim \\chi^2_{(I-1)(J-1)}\\)"
  },
  {
    "objectID": "slides/24/slides24.html#finding-p-values",
    "href": "slides/24/slides24.html#finding-p-values",
    "title": "Chi-Squared Tests",
    "section": "Finding p-values",
    "text": "Finding p-values\n\n“By hand”chisq with vectorschisq with table\n\n\n\n(observed - expected)^2/expected\n\n               happy2018$finrela\nhappy2018$happy far below average below average    average above average\n  not too happy        36.5167974    17.4289220  5.3883932    17.8577972\n  pretty happy          5.3544337     1.0345835  0.7642546     0.6006547\n  very happy            1.0171409    18.2087168  0.1659904    15.7758320\n               happy2018$finrela\nhappy2018$happy far above average\n  not too happy         1.6681654\n  pretty happy          1.0514577\n  very happy            5.2453183\n\nsum((observed - expected)^2/expected)\n\n[1] 128.0785\n\n1-pchisq(sum((observed - expected)^2/expected), df = (3-1)*(5-1))\n\n[1] 0\n\n\n\n\n\nchisq.test(happy2018$happy, happy2018$finrela)\n\n\n    Pearson's Chi-squared test\n\ndata:  happy2018$happy and happy2018$finrela\nX-squared = 128.08, df = 8, p-value &lt; 2.2e-16\n\n\n\n\n\nchisq.test(observed)\n\n\n    Pearson's Chi-squared test\n\ndata:  observed\nX-squared = 128.08, df = 8, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "slides/24/slides24.html#permutation-test",
    "href": "slides/24/slides24.html#permutation-test",
    "title": "Chi-Squared Tests",
    "section": "Permutation test",
    "text": "Permutation test\n\n\nStore the data in a table: one row per observation, one column per variable.\nCalculate a test statistic for the original data.\nRepeat\n\nRandomly permute the rows in one of the columns.\nCalculate the test statistic for the permuted data.\n\nCalculate the \\(p\\)-value as the fraction of times the random statistics exceed the original statistic."
  },
  {
    "objectID": "slides/24/slides24.html#permutation-test-setup",
    "href": "slides/24/slides24.html#permutation-test-setup",
    "title": "Chi-Squared Tests",
    "section": "Permutation test setup",
    "text": "Permutation test setup\nDrop any missing values\n\ndf &lt;- happy2018 |&gt; drop_na(happy, finrela)\n\nCalculate the observed test statistic\n\nobserved &lt;- chisq.test(happy2018$happy, happy2018$finrela)$statistic\nobserved\n\nX-squared \n 128.0785"
  },
  {
    "objectID": "slides/24/slides24.html#section",
    "href": "slides/24/slides24.html#section",
    "title": "Chi-Squared Tests",
    "section": "",
    "text": "Construct the permutation distribution\n\nset.seed(55057)\nN &lt;- 10^4 - 1\nresult &lt;- numeric(N)\nfor(i in 1:N) {\n  finrela_perm &lt;- sample(happy2018$finrela)\n  result[i] &lt;- chisq.test(happy2018$happy, finrela_perm)$statistic\n}"
  },
  {
    "objectID": "slides/24/slides24.html#permutation-distribution",
    "href": "slides/24/slides24.html#permutation-distribution",
    "title": "Chi-Squared Tests",
    "section": "Permutation distribution",
    "text": "Permutation distribution"
  },
  {
    "objectID": "slides/24/slides24.html#p-value",
    "href": "slides/24/slides24.html#p-value",
    "title": "Chi-Squared Tests",
    "section": "p-value",
    "text": "p-value\n\n\n(sum(result &gt;= observed) + 1) / (N + 1)\n## [1] 1e-04"
  },
  {
    "objectID": "slides/24/slides24.html#chi-squared-reference-distribution",
    "href": "slides/24/slides24.html#chi-squared-reference-distribution",
    "title": "Chi-Squared Tests",
    "section": "Chi-squared reference distribution",
    "text": "Chi-squared reference distribution"
  },
  {
    "objectID": "slides/24/slides24.html#section-1",
    "href": "slides/24/slides24.html#section-1",
    "title": "Chi-Squared Tests",
    "section": "",
    "text": "Simulation vs. model-based results\n\nChi-squared test\n\n1 - pchisq(observed, df = (3 - 1) * (5 - 1))\n\nX-squared \n        0 \n\n\n\nPermutation test\n\n(sum(result &gt;= observed) + 1) / (N + 1)\n\n[1] 0.0001"
  },
  {
    "objectID": "slides/24/slides24.html#a-shortcut-to-the-permutation-test",
    "href": "slides/24/slides24.html#a-shortcut-to-the-permutation-test",
    "title": "Chi-Squared Tests",
    "section": "A shortcut to the permutation test",
    "text": "A shortcut to the permutation test\n\nchisq.test(happy2018$happy, happy2018$finrela, simulate.p.value = TRUE)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  happy2018$happy and happy2018$finrela\nX-squared = 128.08, df = NA, p-value = 0.0004998"
  },
  {
    "objectID": "slides/24/slides24.html#caution",
    "href": "slides/24/slides24.html#caution",
    "title": "Chi-Squared Tests",
    "section": "Caution",
    "text": "Caution\nThe \\(\\chi^2\\) distribution provides a reasonable approximation of the null distribution as long as the sample size is “large enough”\nCommon guidelines:\n\n\n“Cochran’s rule:” All of the cells have expected counts &gt; 5\nAll expected counts are at least 1 and no more than 20% of cells have expected counts &lt; 5\n\n\nUse a permutation test if the expected counts aren’t large enough"
  },
  {
    "objectID": "slides/24/slides24.html#example",
    "href": "slides/24/slides24.html#example",
    "title": "Chi-Squared Tests",
    "section": "Example",
    "text": "Example\nSome people think that children who are the older ones in their class at school naturally perform better in sports and that these children then get more coaching and encouragement as they get older. Could that make a difference in who makes it to the professional level in sports? Below is the birth month of 1478 major league players born since 1975, along with the national birth percentage across the same years.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nObs\n137\n121\n116\n121\n126\n114\n102\n165\n134\n115\n105\n122\n\n\nBirth %\n8%\n7%\n8%\n8%\n8%\n8%\n9%\n9%\n9%\n9%\n8%\n9%\n\n\n\n\n\nWrite out an appropriate null and alternative hypothesis\nCalculate the expected counts for each cell under the null hypothesis\nCompute the chi-square test statistic\nCompute the p-value\nDraw a conclusion in context"
  },
  {
    "objectID": "slides/24/slides24.html#likelihood-ratio-test",
    "href": "slides/24/slides24.html#likelihood-ratio-test",
    "title": "Chi-Squared Tests",
    "section": "Likelihood Ratio test",
    "text": "Likelihood Ratio test"
  },
  {
    "objectID": "slides/06/slides06.html#big-idea",
    "href": "slides/06/slides06.html#big-idea",
    "title": "Maximum Likelihood Estimation II",
    "section": "Big Idea",
    "text": "Big Idea\nWe’re interested in the proportion of Carleton students who have had some sort of research experience. We have the resources to randomly sample 10 students.\n\n\nProbability:\n\nStatistics:"
  },
  {
    "objectID": "slides/06/slides06.html#example",
    "href": "slides/06/slides06.html#example",
    "title": "Maximum Likelihood Estimation II",
    "section": "Example",
    "text": "Example\nRecall that the likelihood function for \\(n\\) iid Bernoulli(\\(\\theta\\)) random variable is \\(L(\\theta) = \\theta^{\\sum x_i} (1-\\theta)^{n - \\sum x_i}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nScenario 1: 0 yes responses\n\n\n\n\\(\\theta\\)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\n\n\\(L(\\theta)\\)\n\n\n\n\n\n\n\n\n\nScenario 2: 6 yes responses\n\n\n\n\\(\\theta\\)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\n\n\\(L(\\theta)\\)"
  },
  {
    "objectID": "slides/06/slides06.html#exercise",
    "href": "slides/06/slides06.html#exercise",
    "title": "Maximum Likelihood Estimation II",
    "section": "Exercise",
    "text": "Exercise\nIs the likelihood function a probability distribution? Why or why not?"
  },
  {
    "objectID": "slides/06/slides06.html#exercise-from-last-class",
    "href": "slides/06/slides06.html#exercise-from-last-class",
    "title": "Maximum Likelihood Estimation II",
    "section": "Exercise: from last class",
    "text": "Exercise: from last class\nLet \\(X_1, ..., X_n\\) be an iid random sample from a distribution with PDF\n\\[f(x|\\theta)= (\\theta + 1)x^\\theta, 0 \\le x \\le 1\\]\n\\[L(\\theta) =\\] \\[\\hat\\theta_{MLE}=\\]\nSuppose we observe a sample of size 5: {.83, .49, .72, .57, .66}. Find the maximum likelihood estimate and verify with a graph or numerical approximation"
  },
  {
    "objectID": "slides/06/slides06.html#example-uniform-disribution",
    "href": "slides/06/slides06.html#example-uniform-disribution",
    "title": "Maximum Likelihood Estimation II",
    "section": "Example: Uniform disribution",
    "text": "Example: Uniform disribution\nFind the MLE for \\(Y_1, ..., Y_n \\sim \\text{Unif}(0, \\theta)\\)"
  },
  {
    "objectID": "slides/06/slides06.html#finding-the-mle-when-more-than-one-parameter-is-unknown",
    "href": "slides/06/slides06.html#finding-the-mle-when-more-than-one-parameter-is-unknown",
    "title": "Maximum Likelihood Estimation II",
    "section": "Finding the MLE when more than one parameter is unknown",
    "text": "Finding the MLE when more than one parameter is unknown\nIf the pdf or pmf that we’re using has two or more parameters, say \\(\\theta_1\\) and \\(\\theta_2\\), finding MLEs for the \\(\\theta_i\\)’s requires the solution of a sest of simultaneous equations\n\n\\[\\frac{\\partial}{\\partial \\theta_1} \\ln L(\\theta_1, \\theta_2) = 0\\]\n\n\n\\[\\frac{\\partial}{\\partial \\theta_2} \\ln L(\\theta_1, \\theta_2) = 0\\]"
  },
  {
    "objectID": "slides/06/slides06.html#example-nmu-sigma2-distribution",
    "href": "slides/06/slides06.html#example-nmu-sigma2-distribution",
    "title": "Maximum Likelihood Estimation II",
    "section": "Example: \\(N(\\mu, \\sigma^2)\\) distribution",
    "text": "Example: \\(N(\\mu, \\sigma^2)\\) distribution\nSuppose a random sample of size \\(n\\) is drawn from the two parameter normal pdf:\n\\(f_y(y|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp(-(\\frac{y-\\mu}{\\sigma})^2)\\)\nfind the MLEs \\(\\hat{\\mu}\\) and \\(\\hat\\sigma^2\\)"
  },
  {
    "objectID": "slides/08/slides08.html#recap",
    "href": "slides/08/slides08.html#recap",
    "title": "Evaluating Estimators",
    "section": "Recap",
    "text": "Recap\n\nObserve data \\(X_1, ..., X_n \\sim F_x(x|\\theta)\\), where \\(\\theta\\) is unknown\nGoal: Estimate \\(\\theta\\) based on the values of \\(X_i\\) by formulating an estimator \\(\\widehat \\theta\\)\nOne technique is to use the maximum likelihood estimator, which finds the value of \\(\\theta\\) that maximizes the joint probability \\(\\prod_{i=1}^n f_x(x_i | \\theta)\\)\nA second technique is to use the method of moments estimator, which finds the value of \\(\\theta\\) that make the theoretical moments equal to the sample moments\nToday: if we have multiple estimators, how do we decide which is better?"
  },
  {
    "objectID": "slides/08/slides08.html#warm-up",
    "href": "slides/08/slides08.html#warm-up",
    "title": "Evaluating Estimators",
    "section": "Warm up",
    "text": "Warm up\nSuppose we take a random sample of size 50 from an exponential distribution with rate \\(\\lambda = 1/10\\).\nWhat is \\(\\mu = E(Y)\\)?\nIf we want to design an estimator for the mean, \\(\\widehat \\mu\\), what are some intuitive estimators?"
  },
  {
    "objectID": "slides/08/slides08.html#estimators-for-the-mean",
    "href": "slides/08/slides08.html#estimators-for-the-mean",
    "title": "Evaluating Estimators",
    "section": "Estimators for the mean",
    "text": "Estimators for the mean\nSuppose we take a random sample of size 50 from an exponential distribution with mean 10 (rate is \\(\\lambda = 1/10\\)).\nConsider three estimators of \\(\\mu\\):\n\n\\(\\widehat{\\mu_1} = \\bar{X}\\)\n\\(\\widehat{\\mu_2} = \\widetilde{X}\\)\n\\(\\widehat{\\mu_3} = \\frac{X_{\\max} - X_{\\min}}{2}\\)"
  },
  {
    "objectID": "slides/08/slides08.html#aside-estimators-are-also-random-variables",
    "href": "slides/08/slides08.html#aside-estimators-are-also-random-variables",
    "title": "Evaluating Estimators",
    "section": "Aside: estimators are also random variables",
    "text": "Aside: estimators are also random variables\n\\(X_1, ..., X_n \\sim F_x(\\theta)\\) and each \\(\\widehat{\\theta} = g(X_1, ..., X_n)\\) is a function of the data\n\nThis means that each \\(\\widehat\\theta\\) is itself a random variable, and so it has:\n\na pdf \\(f_\\widehat{\\theta}\\)\nan expectation \\(E(\\widehat\\theta)\\)\na variance \\(\\text{Var}(\\widehat{\\theta})\\)"
  },
  {
    "objectID": "slides/08/slides08.html#comparing-estimators-simulation",
    "href": "slides/08/slides08.html#comparing-estimators-simulation",
    "title": "Evaluating Estimators",
    "section": "Comparing Estimators: Simulation",
    "text": "Comparing Estimators: Simulation\nSuppose we take a random sample of size 50 from an exponential distribution with mean 10 (rate is \\(\\lambda = 1/10\\)).\n\n\nConsider three estimators of \\(\\mu\\):\n\n\\(\\hat{\\mu_1} = \\bar{X}\\)\n\\(\\hat{\\mu_2} = \\tilde{X}\\)\n\\(\\hat{\\mu_3} = \\frac{X_{\\max} - X_{\\min}}{2}\\)\n\n\n\nn &lt;- 50\nN_sims &lt;- 100000\nest1 &lt;- numeric(N_sims)\nest2 &lt;- numeric(N_sims)\nest3 &lt;- numeric(N_sims)\nfor(i in 1:N_sims){\n  x &lt;- rexp(n, rate = .1)\n  est1[i] &lt;- mean(x)\n  est2[i] &lt;- median(x)\n  est3[i] &lt;- (max(x) - min(x))/2\n}"
  },
  {
    "objectID": "slides/08/slides08.html#sampling-distribution-of-the-estimators",
    "href": "slides/08/slides08.html#sampling-distribution-of-the-estimators",
    "title": "Evaluating Estimators",
    "section": "Sampling Distribution of the Estimators",
    "text": "Sampling Distribution of the Estimators\n\nwhich estimator is “best”?"
  },
  {
    "objectID": "slides/08/slides08.html#properties-of-estimators",
    "href": "slides/08/slides08.html#properties-of-estimators",
    "title": "Evaluating Estimators",
    "section": "Properties of Estimators",
    "text": "Properties of Estimators"
  },
  {
    "objectID": "slides/08/slides08.html#estimator-properties-bias",
    "href": "slides/08/slides08.html#estimator-properties-bias",
    "title": "Evaluating Estimators",
    "section": "Estimator properties: bias",
    "text": "Estimator properties: bias\nHow accurate is the estimator?\n\n\n\n\n\n\n\nBias of an estimator\n\n\nIf \\(\\widehat{\\theta}(X)\\) is an estimator of \\(\\theta\\), then the bias of the estimator is equal to\n\\[\\text{Bias}(\\widehat\\theta) = E[\\widehat \\theta(X)] - \\theta\\]\nNote: the expected value is computed from the sampling distribution of \\(\\widehat{\\theta}(X)\\)"
  },
  {
    "objectID": "slides/08/slides08.html#estimator-properties-variance",
    "href": "slides/08/slides08.html#estimator-properties-variance",
    "title": "Evaluating Estimators",
    "section": "Estimator properties: variance",
    "text": "Estimator properties: variance\nHow much variability does the estimator have around its mean?\n\n\n\n\n\n\nVariance of an estimator\n\n\nThe variance of an estimator is\n\\[\\text{Var}(\\widehat{\\theta}) = E[(\\widehat{\\theta} - E[\\widehat{\\theta}])^2]\\]\nNote: the expected value is computed from the sampling distribution of \\(\\widehat{\\theta}(X)\\)"
  },
  {
    "objectID": "slides/08/slides08.html#estimator-properties-mean-square-error-mse",
    "href": "slides/08/slides08.html#estimator-properties-mean-square-error-mse",
    "title": "Evaluating Estimators",
    "section": "Estimator properties: Mean Square Error (MSE)",
    "text": "Estimator properties: Mean Square Error (MSE)\nHow much variability does the estimator have around \\(\\theta\\)?\n\n\n\n\n\n\nMSE\n\n\nThe MSE of an estimator is\n\\[MSE(\\widehat\\theta) = E[(\\widehat\\theta - \\theta)^2] = \\text{Var}(\\widehat{\\theta}) + \\text{Bias}(\\widehat\\theta)^2\\]"
  },
  {
    "objectID": "slides/08/slides08.html#comparing-estimators-simulation-1",
    "href": "slides/08/slides08.html#comparing-estimators-simulation-1",
    "title": "Evaluating Estimators",
    "section": "Comparing Estimators: simulation",
    "text": "Comparing Estimators: simulation\nRecall that \\(E(Y) = \\mu = 1/\\lambda = 10\\) is the true value\n\nsims |&gt;\n  group_by(estimator) |&gt;\n  summarize(\n    bias = mean(value) - 10,\n    var = var(value),\n    MSE = var + bias^2\n  ) \n\n# A tibble: 3 × 4\n  estimator     bias   var    MSE\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 mean       0.00123  1.99   1.99\n2 median    -2.97     1.96  10.8 \n3 midpoint  12.4     40.7  195."
  },
  {
    "objectID": "slides/08/slides08.html#practice-unif0-theta-distribution",
    "href": "slides/08/slides08.html#practice-unif0-theta-distribution",
    "title": "Evaluating Estimators",
    "section": "Practice: Unif(0, \\(\\theta\\)) distribution1",
    "text": "Practice: Unif(0, \\(\\theta\\)) distribution1\nYour task is to compare the estimators\n\\[\\widehat{\\theta}_{MLE} = X_\\max \\hspace{1in} \\widehat{\\theta}_{MoM} = 2\\bar{X}\\]\n\n\nWhat is the bias of each estimator?2\nWhat is the SE of each estimator?\nWhat is the MSE of each estimator?\nWhen does \\(\\widehat{\\theta}_{MLE}\\) “beat” \\(\\widehat{\\theta}_{MoM}\\) in terms of MSE?\n\n\nSo \\(0 \\le x \\le \\theta\\), \\(f_x = \\frac{1}{\\theta}\\), \\(F_x = \\frac{x}{\\theta}\\), \\(E(X) = \\frac{\\theta}{2}\\), and \\(V(X) = \\frac{\\theta^2}{12}\\)A helpful fact is that \\(f_{X_{max}}(x) = n[F(x)]^{n-1}f_X(x)\\)"
  },
  {
    "objectID": "slides/08/slides08.html#section",
    "href": "slides/08/slides08.html#section",
    "title": "Evaluating Estimators",
    "section": "",
    "text": "(d) When does \\(\\widehat{\\theta}_{MLE}\\) “beat” \\(\\widehat{\\theta}_{MoM}\\) in terms of MSE?\nMSE = “MSE factor” \\(\\times \\theta^2\\)"
  },
  {
    "objectID": "slides/08/slides08.html#example-uniform0theta",
    "href": "slides/08/slides08.html#example-uniform0theta",
    "title": "Evaluating Estimators",
    "section": "Example: Uniform(\\(0,\\theta\\))",
    "text": "Example: Uniform(\\(0,\\theta\\))\n\n\nSince \\(\\hat \\theta_{MLE}\\) beats \\(\\hat \\theta_{MoM}\\) in terms of MSE but is biased, can we “unbias” the MLE? Call this third estimator \\(\\hat\\theta_3\\)\nDoes \\(\\hat \\theta_3\\) ever “beat” \\(\\hat \\theta_{MLE}\\) in terms of MSE?"
  },
  {
    "objectID": "slides/08/slides08.html#section-1",
    "href": "slides/08/slides08.html#section-1",
    "title": "Evaluating Estimators",
    "section": "",
    "text": "(f) Does \\(\\hat \\theta_3\\) ever “beat” \\(\\hat \\theta_{MLE}\\) in terms of MSE?\nMSE = “MSE factor” \\(\\times \\theta^2\\)"
  },
  {
    "objectID": "slides/08/slides08.html#comparing-unbiased-estimators-efficiency",
    "href": "slides/08/slides08.html#comparing-unbiased-estimators-efficiency",
    "title": "Evaluating Estimators",
    "section": "Comparing unbiased estimators: efficiency",
    "text": "Comparing unbiased estimators: efficiency\n\n\n\n\n\n\nEfficiency\n\n\nFor two unbiased estimators, \\(\\widehat \\theta_1\\) is more efficient than \\(\\widehat \\theta_2\\) if\n\\[\\text{Var}(\\widehat \\theta_1) &lt; \\text{Var}(\\widehat \\theta_2) \\iff \\text{SE}(\\widehat \\theta_1) &lt; \\text{SE}(\\widehat \\theta_2)\\]"
  },
  {
    "objectID": "slides/08/slides08.html#comparing-unbiased-estimators-cramer-rao-lower-bound-crlb",
    "href": "slides/08/slides08.html#comparing-unbiased-estimators-cramer-rao-lower-bound-crlb",
    "title": "Evaluating Estimators",
    "section": "Comparing unbiased estimators: Cramer-Rao Lower Bound (CRLB)",
    "text": "Comparing unbiased estimators: Cramer-Rao Lower Bound (CRLB)\n\n\n\n\n\n\n\nCRLB\n\n\nIf \\(X_1, ..., X_n\\) are an iid sample from a distribution with pdf \\(f(x|\\theta)\\), then any unbiased estimator \\(\\hat\\theta\\) of \\(\\theta\\) satisfies:\n\\[V(\\hat{\\theta}) \\ge \\frac{1}{n I(\\theta)}\\]\nwhere \\(I(\\theta)\\) is the Fisher Information of \\(X_i\\)"
  },
  {
    "objectID": "slides/08/slides08.html#fisher-information",
    "href": "slides/08/slides08.html#fisher-information",
    "title": "Evaluating Estimators",
    "section": "Fisher Information",
    "text": "Fisher Information\n\n\n\n\n\n\nFisher Information\n\n\nThe Fisher Information of a random variable \\(X\\) is \\[I(\\theta) = E[(\\frac{d}{d\\theta} \\ln f(x|\\theta))^2] = -   E(l''(\\theta))\\]\n*provided certain regularity conditions are met"
  },
  {
    "objectID": "slides/07/slides07.html#recap",
    "href": "slides/07/slides07.html#recap",
    "title": "Method of Moments",
    "section": "Recap",
    "text": "Recap\n\nObserve data \\(X_1, ..., X_n \\sim F_x(x|\\theta)\\), where \\(\\theta\\) is unknown\nGoal: Estimate \\(\\theta\\) based on the values of \\(X_i\\) by formulating an estimator \\(\\hat \\theta\\)\nOne technique is to use the maximum likelihood estimator, which finds the value of \\(\\theta\\) that maximizes the joint probability \\(\\prod_{i=1}^n f_x(x_i | \\theta)\\)\nToday, we’ll talk about another technique for estimating \\(\\theta\\)"
  },
  {
    "objectID": "slides/07/slides07.html#which-population-is-more-likely",
    "href": "slides/07/slides07.html#which-population-is-more-likely",
    "title": "Method of Moments",
    "section": "Which population is more likely?",
    "text": "Which population is more likely?\n\n\n\nSample\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n   mean   var\n  &lt;dbl&gt; &lt;dbl&gt;\n1  20.7  16.5\n\n\n\n\n\n\n\n\\(N(10, 5)\\)\\(N(20, 5)\\)"
  },
  {
    "objectID": "slides/07/slides07.html#which-population-is-more-likely-1",
    "href": "slides/07/slides07.html#which-population-is-more-likely-1",
    "title": "Method of Moments",
    "section": "Which population is more likely?",
    "text": "Which population is more likely?\n\n\n\nSample\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n   mean   var\n  &lt;dbl&gt; &lt;dbl&gt;\n1  20.7  16.5\n\n\n\n\n\n\n\n\\(N(20, 1.5^2)\\)\\(N(20, 4^2)\\)"
  },
  {
    "objectID": "slides/07/slides07.html#method-of-moments-mom",
    "href": "slides/07/slides07.html#method-of-moments-mom",
    "title": "Method of Moments",
    "section": "Method of Moments (MoM)",
    "text": "Method of Moments (MoM)\n\nMean of sample data should match mean of theoretical distribution\nVariance of sample data should match variance of theoretical distribution\nSkewness of sample data should match skewness of theoretical distribution\n\\(\\vdots\\)\n(for as many unknown parameters as we have)"
  },
  {
    "objectID": "slides/17/slides17.html#goal-develop-an-interval-estimate-for-mu_1---mu_2",
    "href": "slides/17/slides17.html#goal-develop-an-interval-estimate-for-mu_1---mu_2",
    "title": "Two Sample Confidence Intervals",
    "section": "Goal: develop an interval estimate for \\(\\mu_1 - \\mu_2\\)",
    "text": "Goal: develop an interval estimate for \\(\\mu_1 - \\mu_2\\)"
  },
  {
    "objectID": "slides/17/slides17.html#section",
    "href": "slides/17/slides17.html#section",
    "title": "Two Sample Confidence Intervals",
    "section": "",
    "text": "Example: There are two types of forensic analyses that are typically performed in cases where firearms evidence has been collected: bullets and cartridges. We are interested in whether there is a difference in the average inconclusive rate (how often the firearms examiner cannot come to a definitive conclusion) for these two evidence types.\n\n\n\n\n\n\n\n\n\n\n\n\ntype\nmean\nsd\nn\n\n\n\n\nbullet\n0.547\n0.276\n38\n\n\ncartridge\n0.461\n0.223\n40"
  },
  {
    "objectID": "slides/17/slides17.html#confidence-interval-toolkit",
    "href": "slides/17/slides17.html#confidence-interval-toolkit",
    "title": "Two Sample Confidence Intervals",
    "section": "Confidence Interval toolkit:",
    "text": "Confidence Interval toolkit:\n\nConstruct a bootstrap distribution and find a percentile-based confidence interval\nEither (a) assume a normal population or (b) use the CLT to construct a formula t-based confidence interval\nAssume a different population distribution and construct a pivot-based confidence interval\nUse the bootstrap to find the distribution of the T statistics to construct a bootstrap t confidence interval"
  },
  {
    "objectID": "slides/17/slides17.html#bootstrap-percentile-interval",
    "href": "slides/17/slides17.html#bootstrap-percentile-interval",
    "title": "Two Sample Confidence Intervals",
    "section": "Bootstrap percentile interval",
    "text": "Bootstrap percentile interval\n\nx &lt;- bullets_inc\ny &lt;- cc_inc\n\nn1 = length(x)\nn2 = length(y)\nN = 10^4\nboot_diff = numeric(N)\n\nfor(i in 1:N){\n  x_boot &lt;- sample(x, size = n1, replace = TRUE)\n  y_boot &lt;- sample(y, size = n2, replace = TRUE)\n  boot_diff[i] &lt;- mean(x_boot, na.rm = TRUE) - mean(y_boot, na.rm = TRUE)\n}\n\nquantile(boot_diff, probs = c(.025, .975))\n\n       2.5%       97.5% \n-0.02295906  0.19685965"
  },
  {
    "objectID": "slides/17/slides17.html#clt-based-t-confidence-interval",
    "href": "slides/17/slides17.html#clt-based-t-confidence-interval",
    "title": "Two Sample Confidence Intervals",
    "section": "CLT-based t confidence interval",
    "text": "CLT-based t confidence interval\n\nqt(.975, df = 37)\n\n[1] 2.026192\n\n\n   \n\nt.test(bullets_inc, cc_inc, conf.level = .95)$conf\n\n[1] -0.02682935  0.20045508\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "slides/17/slides17.html#bootstrap-t-confidence-interval",
    "href": "slides/17/slides17.html#bootstrap-t-confidence-interval",
    "title": "Two Sample Confidence Intervals",
    "section": "Bootstrap t confidence interval",
    "text": "Bootstrap t confidence interval\n\nn1 = length(bullets_inc)\nn2 = length(cc_inc)\nN = 10^4\nboot_tstar &lt;- numeric(N)\nfor(i in 1:N){\n  x_boot &lt;- sample(x, size = n1, replace = TRUE)\n  y_boot &lt;- sample(y, size = n2, replace = TRUE)\n  boot_tstar[i] &lt;- ((mean(x_boot) - mean(y_boot)) - (mean(x) - mean(y)))/sqrt(var(x_boot)/n1 + var(y_boot)/n2)\n}\n\nquantile(boot_tstar, probs = c(.025, .975))\n\n     2.5%     97.5% \n-1.929443  2.097633 \n\n(mean(x)- mean(y)) - quantile(boot_tstar, c(.975, .025))*sqrt(var(x)/n1 + var(y)/n2)\n\n      97.5%        2.5% \n-0.03274359  0.19678315"
  },
  {
    "objectID": "slides/17/slides17.html#is-bootstrap-t-necessary",
    "href": "slides/17/slides17.html#is-bootstrap-t-necessary",
    "title": "Two Sample Confidence Intervals",
    "section": "Is bootstrap t necessary?",
    "text": "Is bootstrap t necessary?"
  },
  {
    "objectID": "slides/17/slides17.html#section-1",
    "href": "slides/17/slides17.html#section-1",
    "title": "Two Sample Confidence Intervals",
    "section": "",
    "text": "Watch out!"
  },
  {
    "objectID": "slides/17/slides17.html#section-2",
    "href": "slides/17/slides17.html#section-2",
    "title": "Two Sample Confidence Intervals",
    "section": "",
    "text": "Example: Oops! I forgot to tell you that this study actually had the same examiners in both samples. (There are just two missing values in bullets_inc)."
  },
  {
    "objectID": "slides/17/slides17.html#observations-are-positive-correlated-to",
    "href": "slides/17/slides17.html#observations-are-positive-correlated-to",
    "title": "Two Sample Confidence Intervals",
    "section": "Observations are positive correlated \\(\\to\\)",
    "text": "Observations are positive correlated \\(\\to\\)"
  },
  {
    "objectID": "slides/17/slides17.html#percentile-bootstrap-for-paired-data",
    "href": "slides/17/slides17.html#percentile-bootstrap-for-paired-data",
    "title": "Two Sample Confidence Intervals",
    "section": "Percentile bootstrap for paired data",
    "text": "Percentile bootstrap for paired data\n\nx &lt;- bullets_inc - cc_inc[-c(1,2)]\nn = length(x)\nN = 10^4\nboot_diff = numeric(N)\n\nfor(i in 1:N){\n  x_boot &lt;- sample(x, size = n, replace = TRUE)\n  boot_diff[i] &lt;- mean(x_boot, na.rm = TRUE)\n}\n\nquantile(boot_diff, probs = c(.025, .975))\n\n      2.5%      97.5% \n0.03888889 0.16491228"
  },
  {
    "objectID": "slides/17/slides17.html#formula-based-t-for-paired-data",
    "href": "slides/17/slides17.html#formula-based-t-for-paired-data",
    "title": "Two Sample Confidence Intervals",
    "section": "Formula-based t for paired data",
    "text": "Formula-based t for paired data\n \n\nt.test(bullets_inc, cc_inc[-c(1,2)], paired = TRUE)$conf\n\n[1] 0.03670524 0.16738833\nattr(,\"conf.level\")\n[1] 0.95\n\nt.test(bullets_inc - cc_inc[-c(1,2)])$conf\n\n[1] 0.03670524 0.16738833\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "slides/17/slides17.html#r-activity",
    "href": "slides/17/slides17.html#r-activity",
    "title": "Two Sample Confidence Intervals",
    "section": "R Activity",
    "text": "R Activity"
  },
  {
    "objectID": "slides/16/slides16.html#goal-develop-an-interval-estimate-of-a-population-parameter",
    "href": "slides/16/slides16.html#goal-develop-an-interval-estimate-of-a-population-parameter",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Goal: develop an interval estimate of a population parameter",
    "text": "Goal: develop an interval estimate of a population parameter"
  },
  {
    "objectID": "slides/16/slides16.html#section",
    "href": "slides/16/slides16.html#section",
    "title": "Confidence intervals via Bootstrap t",
    "section": "",
    "text": "Example: Suppose we observe \\(X_1, ..., X_n\\) , which represent the error rates of \\(n\\) different forensic examiners. I am interested in finding a confidence interval for \\(\\mu\\), the average error rate in the population. We have a sample of size 30 shown below:\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n0.0325\n0.0343\n30"
  },
  {
    "objectID": "slides/16/slides16.html#lets-review-our-confidence-interval-toolkit-so-far",
    "href": "slides/16/slides16.html#lets-review-our-confidence-interval-toolkit-so-far",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Let’s review our confidence interval toolkit so far:",
    "text": "Let’s review our confidence interval toolkit so far:\n\nConstruct a bootstrap distribution and find a percentile-based confidence interval\nEither (a) assume a normal population or (b) use the CLT to construct a formula t-based confidence interval\nAssume a different population distribution and construct a pivot-based confidence interval"
  },
  {
    "objectID": "slides/16/slides16.html#bootstrap-percentile-interval",
    "href": "slides/16/slides16.html#bootstrap-percentile-interval",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Bootstrap percentile interval",
    "text": "Bootstrap percentile interval\n\nerror_rates &lt;- examiner_error_rates$error_rate\nn = length(error_rates)\nN = 10^4\nboot_means = numeric(N)\n\nfor(i in 1:N){\n  x &lt;- sample(error_rates, size = n, replace = TRUE)\n  boot_means[i] &lt;- mean(x, na.rm = TRUE)\n}\n\nquantile(boot_means, probs = c(.025, .975))\n\n      2.5%      97.5% \n0.02147176 0.04576585"
  },
  {
    "objectID": "slides/16/slides16.html#clt-based-t-confidence-interval",
    "href": "slides/16/slides16.html#clt-based-t-confidence-interval",
    "title": "Confidence intervals via Bootstrap t",
    "section": "CLT-based t confidence interval",
    "text": "CLT-based t confidence interval\n\nqt(.975, df = 29)\n\n[1] 2.04523"
  },
  {
    "objectID": "slides/16/slides16.html#clt-based-t-confidence-interval-1",
    "href": "slides/16/slides16.html#clt-based-t-confidence-interval-1",
    "title": "Confidence intervals via Bootstrap t",
    "section": "CLT-based t confidence interval",
    "text": "CLT-based t confidence interval\nCan also use a built-in R function:\n\nt.test(error_rates, conf.level = .95)$conf\n\n[1] 0.01969500 0.04531505\nattr(,\"conf.level\")\n[1] 0.95\n\n\nWhen do we trust this confidence interval?"
  },
  {
    "objectID": "slides/16/slides16.html#look-at-bootstrap-t",
    "href": "slides/16/slides16.html#look-at-bootstrap-t",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Look at bootstrap T*",
    "text": "Look at bootstrap T*\n\nt_star &lt;- numeric(N)\n\nfor(i in 1:N){\n  x &lt;- sample(error_rates, size = n, replace = TRUE)\n  t_star[i] &lt;- (mean(x) - mean(error_rates))/(sd(x)/sqrt(n))\n}"
  },
  {
    "objectID": "slides/16/slides16.html#issues",
    "href": "slides/16/slides16.html#issues",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Issues",
    "text": "Issues\n\nDistribution of \\(T*\\) is slightly left-skewed\nTails of \\(T*\\) don’t seem to match the theoretical t"
  },
  {
    "objectID": "slides/16/slides16.html#why-does-this-happen",
    "href": "slides/16/slides16.html#why-does-this-happen",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Why does this happen?",
    "text": "Why does this happen?"
  },
  {
    "objectID": "slides/16/slides16.html#bootstrap-t-confidence-interval",
    "href": "slides/16/slides16.html#bootstrap-t-confidence-interval",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Bootstrap t confidence interval",
    "text": "Bootstrap t confidence interval\nThe CLT-based confidence interval relies on:\n\\[1-\\alpha = P(t_{\\alpha/2} &lt; \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} &lt; t_{1-\\alpha/2})\\]\nIdea: generate “better” quantiles using the bootstrap distribution:"
  },
  {
    "objectID": "slides/16/slides16.html#then-solve-for-mu",
    "href": "slides/16/slides16.html#then-solve-for-mu",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Then solve for \\(\\mu\\):",
    "text": "Then solve for \\(\\mu\\):"
  },
  {
    "objectID": "slides/16/slides16.html#to-obtain-the-bootstrap-t-confidence-interval",
    "href": "slides/16/slides16.html#to-obtain-the-bootstrap-t-confidence-interval",
    "title": "Confidence intervals via Bootstrap t",
    "section": "to obtain the bootstrap \\(t\\) confidence interval:",
    "text": "to obtain the bootstrap \\(t\\) confidence interval:\n\n\n\n\n\n\n\n\nWatch out!\n\n\nLower bound is computed from the upper percentile and upper bound is computed from the lower percentile"
  },
  {
    "objectID": "slides/16/slides16.html#bootstrap-t-algorithm",
    "href": "slides/16/slides16.html#bootstrap-t-algorithm",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Bootstrap T algorithm",
    "text": "Bootstrap T algorithm\n\n\n\n\n\n\nBootstrap T algorithm\n\n\n\nRepeat the bootstrap procedure many times:\n\nTake bootstrap sample\nCompute mean (\\(\\bar{X}^*\\)) and sd (\\(s^*\\)) of each bootstrap sample\nCompute t-statistic for each bootstrap sample: \\(T^* = \\frac{\\bar{X}^* - \\bar{X}}{s^*/\\sqrt{n}}\\)\n\nFind quantiles \\(Q_{\\alpha/2}^*\\) and \\(Q_{1-\\alpha/2}^*\\) using distribution of \\(T^*\\)\nCompute confidence interval using the bootstrap t quantiles: \\[(\\bar{x} - Q^*_{1-\\alpha/2}\\frac{s}{\\sqrt{n}}, \\bar{x} - Q^*_{\\alpha/2}\\frac{s}{\\sqrt{n}})\\]"
  },
  {
    "objectID": "slides/16/slides16.html#example-bootstrap-t-for-error-rates",
    "href": "slides/16/slides16.html#example-bootstrap-t-for-error-rates",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Example: bootstrap t for error rates",
    "text": "Example: bootstrap t for error rates\n\nn = length(error_rates)\nN = 10^4\nboot_means &lt;- numeric(N)\nboot_tstar &lt;- numeric(N)\nfor(i in 1:N){\n  x &lt;- sample(error_rates, size = n, replace = TRUE)\n  boot_means[i] &lt;- mean(x)\n  t_star[i] &lt;- (mean(x) - mean(error_rates))/(sd(x)/sqrt(n))\n}\n\nquantile(t_star, probs = c(.025, .975))\n\n     2.5%     97.5% \n-2.934144  1.717921 \n\nmean(error_rates) - quantile(t_star, probs = c(.025, .975))*sd(error_rates)/sqrt(n)\n\n      2.5%      97.5% \n0.05088264 0.02174506"
  },
  {
    "objectID": "slides/16/slides16.html#summary-of-results",
    "href": "slides/16/slides16.html#summary-of-results",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Summary of results",
    "text": "Summary of results\n\n\n\n\nLower\nUpper\n\n\n\n\nPercentile Bootstrap\n.021\n.0458\n\n\nCLT t-based\n.020\n.0453\n\n\nBootstrap t\n.022\n.0509"
  },
  {
    "objectID": "slides/16/slides16.html#r-activity",
    "href": "slides/16/slides16.html#r-activity",
    "title": "Confidence intervals via Bootstrap t",
    "section": "R Activity",
    "text": "R Activity\nThe {nycflights23} R package contains a dataset called flights. This dataset contains all flights in and out of NYC-area airports in 2023. Since the dataset has all flights, we can treat it as a population. We’re interested in the average departure delay of flights departing from NYC area airports.\nWe’ll explore the performance of our different confidence interval procedures. To do so, you’ll first draw a sample from the flights dataset. You’ll treat this as your data sample throughout the rest of the activity, and compare your confidence intervals to the true value from the whole dataset."
  },
  {
    "objectID": "slides/16/slides16.html#results",
    "href": "slides/16/slides16.html#results",
    "title": "Confidence intervals via Bootstrap t",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/02/slides02.html#goals-for-today",
    "href": "slides/02/slides02.html#goals-for-today",
    "title": "Intro to Permutation Tests",
    "section": "Goals for today:",
    "text": "Goals for today:\n\nReview (or introduce!) hypothesis testing\nPractice with notation\nAsk questions and talk to each other\nGet comfier in R"
  },
  {
    "objectID": "slides/02/slides02.html#example",
    "href": "slides/02/slides02.html#example",
    "title": "Intro to Permutation Tests",
    "section": "Example",
    "text": "Example\nEvidence suggests that reward systems may operate in the opposite way from what is intended:\n\nRanking systems may decrease productivity;\nRewards may not stimulate learning"
  },
  {
    "objectID": "slides/02/slides02.html#experiment",
    "href": "slides/02/slides02.html#experiment",
    "title": "Intro to Permutation Tests",
    "section": "Experiment",
    "text": "Experiment\n\n47 subjects with considerable experience in creativity were recruited\nRandomly assigned to either intrinsic- or extrinsic-motivation group\nSubjects completed a questionnaire related to either intrinsic or extrinsic reasons for writing\nAll subjects were asked to write a Haiku about laughter\nPoems were scored by a panel of 12 poets, evaluated on 40-point creativity scale\n\n\n\nSource: Amabile, T. M. (1985). Motivation and creativity: Effects of motivational orientation on creative writers. Journal of Personality and Social Psychology, 48(2), 393."
  },
  {
    "objectID": "slides/02/slides02.html#results",
    "href": "slides/02/slides02.html#results",
    "title": "Intro to Permutation Tests",
    "section": "Results",
    "text": "Results\n\n\n\n\nTreatment\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\nExtrinsic\n5\n12.2\n17.2\n19.0\n24.0\n15.7\n5.3\n23\n0\n\n\nIntrinsic\n12\n17.4\n20.4\n22.3\n29.7\n19.9\n4.4\n24\n0"
  },
  {
    "objectID": "slides/02/slides02.html#the-logic-of-hypothesis-testing",
    "href": "slides/02/slides02.html#the-logic-of-hypothesis-testing",
    "title": "Intro to Permutation Tests",
    "section": "The logic of hypothesis testing",
    "text": "The logic of hypothesis testing\n\nFormulate two competing hypotheses about the population\nCalculate a test statistic summarizing the relevant information to the claims\nLook at the behavior of the test statistic assuming that the initial claim is true\nCompare the observed test statistic to the expected behavior (the distribution created in step 3).\nHow likely is it that the observed test statistic occurred by chance alone?"
  },
  {
    "objectID": "slides/02/slides02.html#permutation-resampling",
    "href": "slides/02/slides02.html#permutation-resampling",
    "title": "Intro to Permutation Tests",
    "section": "Permutation resampling",
    "text": "Permutation resampling\n\n\n\n\nIntrinsic\n\n\n12\n16.6\n19.1\n20.5\n22.1\n24\n\n\n12\n17.2\n19.3\n20.6\n22.2\n24.3\n\n\n12.9\n17.5\n19.8\n21.3\n22.6\n26.7\n\n\n13.6\n18.2\n20.3\n21.6\n23.1\n29.7\n\n\n\n\n\n\n\nExtrinsic\n\n\n5\n11.8\n15\n17.4\n18.7\n21.2\n\n\n5.4\n12\n16.8\n17.5\n19.2\n22.1\n\n\n6.1\n12.3\n17.2\n18.5\n19.5\n24\n\n\n10.9\n14.8\n17.2\n18.7\n20.7"
  },
  {
    "objectID": "slides/02/slides02.html#permutation-resampling-1",
    "href": "slides/02/slides02.html#permutation-resampling-1",
    "title": "Intro to Permutation Tests",
    "section": "Permutation resampling",
    "text": "Permutation resampling\n\n\n\n\nIntrinsic\n\n\n5\n12.9\n17.2\n18.2\n20.3\n21.3\n\n\n10.9\n13.6\n17.2\n18.7\n20.6\n22.2\n\n\n12\n14.8\n17.2\n19.2\n20.7\n23.1\n\n\n12.3\n16.8\n17.5\n19.5\n21.2\n26.7\n\n\n\n\n\n\n\nExtrinsic\n\n\n5.4\n12\n17.5\n19.3\n22.1\n24\n\n\n6.1\n15\n18.5\n19.8\n22.1\n24.3\n\n\n11.8\n16.6\n18.7\n20.5\n22.6\n29.7\n\n\n12\n17.4\n19.1\n21.6\n24"
  },
  {
    "objectID": "slides/02/slides02.html#permutation-resampling-2",
    "href": "slides/02/slides02.html#permutation-resampling-2",
    "title": "Intro to Permutation Tests",
    "section": "Permutation resampling",
    "text": "Permutation resampling\n\n\n\n\nIntrinsic\n\n\n5\n12\n17.2\n19.2\n21.3\n22.6\n\n\n6.1\n12.9\n17.5\n19.8\n21.6\n23.1\n\n\n10.9\n14.8\n17.5\n20.7\n22.1\n26.7\n\n\n12\n15\n18.7\n21.2\n22.2\n29.7\n\n\n\n\n\n\n\nExtrinsic\n\n\n5.4\n13.6\n17.2\n18.7\n20.3\n24\n\n\n11.8\n16.6\n17.4\n19.1\n20.5\n24\n\n\n12\n16.8\n18.2\n19.3\n20.6\n24.3\n\n\n12.3\n17.2\n18.5\n19.5\n22.1"
  },
  {
    "objectID": "slides/02/slides02.html#your-turn-1",
    "href": "slides/02/slides02.html#your-turn-1",
    "title": "Intro to Permutation Tests",
    "section": "Your turn",
    "text": "Your turn\nHow likely is it that the observed results occurred by chance alone?\nWhat does this say about the null hypothesis?"
  },
  {
    "objectID": "slides/02/slides02.html#your-turn-2",
    "href": "slides/02/slides02.html#your-turn-2",
    "title": "Intro to Permutation Tests",
    "section": "Your turn",
    "text": "Your turn\nHow likely is it that the observed test statistic occurred by chance alone?\nWhat does this say about the null hypothesis?"
  },
  {
    "objectID": "slides/02/slides02.html#p-value",
    "href": "slides/02/slides02.html#p-value",
    "title": "Intro to Permutation Tests",
    "section": "p-value",
    "text": "p-value\nDefinition: fraction of times the random test statistic exceeds the original test statistic\n\n\n\n\n\n\n\n\n\n\n\n\nObserved test statistic:  \\(\\overline{x}_{int} - \\overline{x}_{ext} \\approx 4.2\\)\n10 of the random test statistics exceed 4.2"
  },
  {
    "objectID": "slides/02/slides02.html#strength-of-evidence",
    "href": "slides/02/slides02.html#strength-of-evidence",
    "title": "Intro to Permutation Tests",
    "section": "Strength of evidence",
    "text": "Strength of evidence\np-values provide a continuous measurement of the strength of evidence against the null hypothesis"
  },
  {
    "objectID": "slides/02/slides02.html#permutation-test-algorithm",
    "href": "slides/02/slides02.html#permutation-test-algorithm",
    "title": "Intro to Permutation Tests",
    "section": "Permutation test algorithm",
    "text": "Permutation test algorithm\n\n\nPool the \\(m+n\\) data values\nDraw a resample of size \\(m\\) without replacement, assign these values to group 1. Assign the remaining \\(n\\) values to group 2.\nCalculate the test statistic comparing the samples from the resampled groups.\nRepeat steps 2 and 3 until we have enough samples.\nEstimate the p-value as the proportion of times the observed test statistic exceeds the original (observed) test statistic\n\\(p\\text{-value}=\\frac{\\text{# statistics that exceed the original} + 1}{\\text{# of statistics in the distribution} + 1}\\)"
  },
  {
    "objectID": "slides/02/slides02.html#where-were-going",
    "href": "slides/02/slides02.html#where-were-going",
    "title": "Intro to Permutation Tests",
    "section": "Where we’re going",
    "text": "Where we’re going\nInstead of a permutation test, we can also use facts about the sampling distribution from probability to compute p-values with math."
  },
  {
    "objectID": "slides/04/slides04.html#plan-for-today",
    "href": "slides/04/slides04.html#plan-for-today",
    "title": "Sampling Distributions + Probability Review",
    "section": "Plan for today:",
    "text": "Plan for today:\n\nHW00 probability review\nMore on sampling distributions\nIntro to Estimation"
  },
  {
    "objectID": "slides/04/slides04.html#overview-of-hw00-topics",
    "href": "slides/04/slides04.html#overview-of-hw00-topics",
    "title": "Sampling Distributions + Probability Review",
    "section": "Overview of HW00 Topics",
    "text": "Overview of HW00 Topics\n\n\nWorking with a PDF\nWorking with a CDF\nCalculating Moments\nMoments from PDF\nDeriving and Using MGFs\nRecognizing distribution from MGF\nTransformation of a Random Variable\nJoint PDF Calculations"
  },
  {
    "objectID": "slides/04/slides04.html#generally-went-ok",
    "href": "slides/04/slides04.html#generally-went-ok",
    "title": "Sampling Distributions + Probability Review",
    "section": "Generally went OK",
    "text": "Generally went OK\n\n\nWorking with a PDF\nWorking with a CDF\nCalculating Moments\nMoments from PDF\nDeriving and Using MGFs\nRecognizing distribution from MGF\nTransformation of a Random Variable\nJoint PDF Calculations"
  },
  {
    "objectID": "slides/04/slides04.html#big-mechanics-to-know-1-variable",
    "href": "slides/04/slides04.html#big-mechanics-to-know-1-variable",
    "title": "Sampling Distributions + Probability Review",
    "section": "Big mechanics to know (1 variable)",
    "text": "Big mechanics to know (1 variable)\n\nPDF \\(f_X(x)\\) gives probabilities by integrating\nCDF \\(F_X(x)\\) gives probabilities directly\nFind \\(E[X]\\) with pdf by \\(\\int_\\infty x f_X(x) dx\\)\nLoTUS: \\(E[g(X)] = \\int_\\infty g(x) f_X(x) dx\\)\n\\(V[X] = E[(X-E[X])^2] = E[X^2] - E[X]^2\\)"
  },
  {
    "objectID": "slides/04/slides04.html#big-mechanics-to-know-2-variable",
    "href": "slides/04/slides04.html#big-mechanics-to-know-2-variable",
    "title": "Sampling Distributions + Probability Review",
    "section": "Big mechanics to know (2 variable)",
    "text": "Big mechanics to know (2 variable)\n\nPDF \\(f_{X,Y}(x,y)\\) gives probabilities by integrating\nCDF \\(F_{X, Y}(x,y)\\) gives probabilities directly\nMarginal pdf: \\(f_X(x) = \\int_\\infty y f_{X,Y} dy\\)\nConditional pdf \\(f_{X|Y}(x,y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\\)\nTwo variables are independent if \\(f_{X,Y} = f_{X} f_{Y}\\) or \\(f_X = f_{X|Y}\\)"
  },
  {
    "objectID": "slides/04/slides04.html#working-with-moments",
    "href": "slides/04/slides04.html#working-with-moments",
    "title": "Sampling Distributions + Probability Review",
    "section": "Working with moments",
    "text": "Working with moments\nFor any two variables X and Y:\n\n\\(E[aX + b] = a E[X] + b\\)\n\\(V[aX + b] = a^2 V[X]\\)\n\\(E[X + Y] = E[X] + E[Y]\\)\n\\(E[aX + bY + c] = a E[X] + b E[Y] + c\\)\n\\(V[X + Y] = V[X] + V[Y] + 2 Cov(X,Y)\\)\n\\(Cov(X,Y) = E[(X - E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]\\)\n\n\nIf X and Y are independent:\n\n\\(Cov(X,Y) = 0\\)\n\\(V[X + Y] = V[X] + V[Y]\\)\n\\(V[aX + bY + c] = a^2 V[X] + b^2 V[Y]\\)"
  },
  {
    "objectID": "slides/04/slides04.html#some-issues",
    "href": "slides/04/slides04.html#some-issues",
    "title": "Sampling Distributions + Probability Review",
    "section": "Some issues",
    "text": "Some issues\n\n\nWorking with a PDF\nWorking with a CDF\nCalculating Moments\nMoments from PDF\nDeriving and Using MGFs\nRecognizing distribution from MGF\nTransformation of a Random Variable\nJoint PDF Calculations"
  },
  {
    "objectID": "slides/04/slides04.html#recap-from-friday",
    "href": "slides/04/slides04.html#recap-from-friday",
    "title": "Sampling Distributions + Probability Review",
    "section": "Recap from Friday",
    "text": "Recap from Friday\n\n\n\n\n\n\nCentral Limit Theorem\n\n\nSuppose we have an iid sample \\(X_1, ..., X_n \\sim F_x\\). The CLT tells us that, as our sample size approaches \\(\\infty\\),\n\\[F_\\bar{X}(\\bar{X}) \\to N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\]\n\n\n\n\nSampling distribution is centered at population mean\nAs \\(n \\to \\infty\\), \\(\\sigma_\\bar{X} \\to 0\\)\nIt doesn’t matter what shape \\(X_i\\) is!"
  },
  {
    "objectID": "slides/04/slides04.html#example-binomial-data",
    "href": "slides/04/slides04.html#example-binomial-data",
    "title": "Sampling Distributions + Probability Review",
    "section": "Example: Binomial data",
    "text": "Example: Binomial data\nAccording to the 2004 American Community Survey, 28% of adults over 25 years old in Utah have completed a bachelor’s degree. In a random sample of 64 adults over age 25 from Utah, what is the probability that at least 30 have a bachelor’s degree?\nLet \\(X_i\\) indicate whether a sampled person has a bachelor’s degree. Then, \\(X_1, ...., X_{64} \\sim \\text{Binom}(n=1,p=.28)\\).\n\nOr, \\(X = \\sum X_i \\sim \\text{Binom}(64, p = .28)\\)\n\n\nUsing the CLT, \\(\\bar{X} \\sim N(p, SE_\\bar{X}) \\sim N(0.28, \\sqrt{\\frac{.28(1-.28)}{64}} = .056)\\)"
  },
  {
    "objectID": "slides/04/slides04.html#example-binomial-data-1",
    "href": "slides/04/slides04.html#example-binomial-data-1",
    "title": "Sampling Distributions + Probability Review",
    "section": "Example: Binomial data",
    "text": "Example: Binomial data\n\\(\\bar{X} \\sim N(0.28, .056)\\). To find the probability that at least 30% of people in the sample have a bachelor’s degree,\n\\[P(\\hat{p} \\ge 0.30) = P(\\bar{X} \\ge 0.30)\\]\n\n\npnorm(0.30, mean = 0.28, sd = .056, lower.tail = FALSE)\n\n[1] 0.3604924\n\n\n\n\n\nNote: It’s really important to be careful about when you’re working with variance and when you’re working with standard error"
  },
  {
    "objectID": "slides/04/slides04.html#issue",
    "href": "slides/04/slides04.html#issue",
    "title": "Sampling Distributions + Probability Review",
    "section": "Issue",
    "text": "Issue\nIn this case, the CLT uses a continuous density to approximate a discrete random variable. 30% of 64 people is 19.2 – we can’t actually have 19.2 answer “yes”!\n\n\\[P(X \\ge 19.2) = P(X \\ge 20)\\]\n\n\n\\[\\sum_{k=20}^{64} \\binom{64}{k} .28^k .72^{64-k}\\]\n\n\n\n1 - pbinom(19, 64, .28)\n\n[1] 0.3242167\n\n\n\n\nThe CLT overestimates this probability by .04!"
  },
  {
    "objectID": "slides/04/slides04.html#continuity-correction",
    "href": "slides/04/slides04.html#continuity-correction",
    "title": "Sampling Distributions + Probability Review",
    "section": "Continuity Correction",
    "text": "Continuity Correction\nWhen using the CLT with discrete data, split the difference between 19 and 20:\n\\[P(X \\ge 19.2) \\approx P(X \\ge 19.5)\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat250-S25",
    "section": "",
    "text": "Landing page for Stat250 course materials in Spring 2025\n\nIn-Class Materials\nDay02: slides handout r code\nDay03: handout activity qmd\nDay04: slides | handout\nDay05: slides | handout\nDay06: slides | handout\nDay07: slides | handout\nDay08: slides | handout\nDay09: slides | handout\nDay10: handout | R activity\nDay11: handout\nDay12: Exam I corrections info\nDay13: slides | handout | R activity\nDay14: slides | handout\nDay15: handout\nDay16: handout | R activity | slides\nDay17: handout | R activity | slides\nDay18: handout |\nDay19: handout | slides\nDay20: handout\nDay21: handout\nDay22: handout | slides | Exam 02 Review\nDay23: Exam II | Exam II corrections info\nDay24: handout | slides\nDay25: handout\nDay26: handout\nDay27: handout | slides\nDay28: handout | slides\n\n\nProject\nProposal Info and Rubric\nProject Topic Ideas (Note: Will continue to be updated)\n\n\nOther Resources and Readings\n\nR Basics\nHow to do homework in R in Stat250"
  },
  {
    "objectID": "slides/05/slides05.html#section",
    "href": "slides/05/slides05.html#section",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "You are testing seeds from a new plant variety. You plant 10 seeds (n=10) and observe that 3 of them successfully germinate (k=3). Consider two hypotheses about the true germination probability (p) for this variety:\n\nHypothesis A: \\(p = 0.1\\) (10% germination rate)\nHypothesis B: \\(p = 0.4\\) (40% germination rate)\n\n\nGiven that you observed 3 germinations out of 10 seeds, which hypothesis (A or B) is more supported by the data?\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/05/slides05.html#evaluating-intuitively",
    "href": "slides/05/slides05.html#evaluating-intuitively",
    "title": "Maximum Likelihood Estimation",
    "section": "Evaluating Intuitively",
    "text": "Evaluating Intuitively"
  },
  {
    "objectID": "slides/05/slides05.html#evaluating-formally-problem-set-up",
    "href": "slides/05/slides05.html#evaluating-formally-problem-set-up",
    "title": "Maximum Likelihood Estimation",
    "section": "Evaluating Formally: Problem Set Up",
    "text": "Evaluating Formally: Problem Set Up\n\nLet \\(X\\) be the number of plants that successfully germinate\n\\(X \\sim \\text{Binom}(10, p)\\) and we observed \\(X=3\\)\n\n\n\n\n\\(P(X=3)\\) if \\(p=.1\\)\n\n\\(P(X=3)\\) if \\(p=.4\\)"
  },
  {
    "objectID": "slides/05/slides05.html#section-1",
    "href": "slides/05/slides05.html#section-1",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "In general, the likelihood of seeing \\(X=3\\) given \\(p\\) is:\n\\[{10 \\choose 3} p^3 (1-p)^7\\]\n\nWhat value of \\(p\\) maximizes this likelihood?"
  },
  {
    "objectID": "slides/05/slides05.html#methods-of-maximizing",
    "href": "slides/05/slides05.html#methods-of-maximizing",
    "title": "Maximum Likelihood Estimation",
    "section": "Methods of maximizing",
    "text": "Methods of maximizing\n\nApproximate the solution graphically\nFind a numerical approximation\nUse calculus"
  },
  {
    "objectID": "slides/05/slides05.html#approximating-with-a-graph",
    "href": "slides/05/slides05.html#approximating-with-a-graph",
    "title": "Maximum Likelihood Estimation",
    "section": "Approximating with a graph",
    "text": "Approximating with a graph"
  },
  {
    "objectID": "slides/05/slides05.html#approximating-with-a-graph-code",
    "href": "slides/05/slides05.html#approximating-with-a-graph-code",
    "title": "Maximum Likelihood Estimation",
    "section": "Approximating with a graph: code",
    "text": "Approximating with a graph: code\n\ngerm_function &lt;- function(p) {choose(10,3) * p^3 * (1-p)^7} # define function\nggplot() + \n  geom_function(fun = germ_function) +\n  xlim(c(0,1))"
  },
  {
    "objectID": "slides/05/slides05.html#approximating-numerically",
    "href": "slides/05/slides05.html#approximating-numerically",
    "title": "Maximum Likelihood Estimation",
    "section": "Approximating numerically",
    "text": "Approximating numerically\n\noptimize(germ_function, interval = c(0,1), maximum = TRUE)\n\n\n\n$maximum\n[1] 0.3000157\n\n$objective\n[1] 0.2668279"
  },
  {
    "objectID": "slides/05/slides05.html#with-calculus",
    "href": "slides/05/slides05.html#with-calculus",
    "title": "Maximum Likelihood Estimation",
    "section": "With calculus",
    "text": "With calculus"
  },
  {
    "objectID": "slides/05/slides05.html#a-trick",
    "href": "slides/05/slides05.html#a-trick",
    "title": "Maximum Likelihood Estimation",
    "section": "A “trick”",
    "text": "A “trick”\nProbability functions often are tricky to differentiate because of the product rule. Lucky for us, we can maximize the log instead."
  },
  {
    "objectID": "slides/05/slides05.html#can-we-generalize-to-any-data",
    "href": "slides/05/slides05.html#can-we-generalize-to-any-data",
    "title": "Maximum Likelihood Estimation",
    "section": "Can we generalize to any data?",
    "text": "Can we generalize to any data?"
  },
  {
    "objectID": "slides/05/slides05.html#section-2",
    "href": "slides/05/slides05.html#section-2",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Likelihood function\n\n\nLet \\(f(x; \\theta)\\) denote the probability mass function for a discrete distribution with associated parameter \\(\\theta\\). Suppose \\(X_1, X_2, ... , X_n\\) are a random sample from this distribution and \\(x_1, x_2, ... , x_n\\) are the actual observed values. Then, the likelihood function of \\(\\theta\\) is:\n  \n\n\n\n\n\n\n\n\n\n\nMaximum likelihood estimate\n\n\nA maximum likelihood estimate (MLE), \\(\\hat{\\theta}_{MLE}\\) is the value of \\(\\theta\\) that maximizes the likelihood function, or equivalently, that maximizes the log-likelihood \\(\\ln L(\\theta)\\)"
  },
  {
    "objectID": "slides/05/slides05.html#bernoulli-data",
    "href": "slides/05/slides05.html#bernoulli-data",
    "title": "Maximum Likelihood Estimation",
    "section": "Bernoulli data",
    "text": "Bernoulli data\nAn alternative way to express this example is \\(X_1, ..., X_n \\sim \\text{Bernoulli}(\\theta)\\)"
  },
  {
    "objectID": "slides/05/slides05.html#is-the-likelihood-also-a-pdf",
    "href": "slides/05/slides05.html#is-the-likelihood-also-a-pdf",
    "title": "Maximum Likelihood Estimation",
    "section": "Is the likelihood also a PDF?",
    "text": "Is the likelihood also a PDF?\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/05/slides05.html#aside-notation",
    "href": "slides/05/slides05.html#aside-notation",
    "title": "Maximum Likelihood Estimation",
    "section": "Aside: notation",
    "text": "Aside: notation\n\n\n\n\n\n\nEstimator\n\n\n  \n\n\n\n\n\n\n\n\n\nEstimate"
  },
  {
    "objectID": "slides/05/slides05.html#section-3",
    "href": "slides/05/slides05.html#section-3",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Let \\(X_1, ..., X_n\\) be an iid random sample from a distribution with PDF\n\\[f(x|\\theta)= (\\theta + 1)x^\\theta, 0 \\le x \\le 1\\]\n\n\nFind the maximum likelihood estimator for a random sample of size \\(n\\) using calculus.\nSuppose we observe a sample of size 5: {.83, .49, .72, .57, .66}. Find the maximum likelihood estimate and verify with a graph or numerical approximation"
  },
  {
    "objectID": "slides/27/slides27.html#question-1",
    "href": "slides/27/slides27.html#question-1",
    "title": "Intro to Bayesian Inference",
    "section": "Question 1",
    "text": "Question 1\nWhen flipping a fair coin, we say that “the probability of flipping Heads is 0.5.” How do you interpret this probability?\n\n\nIf I flip this coin over and over, roughly 50% will be Heads.\nHeads and Tails are equally plausible.\nBoth a and b make sense."
  },
  {
    "objectID": "slides/27/slides27.html#question-2",
    "href": "slides/27/slides27.html#question-2",
    "title": "Intro to Bayesian Inference",
    "section": "Question 2",
    "text": "Question 2\nAn election is coming up and a pollster claims that “candidate A has a 0.9 probability of winning.” How do you interpret this probability?\n\n\nIf we observe the election over and over, candidate A will win roughly 90% of the time.\nCandidate A is much more likely to win than to lose.\nThe pollster’s calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1."
  },
  {
    "objectID": "slides/27/slides27.html#question-3",
    "href": "slides/27/slides27.html#question-3",
    "title": "Intro to Bayesian Inference",
    "section": "Question 3",
    "text": "Question 3\nConsider two claims.\n\n\nLou claims that he can predict the outcome of a coin flip. To test his claim, you flip a fair coin 10 times and he correctly predicts all 10.\nKavya claims that she can distinguish natural and artificial sweeteners. To test her claim, you give her 10 sweetener samples and she correctly identifies each.\n\n\nIn light of these experiments, what do you conclude?\n\n\nYou’re more confident in Kavya’s claim than Lou’s claim.\nThe evidence supporting Lou’s claim is just as strong as the evidence supporting Kavya’s claim."
  },
  {
    "objectID": "slides/27/slides27.html#question-4",
    "href": "slides/27/slides27.html#question-4",
    "title": "Intro to Bayesian Inference",
    "section": "Question 4",
    "text": "Question 4\nSuppose that during a recent doctor’s visit, you tested positive for a very rare disease. If you only get to ask the doctor one question, which would it be?\n\n\nWhat’s the chance that I actually have the disease?\nIf in fact I don’t have the disease, what’s the chance that I would’ve gotten this positive test result?"
  },
  {
    "objectID": "slides/27/slides27.html#tally-your-points",
    "href": "slides/27/slides27.html#tally-your-points",
    "title": "Intro to Bayesian Inference",
    "section": "Tally your points",
    "text": "Tally your points\n\n\nQuestion 1:\n\n1 = 1 points\n2 = 3 points\n3 = 2 points\n\nQuestion 2:\n\n1 = 1 points\n2 = 3 points\n3 = 1 points\n\n\nQuestion 3:\n\n1 = 3 points\n2 = 1 points  \n\nQuestion 4:\n\n1 = 3 points\n2 = 1 points"
  },
  {
    "objectID": "slides/27/slides27.html#what-does-your-score-mean",
    "href": "slides/27/slides27.html#what-does-your-score-mean",
    "title": "Intro to Bayesian Inference",
    "section": "What does your score mean?",
    "text": "What does your score mean?\n\n4-5 \\(\\rightarrow\\) you’re more of a frequentist thinker\n6-8 \\(\\rightarrow\\) you see the merit in both (a pragmatist?)\n9-12 \\(\\rightarrow\\) you’re more of a Bayesian thinker"
  },
  {
    "objectID": "slides/27/slides27.html#section",
    "href": "slides/27/slides27.html#section",
    "title": "Intro to Bayesian Inference",
    "section": "",
    "text": "Question 1: Interpreting probability\nWhen flipping a fair coin, we say that “the probability of flipping Heads is 0.5.” How do you interpret this probability?\n\n\n(Frequentist) If I flip this coin over and over, roughly 50% will be Heads.\n(Bayesian) Heads and Tails are equally plausible.\nBoth a and b make sense."
  },
  {
    "objectID": "slides/27/slides27.html#section-1",
    "href": "slides/27/slides27.html#section-1",
    "title": "Intro to Bayesian Inference",
    "section": "",
    "text": "Question 2: Interpreting probability\nAn election is coming up and a pollster claims that “candidate A has a 0.9 probability of winning.” How do you interpret this probability?\n\n\n(Frequentist) If we observe the election over and over, candidate A will win roughly 90% of the time.\n(Bayesian) Candidate A is much more likely to win than to lose.\n(Rabid frequentist) The pollster’s calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1."
  },
  {
    "objectID": "slides/27/slides27.html#section-2",
    "href": "slides/27/slides27.html#section-2",
    "title": "Intro to Bayesian Inference",
    "section": "",
    "text": "Question 3: Balancing prior info and observed data\n\nConsider two claims.\n\n\nLou claims that he can predict the outcome of a coin flip. To test his claim, you flip a fair coin 10 times and he correctly predicts all 10.\nKavya claims that she can distinguish natural and artificial sweeteners. To test her claim, you give her 10 sweetener samples and she correctly identifies each.\n\n\nIn light of these experiments, what do you conclude?\n\n\n(Bayesian) You’re more confident in Kavya’s claim than Lou’s claim.\n(Frequentist) The evidence supporting Lou’s claim is just as strong as the evidence supporting Kavya’s claim."
  },
  {
    "objectID": "slides/27/slides27.html#section-3",
    "href": "slides/27/slides27.html#section-3",
    "title": "Intro to Bayesian Inference",
    "section": "",
    "text": "Question 4: Asking questions\nSuppose that during a recent doctor’s visit, you tested positive for a very rare disease. If you only get to ask the doctor one question, which would it be?\n\n\n(Bayesian) What’s the chance that I actually have the disease?\n(Frequentist) If in fact I don’t have the disease, what’s the chance that I would’ve gotten this positive test result?"
  },
  {
    "objectID": "slides/27/slides27.html#example",
    "href": "slides/27/slides27.html#example",
    "title": "Intro to Bayesian Inference",
    "section": "Example:",
    "text": "Example:\nA Des Moines register poll a few days before the 2024 Presidential election showed Kamala Harris with 51.61% of the 2-party vote share in a poll of \\(n=808\\) likely voters. This poll result received a lot of buzz, because in the two months leading up to this poll, this proportion was estimated between 44.7 and 47.7.\n\nNov 2: 51.5% (x = 417; n=808)\nNov 2: 44.7% (x = 358; n=800)\nOct 2: 46.8% (x=281; n=600)\nSept 15: 47.7% (x=382; n=800)"
  },
  {
    "objectID": "slides/27/slides27.html#overview-of-the-bayesian-method",
    "href": "slides/27/slides27.html#overview-of-the-bayesian-method",
    "title": "Intro to Bayesian Inference",
    "section": "Overview of the Bayesian method",
    "text": "Overview of the Bayesian method\n\nChoose (or elicit) a probability distribution to express the pre-data belief about the parameter of interest, \\(\\theta\\).\nChoose a model for the data given \\(\\theta\\).\nObserve data, \\(Y_1, \\ldots, Y_n\\).\nUpdate the belief about \\(\\theta\\) by combining the prior belief and the data.\nDraw inferences using this updated belief about \\(\\theta\\)."
  },
  {
    "objectID": "slides/27/slides27.html#section-4",
    "href": "slides/27/slides27.html#section-4",
    "title": "Intro to Bayesian Inference",
    "section": "",
    "text": "Overview of the Bayesian method - math\n\nAssume a prior distribution: \\(\\theta \\sim f_\\theta(\\theta)\\)\nChoose a likelihood function for the data: \\(Y|\\theta \\sim f_{y|\\theta}(y|\\theta)\\)\nObserve data, \\(Y_1, \\ldots, Y_n\\).\nFind the posterior distribution of \\(\\theta\\) given the data: \\(\\theta \\mid Y \\sim f_{\\theta \\mid Y}(\\theta \\mid Y)\\)\nDraw inferences with \\(f_{\\theta|Y}\\)"
  },
  {
    "objectID": "slides/27/slides27.html#choosing-a-data-model",
    "href": "slides/27/slides27.html#choosing-a-data-model",
    "title": "Intro to Bayesian Inference",
    "section": "Choosing a data model",
    "text": "Choosing a data model\n\nLikelihood: a model for our data \\(X_1, \\dotsc, X_n\\)\n\\(X_i\\) is drawn from a population/distribution with pdf/pmf \\[X_i \\sim f(x_i | \\theta)\\]\nThe joint probability model for all \\(n\\) data values (likelihood function) \\[f(x_1, \\dotsc, x_n \\mid \\theta) = \\prod_{i=1}^n f(x_i | \\theta)\\]\nOur inference goal will be to derive a model our unknown parameter(s) \\(\\theta\\) that is informed by our observed data values"
  },
  {
    "objectID": "slides/27/slides27.html#choosing-a-prior",
    "href": "slides/27/slides27.html#choosing-a-prior",
    "title": "Intro to Bayesian Inference",
    "section": "Choosing a prior",
    "text": "Choosing a prior\n\nPrior: we give \\(\\theta\\) an initial probability model that reflects our beliefs about \\(\\theta\\) prior to observing our data \\[\\theta \\sim f(\\theta)\\]\nThe model \\(f(\\theta)\\) can be either a pdf or pmf\nand \\(f(\\theta)\\) should be defined on reasonable values of \\(\\theta\\)\n\ne.g. \\(\\theta = p\\) is a probability of success: then \\(f\\) could be Unif[0,1]\ne.g. \\(\\theta = \\sigma^2\\) from a half-normal: then \\(f\\) could be a Gamma model"
  },
  {
    "objectID": "slides/27/slides27.html#finding-the-posterior",
    "href": "slides/27/slides27.html#finding-the-posterior",
    "title": "Intro to Bayesian Inference",
    "section": "Finding the posterior",
    "text": "Finding the posterior\n\nPosterior: update our prior \\(f\\) to reflect the information about \\(\\theta\\) that is provided by our data \\(x_1, \\dotsc, x_n\\) \\[\\theta \\mid  x_1, \\dotsc, x_n \\sim f(\\theta \\mid x_1, \\dotsc, x_n)\\]\nWe compute our conditional posterior distribution using Bayes theorem: \\[f(\\theta \\mid  x_1, \\dotsc, x_n) = \\dfrac{f(\\theta, x_1, \\dotsc, x_n)}{f(x_1, \\dotsc, x_n)} = \\dfrac{f(\\theta)f(x_1, \\dotsc, x_n \\mid \\theta)}{f(x_1, \\dotsc, x_n)}\\]\nthe first expression is our definition of conditional probability\nthe second expression is “Bayes theorem” which tells us (in words): \\[\\textrm{posterior} = \\dfrac{\\textrm{prior} \\times \\textrm{data }}{\\textrm{marginal  of data}}\\]"
  },
  {
    "objectID": "slides/27/slides27.html#finding-the-posterior-1",
    "href": "slides/27/slides27.html#finding-the-posterior-1",
    "title": "Intro to Bayesian Inference",
    "section": "Finding the posterior",
    "text": "Finding the posterior\n\\[f(\\theta \\mid  x_1, \\dotsc, x_n)  = \\dfrac{f(\\theta)f(x_1, \\dotsc, x_n \\mid \\theta)}{f(x_1, \\dotsc, x_n)} = \\dfrac{\\textrm{prior} \\times \\textrm{data }}{\\textrm{marginal  of data}}\\]\n\nRecall from probability: We find a marginal probability by integrating “out” nuisance variables from a joint distribution \\[\\textrm{marginal pdf of data} = f(x_1, \\dotsc, x_n) = \\int_{-\\infty}^{\\infty} f(\\theta)f(x_1, \\dotsc, x_n \\mid \\theta) d\\theta\\]"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-prior",
    "href": "slides/27/slides27.html#binomial-example-prior",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: prior",
    "text": "Binomial example: prior\n\nGoal is to estimate the parameter: \\[p = \\textrm{proportion of Iowa likely voters who will vote for Harris}\\]\nPrior: A natural prior for a proportion is a uniform: \\[p \\sim Unif[0,1] \\textrm{ so that } f(p) = 1 \\textrm{ for } 0 \\leq p \\leq 1\\]"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-data",
    "href": "slides/27/slides27.html#binomial-example-data",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: data",
    "text": "Binomial example: data\n\nData 808 respondents who responded to the 2-party presidential vote question.\nOur random sample \\(X_1, \\dotsc, X_{808}\\) is Bernoulli: \\[f(x_1, \\dotsc, x_n \\mid p) = \\prod_{i=1}^{808}p^{x_i}(1-p)^{(1-x_i)} = p^{\\sum_{i=1}^{808}x_i}(1-p)^{808 - \\sum_{i=1}^{808}x_i}\\]\nor, an equivalent data model just models the total number of “success” (approves) \\(X = \\sum_{i=1}^{808}X_i\\) which is a Binomial count: \\[f(x \\mid p) = \\binom{808}{x}p^x(1-p)^{808-x}\\]"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior",
    "href": "slides/27/slides27.html#binomial-example-posterior",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\nPosterior We need to derive the following pdf for \\(p\\): \\[f(p \\mid  x)  = \\dfrac{f(\\theta)f(x \\mid p)}{f(x)} = \\dfrac{1 \\times \\binom{n}{x}p^x(1-p)^{n-x}}{\\int_{0}^1 1 \\times \\binom{n}{x}p^x(1-p)^{n-x} dp} \\textrm{ for } 0 \\leq p \\leq 1\\]"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-1",
    "href": "slides/27/slides27.html#binomial-example-posterior-1",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\nThe toughest part of any posterior calculation is usually the marginal integration: \\[f(x) = \\int_{0}^1\\binom{n}{x}p^x(1-p)^{n-x} dp\\]\nAny terms not involving \\(p\\) can come outside the integrand: \\[f(x) = \\binom{n}{x} \\int_{0}^1p^x(1-p)^{n-x} dp\\]\n…which still looks bad, until you remember the Beta distribution which tells us that a Beta pdf integrates to 1 over the interval [0,1]: \\[1 =  \\int_{0}^1\\dfrac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)}p^{a-1}(1-p)^{b-1} dp\\]"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-2",
    "href": "slides/27/slides27.html#binomial-example-posterior-2",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\n…which allows us to solve our integration problem without actually doing the integral \\[ \\dfrac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a + b)} = \\int_{0}^1p^{a-1}(1-p)^{b-1} dp\\]\nLetting \\(a = x + 1\\) and \\(b=n - x + 1\\), we have \\[f(x) = \\binom{n}{x} \\int_{0}^1p^x(1-p)^{n-x} dp = \\binom{n}{x}  \\dfrac{\\Gamma(x + 1)\\Gamma(n - x + 1)}{\\Gamma(n + 2)}\\]"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-3",
    "href": "slides/27/slides27.html#binomial-example-posterior-3",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\n…then for \\(0 \\leq p \\leq 1\\) \\[f(p \\mid  x)  = \\dfrac{\\binom{n}{x}p^x(1-p)^{n-x}}{\\binom{n}{x}  \\dfrac{\\Gamma(x + 1)\\Gamma(n - x + 1)}{\\Gamma(n + 2)}} = \\dfrac{\\Gamma(n + 2)}{\\Gamma(x + 1)\\Gamma(n - x + 1)} p^x(1-p)^{n-x}\\]\nWhich is the pdf for Beta distribution: \\[p \\mid x \\sim Beta(x + 1, n - x + 1)\\]\nIowa data: our posterior distribution for \\(n=808\\) and \\(x=417\\) is \\[p \\mid x = 417 \\sim Beta(418, 392)\\]"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-4",
    "href": "slides/27/slides27.html#binomial-example-posterior-4",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\n\n\nInferences about \\(p\\) use the Beta(391, 611) distribution.\n\nWhat values of \\(p\\) are most probable? (interval)\nWhat is the expected value of \\(p\\)? (point estimate)\nHow much uncertainty do we have in the distribution of \\(p\\)? (SE)"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-5",
    "href": "slides/27/slides27.html#binomial-example-posterior-5",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\\[p \\mid x = 417 \\sim Beta(418, 392)\\]\n\nPoint Estimate: \\(\\hat{p}_{bayes} = E(p \\mid x=417) = \\dfrac{418}{418 + 392} = \\dfrac{418}{810} = 0.5160\\)\nVariation: \\(SD(p \\mid x=417) = \\sqrt{V(p \\mid x=417)} = \\sqrt{\\dfrac{418(392)}{(418 + 392)^2(418 + 392 +1)}} = 0.0003\\)\nCredible Interval for \\(p\\): A common choice of \\(100(1-\\alpha)\\)% interval estimates is to use the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of the posterior\n\n\n\nqbeta(c(.025,.975),418,392)  # 95% credible interval for p\n\n[1] 0.4816267 0.5503970"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-6",
    "href": "slides/27/slides27.html#binomial-example-posterior-6",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\nWhat if we used classical frequentist methods instead of our Bayesian model?\nBayes Point Estimate of \\(p\\): Our posterior expectation for \\(p\\) is \\(\\hat{p}_{bayes} =0.5160\\)\nFrequentist Point Estimate of \\(p\\): MLE of \\(p\\) is \\[\\hat{p}_{MLE} = \\dfrac{x}{n} = \\dfrac{417}{808} = 0.5161\\]"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-7",
    "href": "slides/27/slides27.html#binomial-example-posterior-7",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\nWhy are the Bayes estimate and MLE so close?\nRewrite \\(\\hat{p}_{bayes}\\): \\[\\hat{p}_{bayes} = \\dfrac{x+1}{n + 2} = \\dfrac{n}{n+2}\\dfrac{x}{n} + \\dfrac{2}{n+2}\\dfrac{1}{2} = \\dfrac{n}{n+2}\\hat{p}_{MLE} + \\dfrac{2}{n+2}E(p)\\]\nThe Bayes estimate is a weighted average of the MLE and prior expectation\nThe Unif[0,1] prior is like adding 2 units to the sample: 1 success and 1 failure so we have a prior expectation of 50% for \\(p\\)\n\nweight \\(n/(n+2)\\) favors the likelihood when \\(n &gt; &gt; 2\\) (data size is much bigger than prior sample addition)\nweight \\(2/(n+2)\\) gives more weight to the prior mean when \\(n\\) is closer to 2"
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-8",
    "href": "slides/27/slides27.html#binomial-example-posterior-8",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\n\nBayes SD: \\(\\sqrt{V(p \\mid x=390)} =  0.01754\\)\nFrequentist SE: The sample proportion SE is \\[SE(\\hat{p}) = \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}} = \\sqrt{\\dfrac{0.39(1-0.39)}{1000}}=0.01758\\] Same as the Bayes SD up to 4 decimal spots!\nWe can write the posterior variance as \\[V(p \\mid x) = \\dfrac{(x+1)(n-x+1)}{(n+2)^2(n+2 +1)} = \\dfrac{\\hat{p}_{bayes}(1-\\hat{p}_{bayes})}{n+2 +1}\\] which is similar in form as the frequentist SE."
  },
  {
    "objectID": "slides/27/slides27.html#binomial-example-posterior-9",
    "href": "slides/27/slides27.html#binomial-example-posterior-9",
    "title": "Intro to Bayesian Inference",
    "section": "Binomial example: posterior",
    "text": "Binomial example: posterior\nBayes Credible Interval for \\(p\\): There is a 95% probability that the Harris two-party vote share is between 48.16% and 55.04%.\n\nqbeta(c(.025,.975),418,392)  # 95% credible interval for p\n\n[1] 0.4816267 0.5503970\n\n\n\nFrequentist Confidence Interval for \\(p\\): I am 95% confident that he Harris two-party vote share is between 48.1% and 55.1%.\n\nprop.test(417,808)$conf  # 95% confidence interval for p\n\n[1] 0.4810195 0.5510037\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nIntervals contain very similar values, but are interpreted very differently!"
  },
  {
    "objectID": "slides/27/slides27.html#what-if-we-chose-a-different-prior",
    "href": "slides/27/slides27.html#what-if-we-chose-a-different-prior",
    "title": "Intro to Bayesian Inference",
    "section": "What if we chose a different prior?",
    "text": "What if we chose a different prior?\nIn the three polls leading up to the Des Moines register poll, Harris’ 2-party vote share hovered between 44.7 and 47.7 percent. What if my prior distribution took that information into account?"
  },
  {
    "objectID": "slides/27/slides27.html#what-if-we-chose-a-different-prior-1",
    "href": "slides/27/slides27.html#what-if-we-chose-a-different-prior-1",
    "title": "Intro to Bayesian Inference",
    "section": "What if we chose a different prior?",
    "text": "What if we chose a different prior?\n\nA more flexible prior for \\(p\\) is a Beta( \\(\\alpha, \\beta\\) ) distribution: \\[f(p) = \\dfrac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha - 1}(1-p)^{\\beta-1} \\textrm{ for } 0 \\leq p \\leq 1\\]\nThe posterior then looks like: \\[f(p \\mid  x)   = \\dfrac{\\dfrac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha - 1}(1-p)^{\\beta-1} \\times \\binom{n}{x}p^x(1-p)^{n-x}}{f(x)} \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  = \\dfrac{\\dfrac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\binom{n}{x} }{f(x)}p^{x + \\alpha - 1}(1-p)^{n - x + \\beta -1}  \\textrm{ for } 0 \\leq p \\leq 1\\]"
  },
  {
    "objectID": "slides/27/slides27.html#beta-binomial-model",
    "href": "slides/27/slides27.html#beta-binomial-model",
    "title": "Intro to Bayesian Inference",
    "section": "Beta-Binomial model",
    "text": "Beta-Binomial model\n\nThe kernel of the posterior (the part that involves \\(p\\)) looks like: \\[f(p \\mid  x) \\propto  p^{x + \\alpha - 1}(1-p)^{n - x + \\beta -1}  \\textrm{ for } 0 \\leq p \\leq 1\\]\nSince \\(f(p \\mid x)\\) must integrate to 1 over [0,1], the kernel uniquely identifies the pdf as a Beta( \\(x + \\alpha, n-x+\\beta\\) )\n\nwhich means the normalizing constant for this kernel is \\[\\dfrac{\\Gamma(n + \\alpha + \\beta)}{\\Gamma(x + \\alpha)\\Gamma(n-x+\\beta)} = \\dfrac{\\dfrac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\binom{n}{x} }{f(x)}\\]\nwe just need to look at \\[f(\\theta \\mid x_1, \\dotsc, x_n) \\propto f(\\theta)f(x_1, \\dotsc, x_n \\mid \\theta)\\]"
  },
  {
    "objectID": "slides/27/slides27.html#beta-binomial-model-1",
    "href": "slides/27/slides27.html#beta-binomial-model-1",
    "title": "Intro to Bayesian Inference",
    "section": "Beta-Binomial model",
    "text": "Beta-Binomial model\n\nData: \\(X \\mid p \\sim Binom(n,p)\\)\nPrior: \\(p \\sim Beta(\\alpha, \\beta)\\)\nPrior Expectation: \\(E(p) = \\dfrac{\\alpha}{\\alpha + \\beta}\\)\nPosterior: \\(p \\mid x \\sim Beta(x + \\alpha, n-x + \\beta)\\)\nPosterior Expectation: \\(E(p \\mid x) = \\dfrac{x + \\alpha}{n + \\alpha + \\beta} = \\dfrac{n}{n+\\alpha + \\beta}\\hat{p}_{MLE} + \\dfrac{\\alpha + \\beta}{n+\\alpha + \\beta}E(p)\\)\nThe prior is like adding \\(\\alpha\\) succeses and \\(\\beta\\) failures to the data.\n\nLarger values mean more weight given to the prior (success and/or failures)"
  },
  {
    "objectID": "slides/27/slides27.html#beta-binomial-model-flat-prior",
    "href": "slides/27/slides27.html#beta-binomial-model-flat-prior",
    "title": "Intro to Bayesian Inference",
    "section": "Beta-Binomial model: flat prior",
    "text": "Beta-Binomial model: flat prior"
  },
  {
    "objectID": "slides/27/slides27.html#beta-binomial-model-somewhat-informative-prior",
    "href": "slides/27/slides27.html#beta-binomial-model-somewhat-informative-prior",
    "title": "Intro to Bayesian Inference",
    "section": "Beta-Binomial model: somewhat informative prior",
    "text": "Beta-Binomial model: somewhat informative prior"
  },
  {
    "objectID": "slides/27/slides27.html#beta-binomial-model-extremely-informative-prior",
    "href": "slides/27/slides27.html#beta-binomial-model-extremely-informative-prior",
    "title": "Intro to Bayesian Inference",
    "section": "Beta-Binomial model: extremely informative prior",
    "text": "Beta-Binomial model: extremely informative prior"
  },
  {
    "objectID": "slides/27/slides27.html#beta-binomial-model-2",
    "href": "slides/27/slides27.html#beta-binomial-model-2",
    "title": "Intro to Bayesian Inference",
    "section": "Beta-Binomial model",
    "text": "Beta-Binomial model\nFor \\(n = 808\\) and \\(x=417\\) successes:\n\n\n      model alpha beta      mean      mode          var         sd\n1     prior     1    1 0.5000000       NaN 0.0833333333 0.28867513\n2 posterior   418  392 0.5160494 0.5160891 0.0003079438 0.01754833\n\n\n      model alpha beta      mean      mode          var         sd\n1     prior   140  160 0.4666667 0.4664430 0.0008268734 0.02875541\n2 posterior   557  551 0.5027076 0.5027125 0.0002254217 0.01501405\n\n\n      model alpha beta      mean      mode          var          sd\n1     prior  1021 1179 0.4640909 0.4640582 1.129989e-04 0.010630093\n2 posterior  1438 1570 0.4780585 0.4780439 8.292408e-05 0.009106266\n\n\nAs our prior contains a bigger “sample size”, we see - a posterior mean pulled more towards the prior mean - a reduction in the posterior standard deviation"
  },
  {
    "objectID": "slides/27/slides27.html#comparing-intervals",
    "href": "slides/27/slides27.html#comparing-intervals",
    "title": "Intro to Bayesian Inference",
    "section": "Comparing intervals",
    "text": "Comparing intervals\nIf we compare credible intervals for the 3 posteriors:\n\nqbeta(c(.025,.975),418,392)  \n\n[1] 0.4816267 0.5503970\n\nqbeta(c(.025,.975),557,551)  \n\n[1] 0.4732816 0.5321244\n\nqbeta(c(.025,.975),1438,1570)  \n\n[1] 0.4602256 0.4959190\n\n\nNote: on election day, Harris received 43.27% of the two-party vote share"
  },
  {
    "objectID": "slides/28/slides28.html#why-are-we-here",
    "href": "slides/28/slides28.html#why-are-we-here",
    "title": "Inference Overview",
    "section": "Why are we here",
    "text": "Why are we here"
  },
  {
    "objectID": "slides/28/slides28.html#two-pillars-of-stat-inference",
    "href": "slides/28/slides28.html#two-pillars-of-stat-inference",
    "title": "Inference Overview",
    "section": "Two pillars of stat inference:",
    "text": "Two pillars of stat inference:"
  },
  {
    "objectID": "slides/28/slides28.html#estimation-evaluating-point-estimates",
    "href": "slides/28/slides28.html#estimation-evaluating-point-estimates",
    "title": "Inference Overview",
    "section": "Estimation: evaluating point estimates",
    "text": "Estimation: evaluating point estimates"
  },
  {
    "objectID": "slides/28/slides28.html#estimation-techniques-for-uncertainty",
    "href": "slides/28/slides28.html#estimation-techniques-for-uncertainty",
    "title": "Inference Overview",
    "section": "Estimation: techniques for uncertainty",
    "text": "Estimation: techniques for uncertainty"
  },
  {
    "objectID": "slides/28/slides28.html#hypothesis-testing-setup",
    "href": "slides/28/slides28.html#hypothesis-testing-setup",
    "title": "Inference Overview",
    "section": "Hypothesis Testing: Setup",
    "text": "Hypothesis Testing: Setup"
  },
  {
    "objectID": "slides/28/slides28.html#hypothesis-testing-errors",
    "href": "slides/28/slides28.html#hypothesis-testing-errors",
    "title": "Inference Overview",
    "section": "Hypothesis Testing: Errors",
    "text": "Hypothesis Testing: Errors"
  },
  {
    "objectID": "slides/28/slides28.html#flavors-of-inference",
    "href": "slides/28/slides28.html#flavors-of-inference",
    "title": "Inference Overview",
    "section": "“Flavors” of Inference",
    "text": "“Flavors” of Inference"
  },
  {
    "objectID": "slides/19/slides19.html#recap",
    "href": "slides/19/slides19.html#recap",
    "title": "Testing Errors and Power",
    "section": "Recap",
    "text": "Recap\n\nLast time, we covered CLT-based hypothesis tests\n\nDifference in two means\nOne mean\nOne proportion\n\nToday, we’re going to dive into some of the choices we can make when setting up different tests, and how we evaluate if our test is “performing well”"
  },
  {
    "objectID": "slides/19/slides19.html#section",
    "href": "slides/19/slides19.html#section",
    "title": "Testing Errors and Power",
    "section": "",
    "text": "Example: A high school was chosen to participate in the evaluation of a new geometry and algebra curriculum. In the recent past, the school’s students were considered “typical”, receiving scores on standardized tests that were very close to the nationwide average. In the year of the study, 86 sophomores were randomly selected to participate in a special set of classes that integrated geometry and algebra. Those students averaged 502 on the SAT-I math exam; the nationwide average was 494 with a standard deviation of 124.\nAdministrator A thinks that an average above 500 indicates a big enough improvement to warrant a change in curriculum, but Administrator B thinks that the average should be above 600. If there was actually no improvement, what’s the probability we come to the wrong conclusion under Administrator A and B’s cutoffs?"
  },
  {
    "objectID": "slides/19/slides19.html#section-2",
    "href": "slides/19/slides19.html#section-2",
    "title": "Testing Errors and Power",
    "section": "",
    "text": "Decision Rule\n     \nMapping to Z Score"
  },
  {
    "objectID": "slides/19/slides19.html#section-3",
    "href": "slides/19/slides19.html#section-3",
    "title": "Testing Errors and Power",
    "section": "",
    "text": "Critical Region\n\n\n  \n\n\n\nSetting a significance level"
  },
  {
    "objectID": "slides/19/slides19.html#types-of-errors",
    "href": "slides/19/slides19.html#types-of-errors",
    "title": "Testing Errors and Power",
    "section": "Types of Errors",
    "text": "Types of Errors\nIn any hypothesis test procedure, there are two ways we can be wrong: we can (1) conclude \\(H_0\\) is true when \\(H_1\\) is actually true, or we can (2) conclude \\(H_0\\) is false when \\(H_0\\) is actually true.\n\n\n\n\n\\(H_0\\) True\n\\(H_1\\) True\n\n\n\n\nReject \\(H_0\\)\nType I Error\nCorrect\n\n\nFail to reject \\(H_0\\)\nCorrect\nType II Error"
  },
  {
    "objectID": "slides/19/slides19.html#section-4",
    "href": "slides/19/slides19.html#section-4",
    "title": "Testing Errors and Power",
    "section": "",
    "text": "Image credit: Allison Horst"
  },
  {
    "objectID": "slides/19/slides19.html#section-5",
    "href": "slides/19/slides19.html#section-5",
    "title": "Testing Errors and Power",
    "section": "",
    "text": "Image credit: Allison Horst"
  },
  {
    "objectID": "slides/19/slides19.html#type-i-and-type-ii-errors",
    "href": "slides/19/slides19.html#type-i-and-type-ii-errors",
    "title": "Testing Errors and Power",
    "section": "Type I and type II errors",
    "text": "Type I and type II errors\n\nHow could we decrease the Type I error rate?"
  },
  {
    "objectID": "slides/19/slides19.html#smaller-type-i-to-bigger-type-ii",
    "href": "slides/19/slides19.html#smaller-type-i-to-bigger-type-ii",
    "title": "Testing Errors and Power",
    "section": "Smaller Type I \\(\\to\\) bigger Type II",
    "text": "Smaller Type I \\(\\to\\) bigger Type II"
  },
  {
    "objectID": "slides/19/slides19.html#effect-size-and-type-ii-error",
    "href": "slides/19/slides19.html#effect-size-and-type-ii-error",
    "title": "Testing Errors and Power",
    "section": "Effect size and type II error",
    "text": "Effect size and type II error"
  },
  {
    "objectID": "slides/19/slides19.html#se-and-type-ii-error",
    "href": "slides/19/slides19.html#se-and-type-ii-error",
    "title": "Testing Errors and Power",
    "section": "SE and type II error",
    "text": "SE and type II error\n\nLarger SESmaller SE"
  },
  {
    "objectID": "slides/19/slides19.html#section-6",
    "href": "slides/19/slides19.html#section-6",
    "title": "Testing Errors and Power",
    "section": "",
    "text": "Power\n\n\n   \n\n\n\nExample: If the true \\(\\mu\\) in the Math curriculum example is 498 (so \\(H_0\\) is false), what is the power of the test of Administrator A? What if true \\(\\mu\\) is 510?"
  },
  {
    "objectID": "slides/19/slides19.html#section-7",
    "href": "slides/19/slides19.html#section-7",
    "title": "Testing Errors and Power",
    "section": "",
    "text": "Power Function\n\n\n   \n\n\n\nNote: What does an ideal power function look like?\n\nif \\(\\theta = \\theta_0\\)\nif \\(\\theta \\in \\Omega_A\\)"
  },
  {
    "objectID": "slides/19/slides19.html#example-math-curriculum",
    "href": "slides/19/slides19.html#example-math-curriculum",
    "title": "Testing Errors and Power",
    "section": "Example: Math Curriculum",
    "text": "Example: Math Curriculum"
  },
  {
    "objectID": "slides/19/slides19.html#example-one-sample-t-test-for-h_0-mu-5-vs.-h_a-mu-ne-5-at-the-alpha-.05-level-assuming-s2.",
    "href": "slides/19/slides19.html#example-one-sample-t-test-for-h_0-mu-5-vs.-h_a-mu-ne-5-at-the-alpha-.05-level-assuming-s2.",
    "title": "Testing Errors and Power",
    "section": "Example: One sample t-test for \\(H_0: \\mu = 5\\) vs. \\(H_a: \\mu \\ne 5\\) at the \\(\\alpha = .05\\) level (assuming \\(s=2\\)).",
    "text": "Example: One sample t-test for \\(H_0: \\mu = 5\\) vs. \\(H_a: \\mu \\ne 5\\) at the \\(\\alpha = .05\\) level (assuming \\(s=2\\))."
  },
  {
    "objectID": "slides/19/slides19.html#section-8",
    "href": "slides/19/slides19.html#section-8",
    "title": "Testing Errors and Power",
    "section": "",
    "text": "Example: Let \\(Y_i \\sim N(\\mu,52)\\). We wish to test \\(H_0: \\mu = 7\\) vs. \\(H_A:\\mu&gt;7\\) at the \\(\\alpha=0.05\\) level. What is the smallest sample size such that the test has power at least .80 when \\(\\mu=8\\)?"
  },
  {
    "objectID": "slides/09/slides09.html#recap",
    "href": "slides/09/slides09.html#recap",
    "title": "Efficiency & CRLB",
    "section": "Recap",
    "text": "Recap\n\nObserve data \\(X_1, ..., X_n \\sim F_x(x|\\theta)\\), where \\(\\theta\\) is unknown\nGoal: Estimate \\(\\theta\\) based on the values of \\(X_i\\) by formulating an estimator \\(\\widehat \\theta\\)\nOne technique is to use the maximum likelihood estimator\nA second technique is to use the method of moments estimator\nWe can compare estimators by comparing their bias, variance, and mean squared error.\nToday: is there a way to know if we’ve found an “optimal” estimator?"
  },
  {
    "objectID": "slides/09/slides09.html#last-time-unif0-theta-distribution",
    "href": "slides/09/slides09.html#last-time-unif0-theta-distribution",
    "title": "Efficiency & CRLB",
    "section": "Last time: Unif(0, \\(\\theta\\)) distribution1",
    "text": "Last time: Unif(0, \\(\\theta\\)) distribution1\nYour task is to compare the estimators\n\\[\\widehat{\\theta}_{MLE} = X_\\max \\hspace{1in} \\widehat{\\theta}_{MoM} = 2\\bar{X}\\]\n\n\nWhat is the bias of each estimator?2\nWhat is the SE of each estimator?\nWhat is the MSE of each estimator?\nWhen does \\(\\widehat{\\theta}_{MLE}\\) “beat” \\(\\widehat{\\theta}_{MoM}\\) in terms of MSE?\n\n\nSo \\(0 \\le x \\le \\theta\\), \\(f_x = \\frac{1}{\\theta}\\), \\(F_x = \\frac{x}{\\theta}\\), \\(E(X) = \\frac{\\theta}{2}\\), and \\(V(X) = \\frac{\\theta^2}{12}\\)A helpful fact is that \\(f_{X_{max}}(x) = n[F(x)]^{n-1}f_X(x)\\)"
  },
  {
    "objectID": "slides/09/slides09.html#section",
    "href": "slides/09/slides09.html#section",
    "title": "Efficiency & CRLB",
    "section": "",
    "text": "(d) When does \\(\\widehat{\\theta}_{MLE}\\) “beat” \\(\\widehat{\\theta}_{MoM}\\) in terms of MSE?\nMSE = “MSE factor” \\(\\times \\theta^2\\)"
  },
  {
    "objectID": "slides/09/slides09.html#example-uniform0theta",
    "href": "slides/09/slides09.html#example-uniform0theta",
    "title": "Efficiency & CRLB",
    "section": "Example: Uniform(\\(0,\\theta\\))",
    "text": "Example: Uniform(\\(0,\\theta\\))\nFrom last time: \\(E(\\widehat \\theta_{MLE}) = \\frac{n}{n+1} \\theta\\) and \\(V(\\widehat \\theta_{MLE}) = [\\frac{n}{(n+2)(n+1)^2}]\\)\n\n\nSince \\(\\hat \\theta_{MLE}\\) beats \\(\\hat \\theta_{MoM}\\) in terms of MSE but is biased, can we “unbias” the MLE? Call this third estimator \\(\\hat\\theta_3\\)\nDoes \\(\\hat \\theta_3\\) ever “beat” \\(\\hat \\theta_{MLE}\\) in terms of MSE?"
  },
  {
    "objectID": "slides/09/slides09.html#section-1",
    "href": "slides/09/slides09.html#section-1",
    "title": "Efficiency & CRLB",
    "section": "",
    "text": "(f) Does \\(\\hat \\theta_3\\) ever “beat” \\(\\hat \\theta_{MLE}\\) in terms of MSE?\nMSE = “MSE factor” \\(\\times \\theta^2\\)"
  },
  {
    "objectID": "slides/09/slides09.html#moral-of-the-story",
    "href": "slides/09/slides09.html#moral-of-the-story",
    "title": "Efficiency & CRLB",
    "section": "Moral of the story",
    "text": "Moral of the story\nWe can “fix” bias, but it’s harder to fix variance"
  },
  {
    "objectID": "slides/09/slides09.html#comparing-unbiased-estimators-efficiency",
    "href": "slides/09/slides09.html#comparing-unbiased-estimators-efficiency",
    "title": "Efficiency & CRLB",
    "section": "Comparing unbiased estimators: efficiency",
    "text": "Comparing unbiased estimators: efficiency\n\n\n\n\n\n\nEfficiency\n\n\nFor two unbiased estimators, \\(\\widehat \\theta_1\\) is more efficient than \\(\\widehat \\theta_2\\) if\n\\[\\text{Var}(\\widehat \\theta_1) &lt; \\text{Var}(\\widehat \\theta_2) \\iff \\text{SE}(\\widehat \\theta_1) &lt; \\text{SE}(\\widehat \\theta_2)\\]"
  },
  {
    "objectID": "slides/09/slides09.html#example-why-efficiency-matters",
    "href": "slides/09/slides09.html#example-why-efficiency-matters",
    "title": "Efficiency & CRLB",
    "section": "Example: why efficiency matters",
    "text": "Example: why efficiency matters\nWe now have two unbiased estimators: \\[\\widehat \\theta_{MoM}$\\widehat \\theta_{3} = \\frac{n+1}{n} X_\\max\\]\n\nIf \\(n=10\\) what is the expectation and variance of each of these estimators?"
  },
  {
    "objectID": "slides/09/slides09.html#example-why-efficiency-matters-1",
    "href": "slides/09/slides09.html#example-why-efficiency-matters-1",
    "title": "Efficiency & CRLB",
    "section": "Example: why efficiency matters",
    "text": "Example: why efficiency matters\n\nWhat \\(n\\) would we need for \\(\\widehat\\theta_{MOM}\\) to reach the variance of \\(\\widehat\\theta_3\\)?"
  },
  {
    "objectID": "slides/09/slides09.html#comparing-unbiased-estimators-cramer-rao-lower-bound-crlb",
    "href": "slides/09/slides09.html#comparing-unbiased-estimators-cramer-rao-lower-bound-crlb",
    "title": "Efficiency & CRLB",
    "section": "Comparing unbiased estimators: Cramer-Rao Lower Bound (CRLB)",
    "text": "Comparing unbiased estimators: Cramer-Rao Lower Bound (CRLB)\n\n\n\n\n\n\n\nCRLB\n\n\nIf \\(X_1, ..., X_n\\) are an iid sample from a distribution with pdf \\(f(x|\\theta)\\), then any unbiased estimator \\(\\hat\\theta\\) of \\(\\theta\\) satisfies:\n\\[V(\\hat{\\theta}) \\ge \\frac{1}{n I(\\theta)}\\]\nwhere \\(I(\\theta)\\) is the Fisher Information of \\(X_i\\)\n\n\n\n\n\nIf the variance of an estimator is equal to the CRLB, then there is no other unbiased estimator with more precision"
  },
  {
    "objectID": "slides/09/slides09.html#fisher-information",
    "href": "slides/09/slides09.html#fisher-information",
    "title": "Efficiency & CRLB",
    "section": "Fisher Information",
    "text": "Fisher Information\n\n\n\n\n\n\nFisher Information\n\n\nThe Fisher Information of a random variable \\(X\\) is \\[I(\\theta) = E[(\\frac{d}{d\\theta} \\ln f(x|\\theta))^2] = -   E(l''(\\theta))\\]\n*provided the domain of \\(X\\) does not depend on \\(\\theta\\) (and a few other regularity conditions)"
  },
  {
    "objectID": "slides/09/slides09.html#intuition-fisher-information",
    "href": "slides/09/slides09.html#intuition-fisher-information",
    "title": "Efficiency & CRLB",
    "section": "Intuition: Fisher information",
    "text": "Intuition: Fisher information\n\\[I(\\theta) = E_X[(\\frac{d}{d\\theta} \\ln f(x|\\theta))^2] = -   E_X(l''(\\theta))\\]\n\n\\(l(\\theta)\\) is the log-likelihood function\n\\(l'(\\theta)\\) gives the slope of the log-likelihood at any point, \\(l''(\\theta)\\) gives the curvature at any given point\nThe Fisher information “averages out” over \\(X\\) and summarizes the overall curvature of the log-likelihood function"
  },
  {
    "objectID": "slides/09/slides09.html#intuition-fisher-information-1",
    "href": "slides/09/slides09.html#intuition-fisher-information-1",
    "title": "Efficiency & CRLB",
    "section": "Intuition: Fisher information",
    "text": "Intuition: Fisher information\n\n\nHigher Fisher Information\n\n\n\n\n\n\n\n\n\n\nLower Fisher Information\n\n\n\n\n\n\n\n\n\n\n\nDistributions with a “pointier” log-likelihood have higher Fisher information than distributions with lower Fisher information\n\n\nLower Fisher information \\(\\to\\) flat log-likelihood \\(\\to\\) lots of estimators “close” to the MLE"
  },
  {
    "objectID": "slides/09/slides09.html#practice-fisher-information",
    "href": "slides/09/slides09.html#practice-fisher-information",
    "title": "Efficiency & CRLB",
    "section": "Practice: Fisher information",
    "text": "Practice: Fisher information\nFind the Fisher information for \\(Y\\), where \\(X \\sim Exp(\\lambda)\\)"
  },
  {
    "objectID": "slides/09/slides09.html#practice-finding-crlb",
    "href": "slides/09/slides09.html#practice-finding-crlb",
    "title": "Efficiency & CRLB",
    "section": "Practice: finding CRLB",
    "text": "Practice: finding CRLB\nLet \\(Y_1, ..., Y_n\\) be \\(n\\) Exp(\\(\\lambda\\)) random variables Let \\(\\widehat\\lambda = \\frac{n}{\\sum Y_i}\\). How does Var(\\(\\widehat\\lambda\\)) compare with the CRLB?"
  },
  {
    "objectID": "slides/09/slides09.html#section-2",
    "href": "slides/09/slides09.html#section-2",
    "title": "Efficiency & CRLB",
    "section": "",
    "text": "So far, we’ve found the bias, variance, and MSE analytically (with formal mathematical formulas). In practice, we often can’t do this.\n\nFor example: The median \\(m\\) of an Exponential(\\(\\lambda\\)) distribution satisfies P(X ≤ m) = 0.5. Solving \\(1 - e^(-\\lambda m) = 0.5\\) gives \\(m = ln(2) / \\lambda\\).\n\n\nThis suggests an estimator for \\(\\lambda\\) based on the median: \\(\\widehat{\\lambda} = \\frac{ln(2)}{m}\\)\n\n\nFinding the analytical variance of \\(\\widehat \\lambda\\) is complicated. Finding the sampling distribution of \\(m\\) is complicated, and finding the non-linear transformation is also complicated."
  },
  {
    "objectID": "slides/09/slides09.html#your-turn-simulation",
    "href": "slides/09/slides09.html#your-turn-simulation",
    "title": "Efficiency & CRLB",
    "section": "Your turn: simulation",
    "text": "Your turn: simulation\nUse simulation to see whether \\(\\frac{ln(2)}{m}\\) (a) is unbiased and (b) achieves the CRLB"
  },
  {
    "objectID": "slides/01/slides01.html#plan-for-today",
    "href": "slides/01/slides01.html#plan-for-today",
    "title": "Welcome to Stat250!",
    "section": "Plan for today",
    "text": "Plan for today\n\nIntros\nContext for the class\nSyllabus\nSetting course expectations"
  },
  {
    "objectID": "slides/01/slides01.html#about-me",
    "href": "slides/01/slides01.html#about-me",
    "title": "Welcome to Stat250!",
    "section": "About me",
    "text": "About me\n\n\n\nFirst year at Carleton!\nTaught at Swarthmore for 5 years before moving here this fall\nPhD in Statistics & Data Science from Carnegie Mellon University\nGrew up in Minnesota, went to St Ben’s as an undergrad"
  },
  {
    "objectID": "slides/01/slides01.html#three-prongs-of-statistics",
    "href": "slides/01/slides01.html#three-prongs-of-statistics",
    "title": "Welcome to Stat250!",
    "section": "Three prongs of statistics",
    "text": "Three prongs of statistics\n\n\nDesign  The design of surveys/experiments and collection of data to more efficiently/correctly address scientific questions \n\n\n\nExploration  Understand the major features of and detect patterns in data \n\n\n\n\nInference  Account for randomness, variability, and bias in a sample in order to draw reasonable and correct conclusions about a population"
  },
  {
    "objectID": "slides/01/slides01.html#statistics-vs.-probability",
    "href": "slides/01/slides01.html#statistics-vs.-probability",
    "title": "Welcome to Stat250!",
    "section": "Statistics vs. probability",
    "text": "Statistics vs. probability\nProbability (Math 240)\nWe learned how to calculate the probability of seeing a result (data) given a specific probability model (e.g., a specific distribution)\n\n\nStatistics (Stat 250)\nWe will learn how to make statements about the underlying probability models given the data we see"
  },
  {
    "objectID": "slides/01/slides01.html#example-spies-vs.-statisticians",
    "href": "slides/01/slides01.html#example-spies-vs.-statisticians",
    "title": "Welcome to Stat250!",
    "section": "Example: Spies vs. Statisticians",
    "text": "Example: Spies vs. Statisticians\nDuring WWII, the Allies wanted to determine production rates of tanks (and airplanes, missiles, etc.)\nSpies\nGathered intelligence (intercepted messages, interrogated of prisoners, etc.) and made the following estimates:\n\nJune 1940: 1000\nJune 1941: 1550\nAugust 1942: 1550"
  },
  {
    "objectID": "slides/01/slides01.html#example-spies-vs.-statisticians-1",
    "href": "slides/01/slides01.html#example-spies-vs.-statisticians-1",
    "title": "Welcome to Stat250!",
    "section": "Example: Spies vs. Statisticians",
    "text": "Example: Spies vs. Statisticians\nStatisticians\n\n\nThe Allies had a sample of serial numbers (via capture, photography, etc.), \\(X_1, X_2, \\ldots, X_n\\), and there were \\(N\\) produced.\nAllied statisticians needed to devise an estimator to obtain \\(N\\)\nUltimately, they used \\(\\widehat{N} = X_{\\text{max}} + \\dfrac{X_{\\text{max}}}{n} - 1\\)\n\nJune 1940: 169\nJune 1941: 244\nAugust 1942: 327"
  },
  {
    "objectID": "slides/01/slides01.html#example-spies-vs.-statisticians-2",
    "href": "slides/01/slides01.html#example-spies-vs.-statisticians-2",
    "title": "Welcome to Stat250!",
    "section": "Example: Spies vs. Statisticians",
    "text": "Example: Spies vs. Statisticians\nAfter the war, the Allies discovered documents revealing the true number of tanks produced:\n\n\n\n\nMonth\nTruth\nStatisticians\nSpies\n\n\n\n\nJune 1940\n122\n169\n1000\n\n\nJune 1941\n271\n244\n1550\n\n\nAugust 1942\n342\n327\n1550"
  },
  {
    "objectID": "slides/01/slides01.html#prereqs",
    "href": "slides/01/slides01.html#prereqs",
    "title": "Welcome to Stat250!",
    "section": "Prereqs",
    "text": "Prereqs\n\nMath240 (Probability)\nStat120 or other applied intro stats course is helpful but not required\nI’ll assume some experience with R\nWe’re going to use mechanics from calculus & probability, but it’s not the focus of the course\nPart of HW1 is Calc/Prob review. refresh your memory, use resources that you need, talk to me sooner rather than later if it’s especially tough"
  },
  {
    "objectID": "slides/01/slides01.html#section",
    "href": "slides/01/slides01.html#section",
    "title": "Welcome to Stat250!",
    "section": "",
    "text": "Statistics seems to be a difficult subject for mathematicians, perhaps because its elusive and wide-ranging character mitigates against the traditional theorem-proof method of presentation. It may come as some comfort then that statistics is also a difficult subject for statisticians\n\n\nControversies in the Foundations of Statistics, Bradley Efron 1978"
  },
  {
    "objectID": "slides/01/slides01.html#statistical-models",
    "href": "slides/01/slides01.html#statistical-models",
    "title": "Welcome to Stat250!",
    "section": "Statistical models",
    "text": "Statistical models\nA statistical model consists of\n\na collection of random variables to describe observable data,\nthe possible joint distribution(s) of the random variables,\nand the parameters, \\(\\boldsymbol \\theta\\), that define those distributions\n\n\n\nMorris and DeGroot, 377"
  },
  {
    "objectID": "slides/01/slides01.html#r.a.-fisher",
    "href": "slides/01/slides01.html#r.a.-fisher",
    "title": "Welcome to Stat250!",
    "section": "R.A. Fisher",
    "text": "R.A. Fisher\n1890-1962\n\n\n\n\n\nVariance\nANOVA\nNull hypothesis\nMaximum likelihood estimation\np-value\nLots of contributions in genetics\nAlso a eugenecist"
  },
  {
    "objectID": "slides/01/slides01.html#neyman-pearson",
    "href": "slides/01/slides01.html#neyman-pearson",
    "title": "Welcome to Stat250!",
    "section": "Neyman & Pearson",
    "text": "Neyman & Pearson\n\n\n\n\n\nConfidence interval\nCorrelation\nRegression\nStandard deviation\nEffect size\n“Optimal” tests\n\\(\\alpha\\) and \\(\\beta\\)\nType I and II error"
  },
  {
    "objectID": "slides/01/slides01.html#frequentist-vs-bayesian",
    "href": "slides/01/slides01.html#frequentist-vs-bayesian",
    "title": "Welcome to Stat250!",
    "section": "Frequentist vs Bayesian",
    "text": "Frequentist vs Bayesian"
  },
  {
    "objectID": "slides/01/slides01.html#frequentist-vs-bayesian-1",
    "href": "slides/01/slides01.html#frequentist-vs-bayesian-1",
    "title": "Welcome to Stat250!",
    "section": "Frequentist vs Bayesian",
    "text": "Frequentist vs Bayesian\n\n\nFrequentist\n\n“Classical” statistics\nProbability is a long-run frequency\nType I and Type II errors\nConfidence intervals\n\n\nBayesian\n\nProbability is a subjective belief\nUpdate “prior” probabilities with data to obtain “posterior” probabilities\n“Credible” intervals\nComputationally intensive"
  },
  {
    "objectID": "slides/01/slides01.html#parametric-vs-nonparametric",
    "href": "slides/01/slides01.html#parametric-vs-nonparametric",
    "title": "Welcome to Stat250!",
    "section": "Parametric vs Nonparametric",
    "text": "Parametric vs Nonparametric\n\nNonparametric \n\nThe basic idea of nonparametric inference is to use data to infer an unknown quantity while making as few assumptions as possible. Usually, this means using statistical models that are infinite-dimensional. (Wasserman, 2006)\n\n\nParametric\n\nA parametric inference uses models that consist of a set of distributions/densities that can be parameterized by a finite number of parameters."
  },
  {
    "objectID": "slides/01/slides01.html#tentative-schedule",
    "href": "slides/01/slides01.html#tentative-schedule",
    "title": "Welcome to Stat250!",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\n\nTopic\nChapters\nApprox. Duration\n\n\n\n\nReview\n1-4\n1 week\n\n\nParametric estimation\n6\n3 weeks\n\n\nParametric & Nonparametric inference\n3-5, 7-9\n4 weeks\n\n\nModeling\n8-10\n2 weeks"
  },
  {
    "objectID": "slides/01/slides01.html#course-description",
    "href": "slides/01/slides01.html#course-description",
    "title": "Welcome to Stat250!",
    "section": "Course description",
    "text": "Course description\nThis course is an introduction to the mathematical theory of frequentist and Bayesian statistical inference. Topics include parameter estimation, confidence intervals and hypothesis testing, linear models, and Bayesian inference.\nStudents who analyze data, or who aspire to develop new methods for analyzing data, should be well-grounded in mathematical statistics."
  },
  {
    "objectID": "slides/01/slides01.html#course-objectives",
    "href": "slides/01/slides01.html#course-objectives",
    "title": "Welcome to Stat250!",
    "section": "Course Objectives",
    "text": "Course Objectives\nBy the end of this course, you should be able to:\n\nDerive estimators for parameters using maximum likelihood, the method of moments, and Bayesian techniques\nEvaluate the performance of estimators and describe their strengths and weaknesses\nDemonstrate a sophisticated understanding of the mathematics behind hypothesis tests, confidence intervals, and linear models\n\nUse the statistical package R to implement basic simulations of estimation scenarios"
  },
  {
    "objectID": "slides/01/slides01.html#textbook",
    "href": "slides/01/slides01.html#textbook",
    "title": "Welcome to Stat250!",
    "section": "Textbook:",
    "text": "Textbook:\nMathematical Statistics with Resampling and R (3rd edition) by Chihara and Hesterberg\n\nSometimes, it can help to see a second way of topics being explained. I recommend Mathematical Statistics with applications by Larsen & Marx."
  },
  {
    "objectID": "slides/01/slides01.html#computing",
    "href": "slides/01/slides01.html#computing",
    "title": "Welcome to Stat250!",
    "section": "Computing:",
    "text": "Computing:\nWe’ll be using R and RStudio throughout the course. If you’ve downloaded R to your own computer from a different class, great! If not, you can access anything you need through the maize server:\nhttps://maize.mathcs.carleton.edu/"
  },
  {
    "objectID": "slides/01/slides01.html#what-will-you-do-in-this-course",
    "href": "slides/01/slides01.html#what-will-you-do-in-this-course",
    "title": "Welcome to Stat250!",
    "section": "What will you do in this course?",
    "text": "What will you do in this course?\nEach of the following components are important for your learning and therefore part of your final grade calculation:\n\nDaily Prep (5%)\nHomework (15%)\n\nDue once per week, typically Wednesdays but sometimes Fridays\n\nGroup work and attendance (5%)\n\nmore than 5 absences \\(\\to\\) 0\n\nMidterm exams (2 x 17.5%)\nCourse Project (10%)\nFinal Exam (30%)"
  },
  {
    "objectID": "slides/01/slides01.html#what-will-a-typical-dayweek-look-like",
    "href": "slides/01/slides01.html#what-will-a-typical-dayweek-look-like",
    "title": "Welcome to Stat250!",
    "section": "What will a typical day/week look like?",
    "text": "What will a typical day/week look like?\n\n\nBefore class:\n\nRead a chapter\nCome with questions\nBe prepared to try what was covered\n\n\nIn class:\n\nMini lecture\n\nSometimes review\nSometimes new\n\nHands-on work or coding in R\n\n\nAfter class:\n\nFinish in-class exercises\nWork on homework"
  },
  {
    "objectID": "slides/01/slides01.html#office-hours-tentative",
    "href": "slides/01/slides01.html#office-hours-tentative",
    "title": "Welcome to Stat250!",
    "section": "Office hours (tentative)",
    "text": "Office hours (tentative)\n\n\n\nDay\nTime\nType\nLocation\n\n\n\n\nMonday\n4:15-5:15\nDrop-in\nCMC 307\n\n\nTuesday\n10:30-11:30\nDrop-in\nCMC 307\n\n\nWednesday\n2:15-3:15\nDrop-in\nCMC 307\n\n\nFriday\n11-12\nDrop-in\nCMC 307"
  },
  {
    "objectID": "slides/01/slides01.html#where-is-amanda-this-term",
    "href": "slides/01/slides01.html#where-is-amanda-this-term",
    "title": "Welcome to Stat250!",
    "section": "Where is Amanda this term?",
    "text": "Where is Amanda this term?"
  },
  {
    "objectID": "slides/01/slides01.html#communication",
    "href": "slides/01/slides01.html#communication",
    "title": "Welcome to Stat250!",
    "section": "Communication",
    "text": "Communication\n\n\nMoodle: assignments, note sets, and grades\nSlack: homework questions, announcements, discussion\nEmail: personal matters, time-sensitive annoucements\n\n\n\nSlack is the fastest way to reach me. I typically will respond to messages 3x per weekday. I try to respond to emails within 48 hours. I’m online sporadically on evenings and weekends to devote time to family and rest – I hope you also use this time to reset and recharge!"
  },
  {
    "objectID": "slides/01/slides01.html#advice-from-past-students",
    "href": "slides/01/slides01.html#advice-from-past-students",
    "title": "Welcome to Stat250!",
    "section": "Advice from past students:",
    "text": "Advice from past students:\n\nStart the problem sets early and go to office hours!\nWork with other people in the class, collaboration is key\nBe willing to ask lots of questions and don’t be afraid to ask for help!\nI would advise for them to attend office hours and the stat clinic since early on to get help and not be worrying about assignments at the last minute.\nI would advise future students to reach out and ask questions as soon if they have any confusion. Understanding statistical concepts and working with R can be frustrating at times, but people are here to help you along the way!\nDon’t let work snowball! Try to get help early and often and an imperfect problem set is better than no problem set.\nStart the homework early! Give yourself time to get things done, to understand, and to pause. Don’t feel afraid to ask questions; she’s so accessible.\nSpend some time reviewing handouts before doing homework."
  },
  {
    "objectID": "slides/01/slides01.html#the-genius-myth",
    "href": "slides/01/slides01.html#the-genius-myth",
    "title": "Welcome to Stat250!",
    "section": "The “Genius Myth”",
    "text": "The “Genius Myth”\nIt’s sometimes easy to buy into the “genius myth” when it comes to math/stat courses: that you need to be a “math person” and have some innate mathematical ability in order to do well or become a statistics major. This could not be further from the truth! The best statisticians don’t necessarily have the “best” math or programming background, but are people that are able to formulate interesting questions and use math and programming to rigorously answer those questions. Many of the best statisticians I know became statisticians because they were initially interested in something else (biology, public health, psychology, neuroscience, physics, etc.) and realized that being able to answer important questions with data was not only valuable but fun and interesting. Being able to perform interesting statistical analyses is a skill that is learned, not an innate ability, and working hard at developing that skill is the point of this course."
  },
  {
    "objectID": "slides/01/slides01.html#academic-integrity",
    "href": "slides/01/slides01.html#academic-integrity",
    "title": "Welcome to Stat250!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nYou are expected to follow Carleton’s policies regarding academic integrity. I encourage you to discuss the homework problems with others and use the resources available to you to try to figure out tough problems. You should code and write up your solutions on your own. Exams must be done by yourself without communicating with others; all work must be your own. The use of textbook solution manuals (physical or online), course materials from other students, or materials from previous versions of this course are not allowed. Copying, paraphrasing, summarizing, or submitting work generated by anyone but yourself without proper attribution is considered academic dishonesty (this includes output from LLMs).\nPlease ask if you are unsure of whether or not your actions are complying with the assignment/exam/project instructions. Always default to acknowledging any help received. Cases of suspected academic dishonesty are handled by the Provost’s Office and I am obligated to report any suspected violations of this policy."
  },
  {
    "objectID": "slides/01/slides01.html#more-on-ai",
    "href": "slides/01/slides01.html#more-on-ai",
    "title": "Welcome to Stat250!",
    "section": "More on “AI”",
    "text": "More on “AI”\nLarge-language models (e.g. ChatGPT, Gemini, etc.) should only be used for help interpreting R’s error messages. You should not copy and paste course material into or out of an AI text generator.\nI also have a few rules in place to protect my intellectual property. You may not record my lectures using tools such as Otter.ai or upload any video or audio recordings to generate transcripts or study notes. You may not upload my course materials (slides, assignment prompts, note sets, etc.) into AI tools or homework help sites (such as chegg).\n“AI” tools are new for all of us and it’s OK to have questions about what is and isn’t appropriate!"
  },
  {
    "objectID": "slides/01/slides01.html#diversity-inclusion",
    "href": "slides/01/slides01.html#diversity-inclusion",
    "title": "Welcome to Stat250!",
    "section": "Diversity & Inclusion",
    "text": "Diversity & Inclusion\nWe all come to class with different backgrounds and experiences, and this diversity makes our class environment richer. We value diversity and inclusion, and are committed to a climate of mutual respect and full participation in and out of the classroom. This class strives to be a learning environment that is usable, equitable, inclusive and welcoming, regardless of race, ethnicity, religion, gender and gender identities, sexual orientation, ability, socioeconomic background, and nationality. If you anticipate or experience any barriers to learning, please discuss your concerns with me."
  },
  {
    "objectID": "slides/01/slides01.html#accomodations",
    "href": "slides/01/slides01.html#accomodations",
    "title": "Welcome to Stat250!",
    "section": "Accomodations",
    "text": "Accomodations\nCarleton College is committed to providing equitable access to learning opportunities for all students. The Office of Accessibility Resources (Henry House, 107 Union Street) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations. If you have, or think you may have, a disability, please contact OAR@carleton.edu to arrange a confidential discussion regarding equitable access and reasonable accommodations. You are also welcome to contact me privately to discuss your academic needs. However, all disability-related accommodations must be arranged, in advance, through OAR."
  },
  {
    "objectID": "slides/01/slides01.html#stat-lab",
    "href": "slides/01/slides01.html#stat-lab",
    "title": "Welcome to Stat250!",
    "section": "Stat Lab",
    "text": "Stat Lab\nThe Stats Lab (CMC 304) offers drop-in help R/RStudio help sessions run by friendly and knowledgeable lab assistants on most weekday evenings and some weekend times. The Stat Lab is primarily meant to serve Stat120 students, but many of the lab assistants can also help with Stat250."
  },
  {
    "objectID": "slides/01/slides01.html#title-ix",
    "href": "slides/01/slides01.html#title-ix",
    "title": "Welcome to Stat250!",
    "section": "Title IX",
    "text": "Title IX\nPlease be aware that all faculty are “responsible employees”, which means that if you tell me about a situation involving sexual harassment, sexual assault, dating violence, domestic violence, or stalking, I must share that information with the Title IX Coordinator. Although I have to make this notification, you will control how your case will be handled, including whether or not you wish to meet with the Title IX coordinator or pursue a formal complaint."
  },
  {
    "objectID": "slides/01/slides01.html#take-care-of-yourself",
    "href": "slides/01/slides01.html#take-care-of-yourself",
    "title": "Welcome to Stat250!",
    "section": "Take care of yourself",
    "text": "Take care of yourself\nDo your best to maintain a healthy lifestyle this semester by wearing a mask if you don’t feel well, eating a vegetable every day, exercising, avoiding excessive drug and alcohol use, getting enough sleep, and taking some time to relax. Your mental health is more important than your grade in this course. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. If you are experiencing mental health symptoms as a result of coursework, please speak with me so we can address the problem together."
  },
  {
    "objectID": "slides/00-test/00-test.html#r-code",
    "href": "slides/00-test/00-test.html#r-code",
    "title": "Test slides",
    "section": "R Code",
    "text": "R Code\n\npalmerpenguins::penguins %&gt;%\n  ggplot(aes(x = flipper_length_mm)) +\n  geom_histogram(col = \"white\")"
  },
  {
    "objectID": "slides/00-test/00-test.html#plan-for-today",
    "href": "slides/00-test/00-test.html#plan-for-today",
    "title": "Test slides",
    "section": "Plan for today",
    "text": "Plan for today\n\nSyllabus\nContext for the class\nCourse expectations"
  },
  {
    "objectID": "slides/00-test/00-test.html#a-bit-about-me",
    "href": "slides/00-test/00-test.html#a-bit-about-me",
    "title": "Test slides",
    "section": "A bit about me",
    "text": "A bit about me"
  },
  {
    "objectID": "slides/00-test/00-test.html#course-description",
    "href": "slides/00-test/00-test.html#course-description",
    "title": "Test slides",
    "section": "Course description",
    "text": "Course description\nThis course is an introduction to the mathematical theory of frequentist and Bayesian statistical inference. Topics include parameter estimation, confidence intervals and hypothesis testing, linear models, and Bayesian inference.\nStudents who analyze data, or who aspire to develop new methods for analyzing data, should be well-grounded in mathematical statistics."
  },
  {
    "objectID": "slides/00-test/00-test.html#course-objectives",
    "href": "slides/00-test/00-test.html#course-objectives",
    "title": "Test slides",
    "section": "Course Objectives",
    "text": "Course Objectives\nBy the end of this course, you should be able to:\n\nDerive estimators for parameters using maximum likelihood, the method of moments, and Bayesian techniques\nEvaluate the performance of estimators and describe their strengths and weaknesses\nDemonstrate a sophisticated understanding of the mathematics behind hypothesis tests, confidence intervals, and linear models\n\nUse the statistical package R to implement basic simulations of estimation scenarios"
  },
  {
    "objectID": "slides/00-test/00-test.html#textbook",
    "href": "slides/00-test/00-test.html#textbook",
    "title": "Test slides",
    "section": "Textbook:",
    "text": "Textbook:"
  },
  {
    "objectID": "slides/00-test/00-test.html#computing",
    "href": "slides/00-test/00-test.html#computing",
    "title": "Test slides",
    "section": "Computing:",
    "text": "Computing:"
  },
  {
    "objectID": "slides/00-test/00-test.html#assignments",
    "href": "slides/00-test/00-test.html#assignments",
    "title": "Test slides",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "slides/00-test/00-test.html#regrade-request-policy",
    "href": "slides/00-test/00-test.html#regrade-request-policy",
    "title": "Test slides",
    "section": "Regrade request policy",
    "text": "Regrade request policy\nGrading is often a tedious task, and the grading team will sometimes make mistakes. I am always happy to fix these mistakes, and gradescope makes it easy to do so. However, it takes time to read through these requests and I’ve noticed an increase in unwarranted regrade requests in recent years. This semester, I am instituting an “NFL Coaches Challenge”-style regrade request rule. Every student will start the semester with 2 regrade requests available to them. If you submit a regrade request and I agree with you, you get it back. If you submit a regrade request for something that was not a grading mistake, you lose that request."
  },
  {
    "objectID": "slides/00-test/00-test.html#exams",
    "href": "slides/00-test/00-test.html#exams",
    "title": "Test slides",
    "section": "Exams",
    "text": "Exams"
  },
  {
    "objectID": "slides/00-test/00-test.html#final-exam",
    "href": "slides/00-test/00-test.html#final-exam",
    "title": "Test slides",
    "section": "Final Exam",
    "text": "Final Exam"
  },
  {
    "objectID": "slides/00-test/00-test.html#course-project",
    "href": "slides/00-test/00-test.html#course-project",
    "title": "Test slides",
    "section": "Course project",
    "text": "Course project"
  },
  {
    "objectID": "slides/00-test/00-test.html#final-grades",
    "href": "slides/00-test/00-test.html#final-grades",
    "title": "Test slides",
    "section": "Final grades",
    "text": "Final grades"
  },
  {
    "objectID": "slides/00-test/00-test.html#advice-from-past-students",
    "href": "slides/00-test/00-test.html#advice-from-past-students",
    "title": "Test slides",
    "section": "Advice from past students:",
    "text": "Advice from past students:\n\nStart the problem sets early and go to office hours!\nWork with other people in the class, collaboration is key\nBe willing to ask lots of questions and don’t be afraid to ask for help!\nI would advise for them to attend office hours and the stat clinic since early on to get help and not be worrying about assignments at the last minute.\nI would advise future students to reach out and ask questions as soon if they have any confusion. Understanding statistical concepts and working with R can be frustrating at times, but people are here to help you along the way!\nDon’t let work snowball! Try to get help early and often and an imperfect problem set is better than no problem set.\nlots of R here! Organization is key, and you’ll do well as long as you start homeworks early, communicate with the professor and with your fellow classmates. There’s a good support system."
  },
  {
    "objectID": "slides/00-test/00-test.html#the-genius-myth",
    "href": "slides/00-test/00-test.html#the-genius-myth",
    "title": "Test slides",
    "section": "The “Genius Myth”",
    "text": "The “Genius Myth”\nIt’s sometimes easy to buy into the “genius myth” when it comes to math/stat courses: that you need to be a “math person” and have some innate mathematical ability in order to do well or become a statistics major. This could not be further from the truth! The best statisticians don’t necessarily have the “best” math or programming background, but are people that are able to formulate interesting questions and use math and programming to rigorously answer those questions. Many of the best statisticians I know became statisticians because they were initially interested in something else (biology, public health, psychology, neuroscience, physics, etc.) and realized that being able to answer important questions with data was not only valuable but fun and interesting. Being able to perform interesting statistical analyses is a skill that is learned, not an innate ability, and working hard at developing that skill is the point of this course."
  },
  {
    "objectID": "slides/00-test/00-test.html#academic-integrity",
    "href": "slides/00-test/00-test.html#academic-integrity",
    "title": "Test slides",
    "section": "Academic Integrity",
    "text": "Academic Integrity"
  },
  {
    "objectID": "slides/00-test/00-test.html#diversity-inclusion",
    "href": "slides/00-test/00-test.html#diversity-inclusion",
    "title": "Test slides",
    "section": "Diversity & Inclusion",
    "text": "Diversity & Inclusion\nWe all come to class with different backgrounds and experiences, and this diversity makes our class environment richer. We value diversity and inclusion, and are committed to a climate of mutual respect and full participation in and out of the classroom. This class strives to be a learning environment that is usable, equitable, inclusive and welcoming, regardless of race, ethnicity, religion, gender and gender identities, sexual orientation, ability, socioeconomic background, and nationality. If you anticipate or experience any barriers to learning, please discuss your concerns with me."
  },
  {
    "objectID": "slides/00-test/00-test.html#accomodations",
    "href": "slides/00-test/00-test.html#accomodations",
    "title": "Test slides",
    "section": "Accomodations",
    "text": "Accomodations"
  },
  {
    "objectID": "slides/00-test/00-test.html#stat-lab",
    "href": "slides/00-test/00-test.html#stat-lab",
    "title": "Test slides",
    "section": "Stat Lab",
    "text": "Stat Lab"
  },
  {
    "objectID": "slides/00-test/00-test.html#title-ix",
    "href": "slides/00-test/00-test.html#title-ix",
    "title": "Test slides",
    "section": "Title IX",
    "text": "Title IX\nPlease be aware that all faculty are “responsible employees”, which means that if you tell me about a situation involving sexual harassment, sexual assault, dating violence, domestic violence, or stalking, I must share that information with the Title IX Coordinator. Although I have to make this notification, you will control how your case will be handled, including whether or not you wish to meet with the Title IX coordinator or pursue a formal complaint."
  },
  {
    "objectID": "slides/00-test/00-test.html#take-care-of-yourself",
    "href": "slides/00-test/00-test.html#take-care-of-yourself",
    "title": "Test slides",
    "section": "Take care of yourself",
    "text": "Take care of yourself\nDo your best to maintain a healthy lifestyle this semester by wearing a mask if you don’t feel well, eating a vegetable every day, exercising, avoiding excessive drug and alcohol use, getting enough sleep, and taking some time to relax. Your mental health is more important than your grade in this course. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. If you are experiencing mental health symptoms as a result of coursework, please speak with me so we can address the problem together."
  },
  {
    "objectID": "slides/00-test/00-test.html#prerequisites",
    "href": "slides/00-test/00-test.html#prerequisites",
    "title": "Test slides",
    "section": "Prerequisites",
    "text": "Prerequisites"
  },
  {
    "objectID": "slides/00-test/00-test.html#history",
    "href": "slides/00-test/00-test.html#history",
    "title": "Test slides",
    "section": "History",
    "text": "History\n\nStatistics seems to be a difficult subject for mathematicians, perhaps because its elusive and wide-ranging character mitigates against the traditional theorem-proof method of presentation. It may come as some comfort then that statistics is also a difficult subject for statisticians\n\n\nControversies in the Foundations of Statistics, Bradley Efron 1978"
  },
  {
    "objectID": "slides/00-test/00-test.html#fisher-vs-neyman-pearson",
    "href": "slides/00-test/00-test.html#fisher-vs-neyman-pearson",
    "title": "Test slides",
    "section": "Fisher vs Neyman-Pearson",
    "text": "Fisher vs Neyman-Pearson"
  },
  {
    "objectID": "slides/23/slides23.html#can-money-buy-you-happiness",
    "href": "slides/23/slides23.html#can-money-buy-you-happiness",
    "title": "Chi-Squared Tests",
    "section": "Can money buy you happiness?",
    "text": "Can money buy you happiness?\n\nThe General Social Survey (GSS) is a sociological survey used to collect data on demographic characteristics and attitudes of residents of the United States. We’ll consider two questions:\n\nCompared with American families in general, would you say your family income is far below average, below average, average, above average, or far above average?\nTaken all together, how would you say things are these days—would you say that you are very happy, pretty happy, or not too happy?"
  },
  {
    "objectID": "slides/23/slides23.html#can-money-buy-you-happiness-1",
    "href": "slides/23/slides23.html#can-money-buy-you-happiness-1",
    "title": "Chi-Squared Tests",
    "section": "Can money buy you happiness?",
    "text": "Can money buy you happiness?"
  },
  {
    "objectID": "slides/23/slides23.html#happiness-contingency-table",
    "href": "slides/23/slides23.html#happiness-contingency-table",
    "title": "Chi-Squared Tests",
    "section": "Happiness contingency table",
    "text": "Happiness contingency table\n\n\n\n\n\n\n\n\nhappy\nfar below average\nbelow average\naverage\nabove average\nfar above average\nTotal\n\n\n\n\nnot too happy\n50\n123\n120\n33\n4\n330\n\n\npretty happy\n64\n350\n602\n253\n24\n1293\n\n\nvery happy\n39\n121\n319\n190\n25\n694\n\n\nTotal\n153\n594\n1041\n476\n53\n2317\n\n\n\n\n\n\n\nHow can we conclude whether opinion on income and happiness are associated?"
  },
  {
    "objectID": "slides/23/slides23.html#test-statistic",
    "href": "slides/23/slides23.html#test-statistic",
    "title": "Chi-Squared Tests",
    "section": "Test statistic",
    "text": "Test statistic\n\\(H_0:\\) the variables are independent\n\nWhat would the contingency table look like under \\(H_0\\)?\n\n\n\n\n\n\n\n\n\nhappy\nfar below average\nbelow average\naverage\nabove average\nfar above average\n\n\n\n\nnot too happy\n21.79111\n84.60078\n148.2650\n67.79456\n7.548554\n\n\npretty happy\n85.38153\n331.48123\n580.9292\n265.63142\n29.576608\n\n\nvery happy\n45.82736\n177.91800\n311.8058\n142.57402\n15.874838"
  },
  {
    "objectID": "slides/23/slides23.html#how-can-we-compare-what-we-observe-to-what-would-be-expected-under-h_0",
    "href": "slides/23/slides23.html#how-can-we-compare-what-we-observe-to-what-would-be-expected-under-h_0",
    "title": "Chi-Squared Tests",
    "section": "How can we compare what we observe to what would be expected under \\(H_0\\)?",
    "text": "How can we compare what we observe to what would be expected under \\(H_0\\)?\nObserved:\n\n\n\n\n\n\n\n\nhappy\nfar below average\nbelow average\naverage\nabove average\nfar above average\nTotal\n\n\n\n\nnot too happy\n50\n123\n120\n33\n4\n330\n\n\npretty happy\n64\n350\n602\n253\n24\n1293\n\n\nvery happy\n39\n121\n319\n190\n25\n694\n\n\nTotal\n153\n594\n1041\n476\n53\n2317\n\n\n\n\n\n\n\nExpected:\n\n\n\n\n\n\n\n\nhappy\nfar below average\nbelow average\naverage\nabove average\nfar above average\nTotal\n\n\n\n\nnot too happy\n21.79111\n84.60078\n148.2650\n67.79456\n7.548554\n330\n\n\npretty happy\n85.38153\n331.48123\n580.9292\n265.63142\n29.576608\n1293\n\n\nvery happy\n45.82736\n177.91800\n311.8058\n142.57402\n15.874838\n694\n\n\nTotal\n153\n594\n1041\n476\n53\n2317"
  },
  {
    "objectID": "slides/23/slides23.html#chi-square-test-stat",
    "href": "slides/23/slides23.html#chi-square-test-stat",
    "title": "Chi-Squared Tests",
    "section": "Chi-square test stat",
    "text": "Chi-square test stat\n\\[C = \\sum_{\\text{all cells}} \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}} = \\sum (O - E)^2/E\\]\n\nBig when differences are big\nSmall when differences are small\nScaled by “bigness” of expected counts\nAll cells contribute"
  },
  {
    "objectID": "slides/23/slides23.html#when-h_0-is-true",
    "href": "slides/23/slides23.html#when-h_0-is-true",
    "title": "Chi-Squared Tests",
    "section": "When \\(H_0\\) is true",
    "text": "When \\(H_0\\) is true"
  },
  {
    "objectID": "slides/23/slides23.html#section-1",
    "href": "slides/23/slides23.html#section-1",
    "title": "Chi-Squared Tests",
    "section": "",
    "text": "“By hand”chisq with vectorschisq with table\n\n\n\n(observed - expected)^2/expected\n\n               happy2018$finrela\nhappy2018$happy far below average below average    average above average\n  not too happy        36.5167974    17.4289220  5.3883932    17.8577972\n  pretty happy          5.3544337     1.0345835  0.7642546     0.6006547\n  very happy            1.0171409    18.2087168  0.1659904    15.7758320\n               happy2018$finrela\nhappy2018$happy far above average\n  not too happy         1.6681654\n  pretty happy          1.0514577\n  very happy            5.2453183\n\nsum((observed - expected)^2/expected)\n\n[1] 128.0785\n\n1-pchisq(sum((observed - expected)^2/expected), df = (3-1)*(5-1))\n\n[1] 0\n\n\n\n\n\nchisq.test(happy2018$happy, happy2018$finrela)\n\n\n    Pearson's Chi-squared test\n\ndata:  happy2018$happy and happy2018$finrela\nX-squared = 128.08, df = 8, p-value &lt; 2.2e-16\n\n\n\n\n\nchisq.test(observed)\n\n\n    Pearson's Chi-squared test\n\ndata:  observed\nX-squared = 128.08, df = 8, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "slides/23/slides23.html#permutation-test",
    "href": "slides/23/slides23.html#permutation-test",
    "title": "Chi-Squared Tests",
    "section": "Permutation test",
    "text": "Permutation test\n\n\nStore the data in a table: one row per observation, one column per variable.\nCalculate a test statistic for the original data.\nRepeat\n\nRandomly permute the rows in one of the columns.\nCalculate the test statistic for the permuted data.\n\nCalculate the \\(p\\)-value as the fraction of times the random statistics exceed the original statistic."
  },
  {
    "objectID": "slides/23/slides23.html#permutation-test-setup",
    "href": "slides/23/slides23.html#permutation-test-setup",
    "title": "Chi-Squared Tests",
    "section": "Permutation test setup",
    "text": "Permutation test setup\nDrop any missing values\n\ndf &lt;- happy2018 |&gt; drop_na(happy, finrela)\n\nCalculate the observed test statistic\n\nobserved &lt;- chisq.test(happy2018$happy, happy2018$finrela)$statistic\nobserved\n\nX-squared \n 128.0785"
  },
  {
    "objectID": "slides/23/slides23.html#section-2",
    "href": "slides/23/slides23.html#section-2",
    "title": "Chi-Squared Tests",
    "section": "",
    "text": "Construct the permutation distribution\n\nset.seed(55057)\nN &lt;- 10^4 - 1\nresult &lt;- numeric(N)\nfor(i in 1:N) {\n  finrela_perm &lt;- sample(happy2018$finrela)\n  result[i] &lt;- chisq.test(happy2018$happy, finrela_perm)$statistic\n}"
  },
  {
    "objectID": "slides/23/slides23.html#permutation-distribution",
    "href": "slides/23/slides23.html#permutation-distribution",
    "title": "Chi-Squared Tests",
    "section": "Permutation distribution",
    "text": "Permutation distribution"
  },
  {
    "objectID": "slides/23/slides23.html#p-value",
    "href": "slides/23/slides23.html#p-value",
    "title": "Chi-Squared Tests",
    "section": "p-value",
    "text": "p-value\n\n\n(sum(result &gt;= observed) + 1) / (N + 1)\n## [1] 1e-04"
  },
  {
    "objectID": "slides/23/slides23.html#chi-squared-reference-distribution",
    "href": "slides/23/slides23.html#chi-squared-reference-distribution",
    "title": "Chi-Squared Tests",
    "section": "Chi-squared reference distribution",
    "text": "Chi-squared reference distribution"
  },
  {
    "objectID": "slides/23/slides23.html#section-3",
    "href": "slides/23/slides23.html#section-3",
    "title": "Chi-Squared Tests",
    "section": "",
    "text": "Simulation vs. model-based results\n\nChi-squared test\n\n1 - pchisq(observed, df = (3 - 1) * (5 - 1))\n\nX-squared \n        0 \n\n\n\nPermutation test\n\n(sum(result &gt;= observed) + 1) / (N + 1)\n\n[1] 0.0001"
  },
  {
    "objectID": "slides/23/slides23.html#a-shortcut-to-the-permutation-test",
    "href": "slides/23/slides23.html#a-shortcut-to-the-permutation-test",
    "title": "Chi-Squared Tests",
    "section": "A shortcut to the permutation test",
    "text": "A shortcut to the permutation test\n\nchisq.test(happy2018$happy, happy2018$finrela, simulate.p.value = TRUE)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  happy2018$happy and happy2018$finrela\nX-squared = 128.08, df = NA, p-value = 0.0004998"
  },
  {
    "objectID": "slides/23/slides23.html#caution",
    "href": "slides/23/slides23.html#caution",
    "title": "Chi-Squared Tests",
    "section": "Caution",
    "text": "Caution\nThe \\(\\chi^2\\) distribution provides a reasonable approximation of the null distribution as long as the sample size is “large enough”\nCommon guidelines:\n\n\n“Cochran’s rule:” All of the cells have expected counts &gt; 5\nAll expected counts are at least 1 and no more than 20% of cells have expected counts &lt; 5\n\n\nUse a permutation test if the expected counts aren’t large enough"
  },
  {
    "objectID": "slides/23/slides23.html#example",
    "href": "slides/23/slides23.html#example",
    "title": "Chi-Squared Tests",
    "section": "Example",
    "text": "Example\nSome people think that children who are the older ones in their class at school naturally per‐ form better in sports and that these children then get more coaching and encouragement. Could that make a difference in who makes it to the professional level in sports? Below is the birth month of 1478 major league players born since 1975, along with the national birth percentage across the same years.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nObs\n137\n121\n116\n121\n126\n114\n102\n165\n134\n115\n105\n122\n\n\nBirth %\n8%\n7%\n8%\n8%\n8%\n8%\n9%\n9%\n9%\n9%\n8%\n9%\n\n\n\n\n\nWrite out an appropriate null and alternative hypothesis\nCompute the chi-square test statistic\nCompute the p-value\nDraw a conclusion in context"
  },
  {
    "objectID": "slides/23/slides23.html#likelihood-ratio-test",
    "href": "slides/23/slides23.html#likelihood-ratio-test",
    "title": "Chi-Squared Tests",
    "section": "Likelihood Ratio test",
    "text": "Likelihood Ratio test"
  },
  {
    "objectID": "slides/14/slides14.html#last-time",
    "href": "slides/14/slides14.html#last-time",
    "title": "CLT-based Confidence Intervals",
    "section": "Last time",
    "text": "Last time\n\n\n\nDr. Kristen Gorman and the Palmer Station, Antarctica LTER, are studying the bill dimensions of a certain species of penguin\nThey want to estimate the average bill depth and bill length (in mm)\n\n\ndata(\"penguins\", package = \"palmerpenguins\")\ngentoo &lt;- filter(penguins, species == \"Gentoo\")\n\n\n\n\n\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package: https://allisonhorst.github.io/palmerpenguins/"
  },
  {
    "objectID": "slides/14/slides14.html#last-time-1",
    "href": "slides/14/slides14.html#last-time-1",
    "title": "CLT-based Confidence Intervals",
    "section": "Last Time",
    "text": "Last Time"
  },
  {
    "objectID": "slides/14/slides14.html#what-are-plausible-values-for-theta-given-hattheta",
    "href": "slides/14/slides14.html#what-are-plausible-values-for-theta-given-hattheta",
    "title": "CLT-based Confidence Intervals",
    "section": "What are plausible values for \\(\\theta\\) given \\(\\hat\\theta\\)?",
    "text": "What are plausible values for \\(\\theta\\) given \\(\\hat\\theta\\)?\nWe want to develop an interval estimate of a population parameter\n\nExact method: Find the sampling distribution in closed form (Chapter 4). Requires knowledge of the distribution of the data!\nBootstrap method: Use the sample to approximate the population and simulate a sampling distribution (Chapter 5).\nAsymptotic method: Use large-sample theory to approximate the sampling distribution (e.g., appeal to CLT; Chapter 7)"
  },
  {
    "objectID": "slides/14/slides14.html#strategy",
    "href": "slides/14/slides14.html#strategy",
    "title": "CLT-based Confidence Intervals",
    "section": "Strategy",
    "text": "Strategy\n\nWe have an estimator \\(\\hat\\theta\\) in hand\nUse \\(\\hat\\theta\\) to find a range of plausible values for \\(\\theta\\) where\n\n\\[P(\\hat\\theta_L \\le \\theta \\le \\hat\\theta_U) = 1-\\alpha\\]"
  },
  {
    "objectID": "slides/14/slides14.html#central-limit-theorem",
    "href": "slides/14/slides14.html#central-limit-theorem",
    "title": "CLT-based Confidence Intervals",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf \\(X_1, ...., X_n\\) are iid normal random variables mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\]"
  },
  {
    "objectID": "slides/14/slides14.html#rnorm",
    "href": "slides/14/slides14.html#rnorm",
    "title": "CLT-based Confidence Intervals",
    "section": "rnorm",
    "text": "rnorm\nDraw random samples from a normal distribution\n\nrnorm(100)\n\n  [1]  0.689997495  0.243009315 -0.807401540  0.374458542 -0.915732432\n  [6]  0.178867707 -1.027265141  0.242949546 -1.842819078 -0.979080488\n [11]  0.600574438 -1.585153699 -0.264674428 -2.354315837 -1.485571171\n [16]  0.387874140 -1.512696626 -0.449453148  0.440283232  0.107874436\n [21]  0.893008371  1.306476920  1.052635158  1.248282276  0.238040548\n [26] -0.726755145 -0.921146552 -1.090287875  1.525105205  0.738510117\n [31] -0.581905367 -1.174651589 -0.647301509  1.236758361  1.329885783\n [36] -0.250956569  0.077823736  0.532409529 -0.868581185 -1.344989890\n [41] -0.055143175 -0.558497326  0.163625827  0.116062082  0.283748319\n [46] -0.185443284 -0.905502739 -0.378970109  2.444398064  1.545879204\n [51]  1.661460098 -1.765290580 -0.640270478 -0.179410439 -0.323670270\n [56]  0.818200954  0.869805697  0.583986275 -0.282794918 -0.455207421\n [61]  0.217383610 -0.023017063 -0.317057367 -0.594464928 -0.180541495\n [66]  0.902926695 -0.346319348 -1.719542799  0.375358299 -0.745854018\n [71] -0.215431321  1.442550814  0.556950688 -0.572394649  0.801313966\n [76] -0.035591859 -0.308874351  1.206091825  0.323264481  0.430756886\n [81] -0.826246153 -3.013584878  0.713907243  0.643560029 -0.323800012\n [86] -0.553170310  1.803857176  0.389636234 -0.953947150  0.075435359\n [91]  0.503398676  0.257374899 -0.009081139  0.065887151 -0.075173157\n [96] -1.128362548  1.121108578  0.987522633  0.528639124 -0.092888292\n\n\n\n\nAll distribution functions in R that start with rXXX draw random samples (rexp, rgamma, rbinom, etc)"
  },
  {
    "objectID": "slides/14/slides14.html#dnorm",
    "href": "slides/14/slides14.html#dnorm",
    "title": "CLT-based Confidence Intervals",
    "section": "dnorm",
    "text": "dnorm\nReturn the pdf evaluated at x\n\ndnorm(1)\n\n[1] 0.2419707"
  },
  {
    "objectID": "slides/14/slides14.html#pnorm",
    "href": "slides/14/slides14.html#pnorm",
    "title": "CLT-based Confidence Intervals",
    "section": "pnorm",
    "text": "pnorm\nReturn the cdf evaluated at x\n\npnorm(1)\n\n[1] 0.8413447"
  },
  {
    "objectID": "slides/14/slides14.html#qnorm",
    "href": "slides/14/slides14.html#qnorm",
    "title": "CLT-based Confidence Intervals",
    "section": "qnorm",
    "text": "qnorm\nReturn the quantile where the cdf is equal to x\n\nqnorm(.8413447)\n\n[1] 0.9999998"
  },
  {
    "objectID": "slides/14/slides14.html#try-it",
    "href": "slides/14/slides14.html#try-it",
    "title": "CLT-based Confidence Intervals",
    "section": "Try it",
    "text": "Try it\nFind the value of q that is needed for the following \\((1-\\alpha)100\\%\\) normal-based CIs:\n\n90%\n95%\n97%"
  },
  {
    "objectID": "slides/14/slides14.html#your-turn",
    "href": "slides/14/slides14.html#your-turn",
    "title": "CLT-based Confidence Intervals",
    "section": "Your turn",
    "text": "Your turn\nFind a 90% confidence interval for the mean bill length of Gentoo penguins.\nAssume that \\(\\sigma = 3.08\\)\nSample statistics:\n\\(\\bar{X}\\) = 47.5\nn = 123"
  },
  {
    "objectID": "slides/14/slides14.html#interpreting-cis-intro-stat-redux",
    "href": "slides/14/slides14.html#interpreting-cis-intro-stat-redux",
    "title": "CLT-based Confidence Intervals",
    "section": "Interpreting CI’s: intro stat redux",
    "text": "Interpreting CI’s: intro stat redux\n\nWe are \\((1-\\alpha)100\\)% confident that the true parameter of interest is between L and U"
  },
  {
    "objectID": "slides/14/slides14.html#section",
    "href": "slides/14/slides14.html#section",
    "title": "CLT-based Confidence Intervals",
    "section": "",
    "text": "(L, U) is a random interval before data are observed\nThe process by which the interval constructed is a random process\n\\((1-\\alpha)100\\)% is the long-run proportion of intervals that will capture the parameter\nIn practice, we don’t know which “type” of interval we have (good/bad)"
  },
  {
    "objectID": "slides/14/slides14.html#plug-in-principle",
    "href": "slides/14/slides14.html#plug-in-principle",
    "title": "CLT-based Confidence Intervals",
    "section": "Plug-in principle",
    "text": "Plug-in principle\nLet \\(X_1, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\).\n\n\n\\(\\dfrac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\)\nPROBLEM: \\(\\bar{x} \\pm z_{1-\\alpha/2} \\left( \\dfrac{\\sigma}{\\sqrt{n}} \\right)\\)\nIn practice both \\(\\mu\\) and \\(\\sigma^2\\) are unknown\nProposed solution: plug in the sample standard deviation \\(s\\)"
  },
  {
    "objectID": "slides/14/slides14.html#estimating-sigma-impacts-the-distribution",
    "href": "slides/14/slides14.html#estimating-sigma-impacts-the-distribution",
    "title": "CLT-based Confidence Intervals",
    "section": "Estimating \\(\\sigma\\) impacts the distribution",
    "text": "Estimating \\(\\sigma\\) impacts the distribution"
  },
  {
    "objectID": "slides/14/slides14.html#estimating-sigma-impacts-the-distribution-1",
    "href": "slides/14/slides14.html#estimating-sigma-impacts-the-distribution-1",
    "title": "CLT-based Confidence Intervals",
    "section": "Estimating \\(\\sigma\\) impacts the distribution",
    "text": "Estimating \\(\\sigma\\) impacts the distribution"
  },
  {
    "objectID": "slides/14/slides14.html#students-t-distribution",
    "href": "slides/14/slides14.html#students-t-distribution",
    "title": "CLT-based Confidence Intervals",
    "section": "(Student’s) t distribution",
    "text": "(Student’s) t distribution\nLet \\(T = \\dfrac{Z}{\\sqrt{V/df}}\\) where \\(Z \\sim N(0, 1)\\), \\(V \\sim \\chi^2_{df}\\), and \\(Z \\perp V \\Longrightarrow T \\sim t_{df}\\)"
  },
  {
    "objectID": "slides/14/slides14.html#t-distribution-properties",
    "href": "slides/14/slides14.html#t-distribution-properties",
    "title": "CLT-based Confidence Intervals",
    "section": "t distribution properties",
    "text": "t distribution properties\n\n\n\nSymmetric around 0\nFor \\(df=1\\), mean doesn’t exist (Cauchy distribution)\nFor \\(df \\ge 2\\), \\(E(T) = E(Z) E \\left(1 / \\sqrt{V/n} \\right) = 0\\)\nHeavier tails than normal distribution\n\\(t_{df} \\to N(0, 1)\\) as \\(df \\to \\infty\\)"
  },
  {
    "objectID": "slides/14/slides14.html#your-turn-finding-t-quantiles-in",
    "href": "slides/14/slides14.html#your-turn-finding-t-quantiles-in",
    "title": "CLT-based Confidence Intervals",
    "section": "Your turn: Finding t quantiles in ",
    "text": "Your turn: Finding t quantiles in \nqt(p, df) will calculate the p quantile of \\(t_{\\rm df}\\)\nFind the value of q that is needed for the following \\((1-\\alpha)100\\%\\) normal-based CIs:\n\n90%, n = 123\n95%, n = 25\n99%, n = 34\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/14/slides14.html#your-turn-1",
    "href": "slides/14/slides14.html#your-turn-1",
    "title": "CLT-based Confidence Intervals",
    "section": "Your turn",
    "text": "Your turn\nFind a 90% confidence interval for the mean bill length of Gentoo penguins.\nAssume that σ = 3.08.\nSample statistics:\n\n\\(n=123\\)\n\\(\\bar{x} = 47.5\\)\n\\(s = 3.08\\)"
  },
  {
    "objectID": "slides/14/slides14.html#underlying-validity-conditions",
    "href": "slides/14/slides14.html#underlying-validity-conditions",
    "title": "CLT-based Confidence Intervals",
    "section": "Underlying validity conditions",
    "text": "Underlying validity conditions\nWe have a random sample from a normal population distribution\n\n\nAsk Yourself…\n\nAre the observations independent?\nAre the observations approximately normal?"
  },
  {
    "objectID": "slides/14/slides14.html#checking-conditions",
    "href": "slides/14/slides14.html#checking-conditions",
    "title": "CLT-based Confidence Intervals",
    "section": "Checking conditions",
    "text": "Checking conditions\n\n\n\nAre the penguins independent?\nAre the bill lengths approximately normal?"
  },
  {
    "objectID": "slides/14/slides14.html#robustness",
    "href": "slides/14/slides14.html#robustness",
    "title": "CLT-based Confidence Intervals",
    "section": "Robustness",
    "text": "Robustness\nIf the a procedure “perform well” even if some of the assumptions under which they were developed do not hold, then they are called robust."
  },
  {
    "objectID": "slides/14/slides14.html#simulation-study",
    "href": "slides/14/slides14.html#simulation-study",
    "title": "CLT-based Confidence Intervals",
    "section": "Simulation study",
    "text": "Simulation study\nTo check whether a procedure is robust, we can use simulation:\n\nSimulate data from a variety of different probability distributions\nRun the procedure (e.g., build a one-sample t-interval)\nCompare the results of the procedure to what should have happened.\nfor a large number of CIs, approximately 95% of 95% CIs should capture the parameter value"
  },
  {
    "objectID": "slides/14/slides14.html#robustness-one-sample-t",
    "href": "slides/14/slides14.html#robustness-one-sample-t",
    "title": "CLT-based Confidence Intervals",
    "section": "Robustness: one-sample \\(t\\)",
    "text": "Robustness: one-sample \\(t\\)\n\n\nIf the population distribution is roughly symmetric and unimodal, then the procedure works well for sample sizes of at least 10–15 (just a rough guide)\nFor skewed population distributions, the t-procedure can be substantially affected, depending on the severity of the skew and the sample size.\nt-procedures are not resistant to outliers.\nIf observations are not independent, everything breaks"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html",
    "href": "notes/10-asymptotics/activity10-sols.html",
    "title": "10: Asymptotic Properties",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html#example-1",
    "href": "notes/10-asymptotics/activity10-sols.html#example-1",
    "title": "10: Asymptotic Properties",
    "section": "Example 1:",
    "text": "Example 1:\nLet \\(Y_1, ..., Y_n\\) be \\(n\\) Exp(\\(\\lambda\\)) random variables Let \\(\\widehat\\lambda = \\frac{n}{\\sum Y_i}\\). How does Var(\\(\\widehat\\lambda\\)) compare with the CRLB?\n\nlambda &lt;- .5\nn &lt;- 20\nn_sims &lt;- 10000\nlambda_hat &lt;- numeric(n_sims)\n\nfor(i in 1:n_sims){\n  sample &lt;- rexp(n, rate = lambda)\n  lambda_hat[i] &lt;- n/sum(sample)\n}\n\nvar(lambda_hat)\n\n[1] 0.0154848\n\nlambda^2/n\n\n[1] 0.0125"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html#example-1-1",
    "href": "notes/10-asymptotics/activity10-sols.html#example-1-1",
    "title": "10: Asymptotic Properties",
    "section": "Example 1:",
    "text": "Example 1:\nExample: The median \\(m\\) of an Exponential(\\(\\lambda\\)) distribution satisfies P(X ≤ m) = 0.5. Solving \\(1 - e^{-\\lambda m} = 0.5\\) gives \\(m = ln(2) / \\lambda\\). This suggests an estimator for \\(\\lambda\\) based on the median: \\(\\widehat{\\lambda} = \\frac{ln(2)}{m}\\). Finding the analytical variance of \\(\\widehat \\lambda\\) is complicated. Finding the sampling distribution of \\(m\\) is complicated, and finding the non-linear transformation is also complicated.\nUse simulation to see whether \\(\\frac{ln(2)}{m}\\) (a) is unbiased and (b) achieves the CRLB\n\nlambda &lt;- .5\nn &lt;- 200\nn_sims &lt;- 10000\nlambda_hat &lt;- numeric(n_sims)\n\nfor(i in 1:n_sims){\n  sample &lt;- rexp(n, rate = lambda)\n  lambda_hat[i] &lt;- log(2)/median(sample)\n}\n\nmean(lambda_hat)\n\n[1] 0.5023425\n\nlambda\n\n[1] 0.5\n\nvar(lambda_hat)\n\n[1] 0.002668319\n\nlambda^2/n\n\n[1] 0.00125"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html#running-means",
    "href": "notes/10-asymptotics/activity10-sols.html#running-means",
    "title": "10: Asymptotic Properties",
    "section": "Running Means",
    "text": "Running Means\nThe code below creates the “running mean” graph from the textbook for the Cauchy distribution. In your groups, talk through the steps and ask if you have questions. (It’s a little different than other simulations that we’ve seen!) Run the code chunk a couple of times to get a sense of the behavior of the mean of a Cauchy distribution.\n\nn &lt;- 10000\nrunning_mean &lt;- numeric(n)\nsample &lt;- rcauchy(n)\n\nfor(i in 1:n){\n  running_mean[i] &lt;- mean(sample[1:i])\n}\n\nggplot() + \n  geom_line(aes(x = 1:n, y = running_mean)) \n\n\n\n\n\n\n\n\nNow, use the code chunk below to do the same “running mean” example for a Normal(0,1) distribution. What do you notice? How is it the same/different from the Cauchy example above?\n\nn &lt;- 10000\nrunning_mean &lt;- numeric(n)\nsample &lt;- rnorm(n)\n\nfor(i in 1:n){\n  running_mean[i] &lt;- mean(sample[1:i])\n}\n\nggplot() + \n  geom_line(aes(x = 1:n, y = running_mean))"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html#variance-of-sample-means",
    "href": "notes/10-asymptotics/activity10-sols.html#variance-of-sample-means",
    "title": "10: Asymptotic Properties",
    "section": "Variance of sample means",
    "text": "Variance of sample means\nThe code chunk below runs a simulation comparing the variance of the sample mean of a Normal(0,1) distribution to the variance of the sample mean for a Cauchy distribution. Run it a few times to get a sense of the behavior, then try larger values of n. What do you notice? What does this mean about the consistency of the estimators?\n\nn &lt;- 20\nn_sims &lt;- 1000\nsample_mean_norm &lt;- numeric(n)\nsample_mean_cauchy &lt;- numeric(n)\n\nfor(i in 1:n_sims){\n  samp1 &lt;- rnorm(n)\n  samp2 &lt;- rcauchy(n)\n  sample_mean_norm[i] = mean(samp1)\n  sample_mean_cauchy[i] = mean(samp2)\n}\n\nvar(sample_mean_norm)\n\n[1] 0.05207371\n\nvar(sample_mean_cauchy)\n\n[1] 297.7243"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html",
    "href": "notes/10-asymptotics/activity10.html",
    "title": "10: Asymptotic Properties",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html#example-1",
    "href": "notes/10-asymptotics/activity10.html#example-1",
    "title": "10: Asymptotic Properties",
    "section": "Example 1:",
    "text": "Example 1:\nLet \\(Y_1, ..., Y_n\\) be \\(n\\) Exp(\\(\\lambda\\)) random variables Let \\(\\widehat\\lambda = \\frac{n}{\\sum Y_i}\\). How does Var(\\(\\widehat\\lambda\\)) compare with the CRLB?\n\nlambda &lt;- .5\nn &lt;- 20\nn_sims &lt;- 10000\nlambda_hat &lt;- numeric(n_sims)\n\nfor(i in 1:n_sims){\n  sample &lt;- rexp(n, rate = lambda)\n  lambda_hat[i] &lt;- n/sum(sample)\n}\n\nvar(lambda_hat)\n\n[1] 0.01488985\n\nlambda^2/n\n\n[1] 0.0125"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html#example-1-1",
    "href": "notes/10-asymptotics/activity10.html#example-1-1",
    "title": "10: Asymptotic Properties",
    "section": "Example 1:",
    "text": "Example 1:\nExample: The median \\(m\\) of an Exponential(\\(\\lambda\\)) distribution satisfies P(X ≤ m) = 0.5. Solving \\(1 - e^{-\\lambda m} = 0.5\\) gives \\(m = ln(2) / \\lambda\\). This suggests an estimator for \\(\\lambda\\) based on the median: \\(\\widehat{\\lambda} = \\frac{ln(2)}{m}\\). Finding the analytical variance of \\(\\widehat \\lambda\\) is complicated. Finding the sampling distribution of \\(m\\) is complicated, and finding the non-linear transformation is also complicated.\nUse simulation to see whether \\(\\frac{ln(2)}{m}\\) (a) is unbiased and (b) achieves the CRLB\n\n# your code here"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html#running-means",
    "href": "notes/10-asymptotics/activity10.html#running-means",
    "title": "10: Asymptotic Properties",
    "section": "Running Means",
    "text": "Running Means\nThe code below creates the “running mean” graph from the textbook for the Cauchy distribution. In your groups, talk through the steps and ask if you have questions. (It’s a little different than other simulations that we’ve seen!) Run the code chunk a couple of times to get a sense of the behavior of the mean of a Cauchy distribution.\n\nn &lt;- 10000\nrunning_mean &lt;- numeric(n)\nsample &lt;- rcauchy(n)\n\nfor(i in 1:n){\n  running_mean[i] &lt;- mean(sample[1:i])\n}\n\nggplot() + \n  geom_line(aes(x = 1:n, y = running_mean)) \n\n\n\n\n\n\n\n\nNow, use the code chunk below to do the same “running mean” example for a Normal(0,1) distribution. What do you notice? How is it the same/different from the Cauchy example above?\n\n# your code here"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html#variance-of-sample-means",
    "href": "notes/10-asymptotics/activity10.html#variance-of-sample-means",
    "title": "10: Asymptotic Properties",
    "section": "Variance of sample means",
    "text": "Variance of sample means\nThe code chunk below runs a simulation comparing the variance of the sample mean of a Normal(0,1) distribution to the variance of the sample mean for a Cauchy distribution. Run it a few times to get a sense of the behavior, then try larger values of n. What do you notice? What does this mean about the consistency of the estimators?\n\nn &lt;- 20\nn_sims &lt;- 1000\nsample_mean_norm &lt;- numeric(n)\nsample_mean_cauchy &lt;- numeric(n)\n\nfor(i in 1:n_sims){\n  samp1 &lt;- rnorm(n)\n  samp2 &lt;- rcauchy(n)\n  sample_mean_norm[i] = mean(samp1)\n  sample_mean_cauchy[i] = mean(samp2)\n}\n\nvar(sample_mean_norm)\n\n[1] 0.05104321\n\nvar(sample_mean_cauchy)\n\n[1] 939.2958"
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html",
    "href": "notes/13-intro-ci/activity13.html",
    "title": "13: Bootstrap",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)"
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html#theoretical-example",
    "href": "notes/13-intro-ci/activity13.html#theoretical-example",
    "title": "13: Bootstrap",
    "section": "Theoretical example",
    "text": "Theoretical example\n\nset.seed(1234)\nsample_data &lt;- rgamma(50, 2, 2)\nmean(sample_data)\n\n[1] 0.9245653\n\nsd(sample_data)\n\n[1] 0.8237878\n\n\n\nBootstrap distribution\n\nn_sims &lt;- 10^5\nboot_dsn &lt;- numeric(n_sims)\nfor(i in 1:n_sims){\n  boot_dsn[i] &lt;- mean(sample(sample_data, length(sample_data), replace = TRUE))\n}\n\nmean(boot_dsn)\n\n[1] 0.924705\n\nsd(boot_dsn)\n\n[1] 0.1157962\n\n\n\n\nSampling distribution\n\nn_sims &lt;- 10^5\nsampling_dsn &lt;- numeric(n_sims)\nfor(i in 1:n_sims){\n  sampling_dsn[i] &lt;- mean(rgamma(50, 2, 2))\n}\n\nmean(sampling_dsn)\n\n[1] 0.9998301\n\nsd(sampling_dsn)\n\n[1] 0.09989004"
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html#data-example",
    "href": "notes/13-intro-ci/activity13.html#data-example",
    "title": "13: Bootstrap",
    "section": "Data example",
    "text": "Data example\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\n# Subsetting to get only one species\ngentoo &lt;- dplyr::filter(penguins, species == \"Gentoo\")\n\n# Bookkeeping\ny &lt;- gentoo$bill_length_mm\nn &lt;- nrow(gentoo)        # sample size\nN &lt;- 10^4                # desired no. resamples\nboot_means &lt;- numeric(N) # a place to store the bootstrap stats\n\n# Resampling from the sample\nfor (i in 1:N) {\n  x &lt;- sample(y, size = n, replace = TRUE)\n  boot_means[i] &lt;- mean(x, na.rm = TRUE)  # you can choose other statistics\n}\n# Calculate a 95% percentile interval\nquantile(boot_means, probs = c(0.025, 0.975))"
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html#your-turn-theoretical-example",
    "href": "notes/13-intro-ci/activity13.html#your-turn-theoretical-example",
    "title": "13: Bootstrap",
    "section": "Your turn: theoretical example",
    "text": "Your turn: theoretical example\nConsider a population that has a gamma distribution with parameters r=5, 𝜆=1∕4.\n\nUse simulation (with n = 200) to generate an approximate sampling distribution of the mean; plot and describe the distribution.\nNow, draw one random sample of size 200 from this population. Create a histogram of your sample and find the mean and standard deviation.\nCompute the bootstrap distribution of the mean for your sample, plot it, and note the bootstrap mean and standard error.\nCompare the bootstrap distribution to the approximate theoretical sampling distribution by creating a table like slide 17\n\nRepeat (a)–(e) for sample sizes of n = 50 and n = 10. Describe carefully your observations about the effects of sample size on the bootstrap distribution."
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html#your-turn-data-example",
    "href": "notes/13-intro-ci/activity13.html#your-turn-data-example",
    "title": "13: Bootstrap",
    "section": "Your turn: data example",
    "text": "Your turn: data example\nThe Bangladesh data set contains information about arsenic, cobalt, and chlorine concentrations from a sample of 271 water wells in Bangladesh.\n\nlibrary(resampledata3)\nlibrary(tidyverse)\n\n\nConduct EDA on the chlorine concentrations and describe the salient features.\nFind the bootstrap distribution of the mean.\nFind and interpret the 95% bootstrap percentile confidence interval.\nWhat is the bootstrap estimate of the bias? What fraction of the bootstrap standard error does it represent?"
  },
  {
    "objectID": "notes/project/proj-ideas.html",
    "href": "notes/project/proj-ideas.html",
    "title": "Ideas for final project",
    "section": "",
    "text": "Note: I will continue to update this list as we go. If more than 2 groups propose the same project, I will ask you to coordinate to make sure your projects are different enough from one another.\n\nPotential Topics\n\nProof of Invariance of MLE\n\nIntroduction: Chihara & Hesterberg 6.3.5\nJournal article: Zehna, Peter (1966) “Invariance of Maximum Likelihood Estimators”\n\nExpectation-Maximization (E-M) algorithm\n\nIntroduction: Chihara & Hesterberg 13.8\nJournal article: Meng, Xiao-Li and David van Dyk (1997). “The EM Algorithm–An Old Folk-Song Sung to a Fast New Tune”. In: Journal of the Royal Statistical Society: Series B (Methodological)\n\nProof of the Cramer-Rao Lower Bound\n\nRice Ch8.7\n\nSufficiency and the Rao-Blackwell Theorem\n\nIntroduction: Chihara & Hesterberg online supplemental materials\n\nThe Zero-Inflated Poisson model and how to estimate parameters\n\nCounting the Unseen: Estimation of Susceptibility Proportions in Zero-Inflated Models Using a Conditional Likelihood Approach in: The American Statistician\n\nCapture-Recapture Model and how to estimate parameters\n\nCase study: Applications of Multiple Systems Estimation in Human Rights Research in The American Statistician\n\nSequential Tests/Optional Stopping for Hypothesis testing\n\nJournal article: Larsen et al 2024 “Statistical Challenges in Online Controlled Experiments: A Review of A/B Testing Methodology” In: The American Statistician\nJournal article: Wald 1945 “Sequential Tests of Statistical Hypotheses” in Annals of Mathematical Statistics\n\nThere are a number of heuristics for when we can use large-sample approximations (e.g. using the normal distribution instead of the T distribution, using the normal distribution instead of the binomial distribution, etc.). Show why these heuristics came to be and explore if/when they fail.\nResearch and provide a proof for the Neyman-Pearson Lemma, which states that the GLRT is at least as powerful as any other test with the same or smaller \\(\\alpha\\).\nResearch and provide a proof for Wilks’ Theorem, which states that if \\(\\lambda\\) is the GLRT statistic, \\(-2 \\ln \\lambda\\) is an approximate \\(\\chi^2\\) random variable. (Taylor expansion alert!!)\nEstimation for a Mixture of Normal Distributions\nResearch and summarize the history of the “p-value”\n\nBefore p &lt; 0.05 to Beyond p &lt; 0.05: Using History to Contextualize p-Values and Significance Testing in: The American Statistician\n\nMultiple testing and the distribution of p-values in published studies\n\nThe distribution of P-values in medical research articles suggested selective reporting associated with statistical significance\n\n\nYou are also welcome to propose your own topic! I encourage you to talk with me before submitting a proposal to make sure that it is appropriate for the class."
  },
  {
    "objectID": "notes/03-sampling-dists/activity03.html",
    "href": "notes/03-sampling-dists/activity03.html",
    "title": "03: Sampling Distributions",
    "section": "",
    "text": "library(tidyverse)\n\nThe goal for today is to understand the concept of a sampling distribution by comparing theoretical results with simulation results using R. We will investigate the sampling distributions of the sample mean.\nYou’ve been assigned to groups of 3-4 for this activity. You should submit your rendered file on gradescope as a group assignment. If folks from your assigned group did not show up, groups of 2 can combine into groups of 4.\n\nIntroduction\nA sampling distribution is the probability distribution of a statistic (like the sample mean \\(\\bar{x}\\) or sample variance \\(s^2\\)) obtained through a large number of samples drawn from a specific population. Understanding sampling distributions is crucial for statistical inference, as it allows us to make probability statements about sample statistics. In this activity, we will generate empirical sampling distributions through simulation and compare them to theoretical results.\nAlong the way, you’ll get some practice in R and some probability review.\n\n\nPart 1: Sampling distribution of the mean (Normal population)\nAssume data points \\(X_1, X_2, ..., X_n\\) are sampled from a normally distributed population\n\nIf \\(X_1, X_2, ..., X_n \\sim N(\\mu, \\sigma^2)\\), what is the expected value \\(E[\\bar{X}]\\) and theoretical standard deviation of \\(\\bar{X}\\) (also known as the standard error)?\nWhat is the theoretical distribution of \\(\\bar{X}\\)? (Note: you do not need to prove/derive this!)\n\nLet’s simulate this. Assume our population is \\(N(50, 10^2)\\) (so \\(\\sigma=10\\)). We will draw samples of size n = 25:\n\npop_mean &lt;- 50\npop_sd &lt;- 10\nsample_size &lt;- 25\nn_simulations &lt;- 100\n\nsample_means &lt;- numeric(n_simulations)\n\nfor(i in 1:n_simulations){\n  x &lt;- rnorm(sample_size, mean = pop_mean, sd = pop_sd)\n  sample_means[i] &lt;- mean(x)\n}\n\n\nWhat is the mean of sample_means? How close is it to \\(E[\\bar{X}]\\) above?\nWhat is the sd of sample_means? How close is it to the theoretical standard error you found above?\nThe code below makes a histogram of sample_means and overlays an incorrect theoretical density curve on top. Replace pop_mean and pop_sd with the theoretical values that you found in Q1.\n\n\ndf &lt;- data.frame(\n  sample_means = sample_means\n)\n\nggplot(df, aes(x = sample_means)) + \n  geom_histogram(aes(y = stat(density)), col = \"white\", bins = 20) + \n  stat_function(fun = dnorm, args = list(mean = pop_mean, s = pop_sd), col = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n\nTry different values for sample_size and n_simulations. When does the histogram match the theoretical density most closely?\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nYou have (hopefully) just demonstrated the Central Limit Theorem in action. Nice work! The CLT tells us that, as our sample size approaches \\(\\infty\\),\n\\[F_\\bar{X}(\\bar{X}) \\to N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\]\n\n\n\n\nPart 2: Non-normal population\nWe often use the Exponential distribution to model the time until an event occurs in a process where events happen randomly and independently at a constant average rate (a Poisson process). A common example is the time between consecutive arrivals of customers, like a coffee shop, during a period where the arrival rate is relatively stable. The key parameter is the rate \\(\\lambda\\) (e.g., average customers per minute) or its reciprocal, the mean time \\(\\mu = 1/\\lambda\\) (e.g., average minutes between customers). Individual inter-arrival times will follow a skewed distribution (many short times, fewer long times), but the CLT tells us something interesting about the average of these times if we look at many samples.\n\nLet \\(X\\) be the time (in minutes) between consecutive customer arrivals at a small coffee shop during a specific hour. Suppose this time can be modeled by an Exponential distribution. We observe that, on average over a long period of time, a customer arrives every 5 minutes. This average time is the mean of the distribution, \\(\\mu = 5\\) minutes. What is the rate, \\(\\lambda\\)? What is \\(E[X]\\) and \\(\\sigma_X\\)? (You can use the distributions cheat sheet in the back of the book)\nThe code below plots a theoretical exponential distribution with \\(\\lambda=1\\). Change it to the appropriate value.\n\n\n\n\n\n\n\n\n\n\n\nAccording to the CLT, if we take a sample of n = 40 consecutive inter-arrival times, what should the approximate distribution of the average inter-arrival time (\\(\\bar{X}\\)) be across many such samples? What are its approximate mean and standard error?\nLet’s check our answers through simulation. Replace all _____ with appropriate values in the code chunk below, and then remove the line with #| eval: false.\n\n\npop_lambda &lt;- ____\nsample_size &lt;- ____\nn_simulations &lt;- 1000\n\nsample_means &lt;- numeric(n_simulations)\n\nfor(i in 1:n_simulations){\n  x &lt;- rexp(sample_size, rate = pop_lambda)\n  sample_means[i] &lt;- mean(x)\n}\n\ndf &lt;- data.frame(\n  sample_means = sample_means\n)\n\nggplot(df, aes(x = sample_means)) + \n  geom_histogram(aes(y = stat(density)), col = \"white\", bins = 20) + \n  stat_function(fun = dnorm, args = list(mean = ______, s = ______), col = \"cornflowerblue\")\n\n\nHow well does the normal curve approximate the distribution of the average inter-arrival times, even though the distribution of individual inter-arrival times is heavily skewed (Exponential)?\nWhat happens if the sample size n is much smaller (e.g., \\(n=5\\))?\n\n\n\nPart 3: Discrete data\nThe true magic of the CLT is that it applies to any probability distribution, even discrete ones!\nExample: According to the 2004 American Community Survey, 28% of adults over 25 years old in Utah have completed a bachelor’s degree. In a random sample of 64 adults over age 25 from Utah, what is the probability that at least 30 have a bachelor’s degree?\nLet \\(X_i\\) indicate whether a sampled person has a bachelor’s degree. Then, \\(X_1, ...., X_n \\sim Binom(n,p)\\).\n\nWhat is \\(n\\) and what is \\(p\\)?\n\nWe are interested in how many people have a bachelor’s degree, or \\(\\frac{\\sum X_i}{n}\\). This is also an \\(\\bar{X}\\)!\n\nUse the CLT to find \\(E[\\hat{p}]\\) and \\(s_{\\hat{p}}\\).\nUse the normal distribution to find \\(P(\\bar{X} &gt; .3)\\) (Hint: use pnorm)\nAlternatively, we can answer this question through simulation. Use sample_means to estimate this probability. How close are you to your answer from (3)?\n\n\npop_p &lt;- .28\nsample_size &lt;- 64\nn_simulations &lt;- 1000\n\nsample_means &lt;- numeric(n_simulations)\n\nfor(i in 1:n_simulations){\n  x &lt;- rbinom(1, size = sample_size, prob = pop_p)\n  sample_means[i] &lt;- x/sample_size\n}\n\n\n\nPart 4: Summary and Reflection\n\nWhat is the difference between a population distribution, sample distribution, and distribution of a sample?\nWhy is the CLT important?\nGive one benefit of finding sampling distributions theoretically with probability\nGive one benefit of finding sampling distributions computationally through simulation\nDoes your group have any lingering questions for me?"
  },
  {
    "objectID": "notes/02-permutation/activity02.html",
    "href": "notes/02-permutation/activity02.html",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "The data for this example lives in the {Sleuth3} R package. This chunk of code loads in the data package, and the {tidyverse} package, which provides data wrangling and visualization code.\n\nlibrary(Sleuth3)\nlibrary(tidyverse)\n\nThe dataset is called case0101. We can print the first 10 rows of the dataset with the following:\n\ncase0101 |&gt;\n  slice_head(n = 10)\n\n   Score Treatment\n1    5.0 Extrinsic\n2    5.4 Extrinsic\n3    6.1 Extrinsic\n4   10.9 Extrinsic\n5   11.8 Extrinsic\n6   12.0 Extrinsic\n7   12.3 Extrinsic\n8   14.8 Extrinsic\n9   15.0 Extrinsic\n10  16.8 Extrinsic\n\n\nThe |&gt; is called a “pipe”, and tells R to take the output of the first line of code and “pipe” it into the second.\n\n\n\nThe next thing we should do is Eploratory Data Analysis, or EDA. In this class, that typically means (1) create a visualization and (2) compute summary statistics.\nWe’ll make the side-by-side boxplots from the slides:\n\ncase0101 |&gt;\n  ggplot(aes(x = Score, y = Treatment, fill = Treatment)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nYou can also use the {esquisse} package for this class.\nTo compute summary statistics, we can either use summary():\n\nsummary(case0101)\n\n     Score           Treatment \n Min.   : 5.00   Extrinsic:23  \n 1st Qu.:14.90   Intrinsic:24  \n Median :18.70                 \n Mean   :17.86                 \n 3rd Qu.:21.25                 \n Max.   :29.70                 \n\n\nor the favstats() function from the {mosaic} package. This function uses formula syntax, which says to group “Score” based on the “Treatment” variable.\n\nlibrary(mosaic) \nfavstats(Score ~ Treatment, data = case0101)\n\n  Treatment min     Q1 median    Q3  max     mean       sd  n missing\n1 Extrinsic   5 12.150   17.2 18.95 24.0 15.73913 5.252596 23       0\n2 Intrinsic  12 17.425   20.4 22.30 29.7 19.88333 4.439513 24       0\n\n\n\n\n\nWe’ll follow the code in the book to create two vectors, one for the “Extrinsic” group and one for the “Intrinsic” group. We can check that we’ve done this correctly using summary() and comparing it to the results from favstats()\n\nScore &lt;- case0101 |&gt;\n  pull(Score)\n\nScore_Extrinsic &lt;- case0101 |&gt;\n  filter(Treatment == \"Extrinsic\") |&gt;\n  pull(Score)\n\nsummary(Score_Extrinsic)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.00   12.15   17.20   15.74   18.95   24.00 \n\n\n\nScore_Intrinsic &lt;- case0101 |&gt;\n  filter(Treatment == \"Intrinsic\") |&gt;\n  pull(Score)\n\nsummary(Score_Intrinsic)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   17.43   20.40   19.88   22.30   29.70 \n\n\nSince our test statistic is the difference between the two means, we will compute the difference in means between Score_Extrinsic and Score_Intrinsic and save it to a vector called observed\n\nobserved &lt;- mean(Score_Intrinsic) - mean(Score_Extrinsic)\nobserved\n\n[1] 4.144203\n\n\n\n\n\nTo conduct the permutation test, we want to do a large number of permutations (called N in the code chunk below). The code in the for loop is run N times. Each time, we randomly sample observations in our dataset and assign them to the “Intrinsic” group. The observations that were not sampled are assigned to the “Extrinsic” group. In each simulation, we compute the difference in the means between the two groups and save it to our result vector.\n\nN &lt;- 10^4 - 1 # Number of permutations to do\nsample_size &lt;- nrow(case0101) # Sample size for each permutation (same as data)\n\nresult &lt;- numeric(N) # Create an empty vector to store results\nfor (i in 1:N){\n  index &lt;- sample(sample_size, 24, replace = FALSE) # Sample indices for group 1\n  result[i] &lt;- mean(Score[index]) - mean(Score[-index]) # Compute differences between groups\n}\n\n\n\n\nWe can visualize the results of the permutation distribution using {ggplot2}. I’ve also overlayed a red line that shows what our observed test statistic was.\n\nggplot() + \n  geom_histogram(aes(x = result), col = \"white\") + \n  geom_vline(xintercept = observed, linetype = \"dashed\", col = \"darkred\")\n\n\n\n\n\n\n\n\n\n\n\nThe following code chunk computes the fraction of simulations which resulted in a test statistic that was larger than the observed difference:\n\n(sum(result &gt;= observed) + 1)/(N+1)\n\n[1] 0.003"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#load-data",
    "href": "notes/02-permutation/activity02.html#load-data",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "The data for this example lives in the {Sleuth3} R package. This chunk of code loads in the data package, and the {tidyverse} package, which provides data wrangling and visualization code.\n\nlibrary(Sleuth3)\nlibrary(tidyverse)\n\nThe dataset is called case0101. We can print the first 10 rows of the dataset with the following:\n\ncase0101 |&gt;\n  slice_head(n = 10)\n\n   Score Treatment\n1    5.0 Extrinsic\n2    5.4 Extrinsic\n3    6.1 Extrinsic\n4   10.9 Extrinsic\n5   11.8 Extrinsic\n6   12.0 Extrinsic\n7   12.3 Extrinsic\n8   14.8 Extrinsic\n9   15.0 Extrinsic\n10  16.8 Extrinsic\n\n\nThe |&gt; is called a “pipe”, and tells R to take the output of the first line of code and “pipe” it into the second."
  },
  {
    "objectID": "notes/02-permutation/activity02.html#eda",
    "href": "notes/02-permutation/activity02.html#eda",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "The next thing we should do is Eploratory Data Analysis, or EDA. In this class, that typically means (1) create a visualization and (2) compute summary statistics.\nWe’ll make the side-by-side boxplots from the slides:\n\ncase0101 |&gt;\n  ggplot(aes(x = Score, y = Treatment, fill = Treatment)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nYou can also use the {esquisse} package for this class.\nTo compute summary statistics, we can either use summary():\n\nsummary(case0101)\n\n     Score           Treatment \n Min.   : 5.00   Extrinsic:23  \n 1st Qu.:14.90   Intrinsic:24  \n Median :18.70                 \n Mean   :17.86                 \n 3rd Qu.:21.25                 \n Max.   :29.70                 \n\n\nor the favstats() function from the {mosaic} package. This function uses formula syntax, which says to group “Score” based on the “Treatment” variable.\n\nlibrary(mosaic) \nfavstats(Score ~ Treatment, data = case0101)\n\n  Treatment min     Q1 median    Q3  max     mean       sd  n missing\n1 Extrinsic   5 12.150   17.2 18.95 24.0 15.73913 5.252596 23       0\n2 Intrinsic  12 17.425   20.4 22.30 29.7 19.88333 4.439513 24       0"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#compute-test-statistic",
    "href": "notes/02-permutation/activity02.html#compute-test-statistic",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "We’ll follow the code in the book to create two vectors, one for the “Extrinsic” group and one for the “Intrinsic” group. We can check that we’ve done this correctly using summary() and comparing it to the results from favstats()\n\nScore &lt;- case0101 |&gt;\n  pull(Score)\n\nScore_Extrinsic &lt;- case0101 |&gt;\n  filter(Treatment == \"Extrinsic\") |&gt;\n  pull(Score)\n\nsummary(Score_Extrinsic)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.00   12.15   17.20   15.74   18.95   24.00 \n\n\n\nScore_Intrinsic &lt;- case0101 |&gt;\n  filter(Treatment == \"Intrinsic\") |&gt;\n  pull(Score)\n\nsummary(Score_Intrinsic)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   17.43   20.40   19.88   22.30   29.70 \n\n\nSince our test statistic is the difference between the two means, we will compute the difference in means between Score_Extrinsic and Score_Intrinsic and save it to a vector called observed\n\nobserved &lt;- mean(Score_Intrinsic) - mean(Score_Extrinsic)\nobserved\n\n[1] 4.144203"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#conduct-permutations",
    "href": "notes/02-permutation/activity02.html#conduct-permutations",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "To conduct the permutation test, we want to do a large number of permutations (called N in the code chunk below). The code in the for loop is run N times. Each time, we randomly sample observations in our dataset and assign them to the “Intrinsic” group. The observations that were not sampled are assigned to the “Extrinsic” group. In each simulation, we compute the difference in the means between the two groups and save it to our result vector.\n\nN &lt;- 10^4 - 1 # Number of permutations to do\nsample_size &lt;- nrow(case0101) # Sample size for each permutation (same as data)\n\nresult &lt;- numeric(N) # Create an empty vector to store results\nfor (i in 1:N){\n  index &lt;- sample(sample_size, 24, replace = FALSE) # Sample indices for group 1\n  result[i] &lt;- mean(Score[index]) - mean(Score[-index]) # Compute differences between groups\n}"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#make-plot-of-test-statistics",
    "href": "notes/02-permutation/activity02.html#make-plot-of-test-statistics",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "We can visualize the results of the permutation distribution using {ggplot2}. I’ve also overlayed a red line that shows what our observed test statistic was.\n\nggplot() + \n  geom_histogram(aes(x = result), col = \"white\") + \n  geom_vline(xintercept = observed, linetype = \"dashed\", col = \"darkred\")"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#compute-p-value",
    "href": "notes/02-permutation/activity02.html#compute-p-value",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "The following code chunk computes the fraction of simulations which resulted in a test statistic that was larger than the observed difference:\n\n(sum(result &gt;= observed) + 1)/(N+1)\n\n[1] 0.003"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#eda-1",
    "href": "notes/02-permutation/activity02.html#eda-1",
    "title": "02: Permutation Tests",
    "section": "EDA",
    "text": "EDA\nCreate an appropriate EDA\n\n# your graph code here\n\n\n# your summary code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#test-statistic",
    "href": "notes/02-permutation/activity02.html#test-statistic",
    "title": "02: Permutation Tests",
    "section": "Test statistic",
    "text": "Test statistic\nCompute the appropriate test statistic\n\n# your code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#conduct-permutations-1",
    "href": "notes/02-permutation/activity02.html#conduct-permutations-1",
    "title": "02: Permutation Tests",
    "section": "Conduct Permutations",
    "text": "Conduct Permutations\n\n# your permutation code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#make-plot-of-test-statistics-1",
    "href": "notes/02-permutation/activity02.html#make-plot-of-test-statistics-1",
    "title": "02: Permutation Tests",
    "section": "Make plot of test statistics",
    "text": "Make plot of test statistics\n\n# your code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#compute-p-value-1",
    "href": "notes/02-permutation/activity02.html#compute-p-value-1",
    "title": "02: Permutation Tests",
    "section": "Compute p-value",
    "text": "Compute p-value\n\n# your code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#what-do-you-conclude",
    "href": "notes/02-permutation/activity02.html#what-do-you-conclude",
    "title": "02: Permutation Tests",
    "section": "What do you conclude?",
    "text": "What do you conclude?"
  },
  {
    "objectID": "readings/r-homework-stat250.html",
    "href": "readings/r-homework-stat250.html",
    "title": "Using R for Homework in Stat250",
    "section": "",
    "text": "In Stat250, I will distribute homework assignment templates in either .rmd or .qmd format."
  },
  {
    "objectID": "readings/r-homework-stat250.html#uploading-files-to-maize",
    "href": "readings/r-homework-stat250.html#uploading-files-to-maize",
    "title": "Using R for Homework in Stat250",
    "section": "Uploading files to maize",
    "text": "Uploading files to maize\n\nClick the “upload” button in the files pane\nSelect the file from your local computer (likely in your Downloads folder if you downloaded from moodle)\nChoose the “Target Directory” – this is the folder on your maize account where the file will be saved. I recommend creating a “Stat250” folder for this class"
  },
  {
    "objectID": "readings/r-homework-stat250.html#accessing-maize-from-off-campus",
    "href": "readings/r-homework-stat250.html#accessing-maize-from-off-campus",
    "title": "Using R for Homework in Stat250",
    "section": "Accessing maize from off campus",
    "text": "Accessing maize from off campus\nIf you are using the maize server, you should be able to access it on campus with only your Carleton ID and password. If you plan to use the maize server and you plan to do any work off campus this term (e.g., while on a field trip, travel for athletics, or just sitting in Little Joy) you need to install Carleton’s VPN to have access.\nTo install the GlobalProtect VPN follow directions provided by ITS."
  },
  {
    "objectID": "readings/r-homework-stat250.html#installing-latex-not-needed-if-you-are-using-the-maize-server",
    "href": "readings/r-homework-stat250.html#installing-latex-not-needed-if-you-are-using-the-maize-server",
    "title": "Using R for Homework in Stat250",
    "section": "Installing LaTeX (not needed if you are using the maize server)",
    "text": "Installing LaTeX (not needed if you are using the maize server)\nIf you don’t already have a tex package installed on your computer, the easiest option to create pdf’s is to use the tinytex R package. This can be installed with the following R commands:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()  # install TinyTeX\n\nIf you’d like a standalone LaTeX package that will work with programs other than RStudio, you could install the basic installations of either:\nIf you’d like a stand alone LaTeX package, you could install the basic installations of either:\n\nMacTeX for Mac (3.2GB!)\nMiKTeX for Windows (190MB)"
  }
]