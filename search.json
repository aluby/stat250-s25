[
  {
    "objectID": "readings/r-basics.html",
    "href": "readings/r-basics.html",
    "title": "R Basics",
    "section": "",
    "text": "For example, the data used in our textbook problems is included in the {resampledata3} R package. It is already installed on maize, but if you’re using a local version of R, you’ll need to install it by going to tools –&gt; install packages and then typing the name of the package in.\n\nlibrary(resampledata3)\n\n\nAttaching package: 'resampledata3'\n\n\nThe following object is masked from 'package:datasets':\n\n    Titanic\n\n\nIn this course, we’ll use a lot of tools found in the tidyverse of R packages. To load many of these packages at once, you can use the library(&lt;package_name&gt;) command. So to load the tidyverse we run:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we see a lot of extra info printed when we load the tidyverse. These messages are just telling you what packages are now available to you and warning you that a few functions (e.g., filter) has been replaced by the tidyverse version. We’ll see how to suppress these messages later."
  },
  {
    "objectID": "readings/r-basics.html#loading-r-packages",
    "href": "readings/r-basics.html#loading-r-packages",
    "title": "R Basics",
    "section": "",
    "text": "For example, the data used in our textbook problems is included in the {resampledata3} R package. It is already installed on maize, but if you’re using a local version of R, you’ll need to install it by going to tools –&gt; install packages and then typing the name of the package in.\n\nlibrary(resampledata3)\n\n\nAttaching package: 'resampledata3'\n\n\nThe following object is masked from 'package:datasets':\n\n    Titanic\n\n\nIn this course, we’ll use a lot of tools found in the tidyverse of R packages. To load many of these packages at once, you can use the library(&lt;package_name&gt;) command. So to load the tidyverse we run:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we see a lot of extra info printed when we load the tidyverse. These messages are just telling you what packages are now available to you and warning you that a few functions (e.g., filter) has been replaced by the tidyverse version. We’ll see how to suppress these messages later."
  },
  {
    "objectID": "readings/r-basics.html#creating-and-naming-objects",
    "href": "readings/r-basics.html#creating-and-naming-objects",
    "title": "R Basics",
    "section": "Creating and naming objects",
    "text": "Creating and naming objects\nAll R statements where you create objects have the form:\n\nobject_name &lt;- value\n\n\n\n\n\n\n\nNote\n\n\n\nCan’t I use the equal operator (=) to assign objects? Of course you can! The assignment operator (&lt;-) is unique to R and I try to use it when assigning objects. If the = sign comes more naturally to you, that’s also just fine\n\n\nAt first, we’ll be creating a lot of data objects. For example, we an load a data set containing the ratings for each episode of The Office using the code\n\noffice_ratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv\")\n\nIn this class you will be creating a lot of objects, so you’ll need to come up with names for those objects. Trying to think of informative/meaningful names for objects is hard, but necessary work! Below are the fundamental rules for naming objects in R:\n\nnames can’t start with a number\nnames are case-sensitive\nsome common letters are used internally by R and should be avoided as variable names (c, q, t, C, D, F, T, I)\nThere are reserved words that R won’t let you use for variable names (for, in, while, if, else, repeat, break, next)\nR will let you use the name of a predefined function—but don’t do it!\n\nYou can always check to see if you the name you want to use is already taken via exists():\nFor example lm exists\n\nexists(\"lm\")\n\n[1] TRUE\n\n\nbut carleton_college doesn’t.\n\nexists(\"carleton_college\")\n\n[1] FALSE\n\n\nThere are also a lot of naming styles out there, and if you have coded in another language, you may have already developed a preference. Below is an illustration by Allison Horst\n\n\n\n\n\n\n\n\n\nI generally following the tidyverse style guide, so you’ll see that I use only lowercase letters, numbers, and _ (snake case)."
  },
  {
    "objectID": "readings/r-basics.html#overviews-of-data-frames",
    "href": "readings/r-basics.html#overviews-of-data-frames",
    "title": "R Basics",
    "section": "Overviews of data frames",
    "text": "Overviews of data frames\nAbove, you loaded in a data set called office_ratings. Data sets are stored as a special data structure called a data frame. Data frames are the most-commonly used data structure for data analysis in R. For now, think of them like spreadsheets.\nOnce you have your data frame, you can get a quick overview of it using a few commands (below I use data_set as a generic placeholder for the data frame’s name):\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nhead(data_set)\nprint the first 6 rows\n\n\ntail(data_set)\nprint the last 6 rows\n\n\nglimpse(data_set)\na quick overview where columns run down the screen and the data values run across. This allows you to see every column in the data frame.\n\n\nstr(data_set)\na quick overview like glimpse(), but without some of the formatting\n\n\nsummary(data_set)\nquick summary statistics for each column\n\n\ndim(data_set)\nthe number of rows and columns\n\n\nnrow(data_set)\nthe number of rows\n\n\nncol(data_set)\nthe number of columns\n\n\n\n## Tibbles\nA tibble, or a tbl_df is another version of a data frame which is used by default in a lot of the tidyverse packages that we’ll use.\n\nTibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\n\n\n\n\n\n\n\n Check point\n\n\n\nRun the above commands on the office_ratings data set. Compare and contrast the information returned by each command.\n\n\n\n\n\n\n\n\nGetting a spreadsheet\n\n\n\nIn RStudio, you can run the command View(data_set) to pull up a spreadsheet representation of a data frame. You can also click on the name of the data frame in the Environment pane. This can be a great way help you think about the data, and even has some interactive functions (e.g., filtering and searching); however, never include View(data_set) in an .Rmd file!!\n\n\n\n\n\n\n\n\nReview from intro stats\n\n\n\nIn intro stats we used the terms cases (or observations) and variables to describe the rows and columns of a data frame, respectively."
  },
  {
    "objectID": "readings/r-basics.html#extracting-pieces-of-data-frames",
    "href": "readings/r-basics.html#extracting-pieces-of-data-frames",
    "title": "R Basics",
    "section": "Extracting pieces of data frames",
    "text": "Extracting pieces of data frames\nSince data frames are the fundamental data structure for most analyses in R, it’s important to know how to work with them. You already know how to get an overview of a data frame, but that isn’t always very informative. Often, you want to extract pieces of a data frame, such as a specific column or row.\n\nExtracting rows\nData frames can be indexed by their row/column numbers. To extract elements of a data frame, the basic syntax is data_set[row.index, column.index]. So, to extract the 10th row of office_ratings we run\n\noffice_ratings[10, ]\n\n# A tibble: 1 × 6\n  season episode title    imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      2       4 The Fire         8.4        2713 2005-10-11\n\n\nNotice that to extract an entire row, we leave the column index position blank.\nWe can also extract multiple rows by creating a vector of row indices. For example, we can extract the first 5 rows via\n\noffice_ratings[1:5, ]\n\n# A tibble: 5 × 6\n  season episode title         imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      1       1 Pilot                 7.6        3706 2005-03-24\n2      1       2 Diversity Day         8.3        3566 2005-03-29\n3      1       3 Health Care           7.9        2983 2005-04-05\n4      1       4 The Alliance          8.1        2886 2005-04-12\n5      1       5 Basketball            8.4        3179 2005-04-19\n\n\nHere, 1:5 create a sequence of integers from 1 to 5.\nWe could also specify arbitrary row index values by combing the values into a vector. For example, we could extract the 1st, 13th, 64th, and 128th rows via\n\noffice_ratings[c(1, 13, 64, 128), ]\n\n# A tibble: 4 × 6\n  season episode title            imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      1       1 Pilot                    7.6        3706 2005-03-24\n2      2       7 The Client               8.6        2631 2005-11-08\n3      4      13 Job Fair                 7.9        1977 2008-05-08\n4      7      11 Classy Christmas         8.9        2138 2010-12-09\n\n\n\n\nExtracting columns\nSimilar to extracting rows, we can use a numeric index to extract the columns of a data frame. For example, to extract the 3rd column, we can run\n\noffice_ratings[,3]\n\n# A tibble: 188 × 1\n   title            \n   &lt;chr&gt;            \n 1 Pilot            \n 2 Diversity Day    \n 3 Health Care      \n 4 The Alliance     \n 5 Basketball       \n 6 Hot Girl         \n 7 The Dundies      \n 8 Sexual Harassment\n 9 Office Olympics  \n10 The Fire         \n# ℹ 178 more rows\n\n\nAlternatively, we can pass in the column name in quotes instead of the column number\n\noffice_ratings[,\"title\"]\n\n# A tibble: 188 × 1\n   title            \n   &lt;chr&gt;            \n 1 Pilot            \n 2 Diversity Day    \n 3 Health Care      \n 4 The Alliance     \n 5 Basketball       \n 6 Hot Girl         \n 7 The Dundies      \n 8 Sexual Harassment\n 9 Office Olympics  \n10 The Fire         \n# ℹ 178 more rows\n\n\nNotice that the extracted column is still formatted as a data frame (or tibble). If you want to extract the contents of the column and just have a vector of titles, you have a few options.\n\nYou could use double brackets with the column number:\n\n\noffice_ratings[[3]]\n\n\nYou could use double brackets with the column name in quotes:\n\n\noffice_ratings[[\"title\"]]\n\n\nYou could use the $ extractor with the column name (not in quotes):\n\n\noffice_ratings$title\n\n\n\n\n\n\n\n Check point\n\n\n\n\nExtract the 35th row of office_ratings.\nExtract rows 35, 36, 37, and 38 of office_ratings.\nExtract the imdb_rating column from office ratings using the column index number.\nExtract the imdb_rating column from office ratings using the column name."
  },
  {
    "objectID": "readings/r-basics.html#lists-very-optional-for-stat250",
    "href": "readings/r-basics.html#lists-very-optional-for-stat250",
    "title": "R Basics",
    "section": "Lists very optional for Stat250",
    "text": "Lists very optional for Stat250\nIt turns out that data frames are special cases of lists, a more general data structure. In a data frame, each column is an element of the data list and each column must be of the same length. In general, lists can be comprised of elements of vastly different lengths and data types.\nAs an example, let’s construct a list of the faculty in the MAST department and what is being taught this winter.\n\nstat_faculty &lt;- c(\"Kelling\", \"Loy\", \"Luby\", \"Poppick\", \"St. Clair\", \"Wadsworth\")\nstat_courses &lt;- c(120, 220, 230, 250, 285, 330)\nmath_faculty &lt;- c(\"Brooke\", \"Davis\", \"Egge\", \"Gomez-Gonzales\", \"Haunsperger\", \"Johnson\", \n                  \"Meyer\", \"Montee\", \"Shrestha\",\"Terry\", \"Thompson\", \"Turnage-Butterbaugh\")\nmath_courses &lt;- c(101, 106, 111, 120, 210, 211, 232, 236, 240, 241, 251, 321, 333, 395)\n\nmast &lt;- list(stat_faculty = stat_faculty, stat_courses = stat_courses, \n             math_faculty = math_faculty, math_courses = math_courses)\n\n\nOverview of a list\nYou can get an overview of a list a few ways:\n\nglimpse(list_name) and str(list_name) list the elements of the list and the first few entries of each element.\n\n\nglimpse(mast)\n\nList of 4\n $ stat_faculty: chr [1:6] \"Kelling\" \"Loy\" \"Luby\" \"Poppick\" ...\n $ stat_courses: num [1:6] 120 220 230 250 285 330\n $ math_faculty: chr [1:12] \"Brooke\" \"Davis\" \"Egge\" \"Gomez-Gonzales\" ...\n $ math_courses: num [1:14] 101 106 111 120 210 211 232 236 240 241 ...\n\n\n\nlength(list_name) will tell you how many elements are in the list\n\n\nlength(mast)\n\n[1] 4\n\n\n\n\nExtracting elements of a list\nSince data frames are lists, you’ve already seen how to extract elements of a list. For example, to extract the stat_faculty you could run\n\nmast[[1]]\n\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\nor\n\nmast[[\"stat_faculty\"]]\n\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you had only used a single bracket above, the returned object would still be a list, which is typically not what we would want.\n\nmast[1]\n\n$stat_faculty\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\n\n\n\n\n\n\n\n\n Check point\n\n\n\nExtract the statistics courses offered this term."
  },
  {
    "objectID": "readings/r-basics.html#vectors",
    "href": "readings/r-basics.html#vectors",
    "title": "R Basics",
    "section": "Vectors",
    "text": "Vectors\nThe columns of the office_ratings data frame and the elements of the mast list were comprised of (atomic) vectors. Unlike lists, all elements within a vector share the same type. For example, all names in the stat_faculty vector were character strings and all ratings in the imdb_rating column were numeric. We’ll deal with a variety of types of vectors in this course, including:\n\nnumeric\ncharacter (text)\nlogical (TRUE/FALSE)\n\n\nExtracting elements of a vector\nJust like with lists (and therefore data frames), we use brackets to extract elements from a vector. As an example, let’s work with the title column from office_ratings.\n\ntitle &lt;- office_ratings$title # vector of titles\n\nTo extract the 111th title, we run\n\ntitle[111]\n\n[1] \"New Leads\"\n\n\nor two extract the 100th through 111th titles, we run\n\ntitle[100:111]\n\n [1] \"Double Date\"          \"Murder\"               \"Shareholder Meeting\" \n [4] \"Scott's Tots\"         \"Secret Santa\"         \"The Banker\"          \n [7] \"Sabre\"                \"Manager and Salesman\" \"The Delivery: Part 1\"\n[10] \"The Delivery: Part 2\" \"St. Patrick's Day\"    \"New Leads\"           \n\n\n\n\nNegative indices\nSometimes, we want to “kick out” elements of our vector. To do this, we can use a negative index value. For example,\n\ntitle[-1]\n\nreturns all but the first title—that is, it kicks out the first title. To kick out multiple elements, we need to negate a vector of indices. For example, below we kick out the first 10 titles\n\ntitle[-c(1:10)]\n\nAnd now we kick out the 5th, 50th, and 150th titles\n\ntitle[-c(5, 50, 150)]\n\nThis idea can be adapted to lists and data frames. For example, to kick out the first row of office_ratings, we run\n\noffice_ratings[-1,]\n\n# A tibble: 187 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       2 Diversity Day             8.3        3566 2005-03-29\n 2      1       3 Health Care               7.9        2983 2005-04-05\n 3      1       4 The Alliance              8.1        2886 2005-04-12\n 4      1       5 Basketball                8.4        3179 2005-04-19\n 5      1       6 Hot Girl                  7.8        2852 2005-04-26\n 6      2       1 The Dundies               8.7        3213 2005-09-20\n 7      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 8      2       3 Office Olympics           8.4        2742 2005-10-04\n 9      2       4 The Fire                  8.4        2713 2005-10-11\n10      2       5 Halloween                 8.2        2561 2005-10-18\n# ℹ 177 more rows\n\n\nor to kick out the math courses from the mast list we run\n\nmast[-4]\n\n$stat_faculty\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n$stat_courses\n[1] 120 220 230 250 285 330\n\n$math_faculty\n [1] \"Brooke\"              \"Davis\"               \"Egge\"               \n [4] \"Gomez-Gonzales\"      \"Haunsperger\"         \"Johnson\"            \n [7] \"Meyer\"               \"Montee\"              \"Shrestha\"           \n[10] \"Terry\"               \"Thompson\"            \"Turnage-Butterbaugh\"\n\n\n\n\nLogical indices\nIt’s great to be able to extract (or omit) elements using indices, but sometimes we don’t know what index value we should use. For example, if you wanted to extract all of the 300-level statistics courses from the stat_courses vector, you would need to manually determine that positions 2:5 meet that requirement. That’s a lot of work! A better alternative is to allow R to find the elements meeting that requirement using logical operators. Below is a table summarizing common logical operators in R.\n\n\n\nComparison\nMeaning\n\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nis equal to\n\n\n!=\nnot equal to\n\n\n\nIn order to extract the 300-level statistics courses, we’ll take two steps:\n\nWe’ll determine whether each course is numbered at least 300,\nthen we’ll use that sequence of TRUEs/FALSEs to extract the course.\n\nSo, first we use the logical operator &gt;= to compare stat_courses and 300. This returns TRUE if the element meets the specification and FALSE otherwise.\n\nstat_courses &gt;= 300\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nNow, we can use this vector as our index. Only the TRUE elements will be extracted:\n\nstat_courses[stat_courses &gt;= 300]\n\n[1] 330\n\n\nThe same idea can be used with data frames and lists, just remember how to format the brackets and indices!\n\n\n\n\n\n\n Check point\n\n\n\n\nExtract all statistics courses below 250 from stat_courses.\nExtract all math courses except for 240 (probability) from math_courses.\nExtract all rows from season 3 of The Office."
  },
  {
    "objectID": "notes/03-sampling-dists/activity03.html",
    "href": "notes/03-sampling-dists/activity03.html",
    "title": "03: Sampling Distributions",
    "section": "",
    "text": "library(tidyverse)\n\nThe goal for today is to understand the concept of a sampling distribution by comparing theoretical results with simulation results using R. We will investigate the sampling distributions of the sample mean.\nYou’ve been assigned to groups of 3-4 for this activity. You should submit your rendered file on gradescope as a group assignment. If folks from your assigned group did not show up, groups of 2 can combine into groups of 4.\n\nIntroduction\nA sampling distribution is the probability distribution of a statistic (like the sample mean \\(\\bar{x}\\) or sample variance \\(s^2\\)) obtained through a large number of samples drawn from a specific population. Understanding sampling distributions is crucial for statistical inference, as it allows us to make probability statements about sample statistics. In this activity, we will generate empirical sampling distributions through simulation and compare them to theoretical results.\nAlong the way, you’ll get some practice in R and some probability review.\n\n\nPart 1: Sampling distribution of the mean (Normal population)\nAssume data points \\(X_1, X_2, ..., X_n\\) are sampled from a normally distributed population\n\nIf \\(X_1, X_2, ..., X_n \\sim N(\\mu, \\sigma^2)\\), what is the expected value \\(E[\\bar{X}]\\) and theoretical standard deviation of \\(\\bar{X}\\) (also known as the standard error)?\nWhat is the theoretical distribution of \\(\\bar{X}\\)? (Note: you do not need to prove/derive this!)\n\nLet’s simulate this. Assume our population is \\(N(50, 10^2)\\) (so \\(\\sigma=10\\)). We will draw samples of size n = 25:\n\npop_mean &lt;- 50\npop_sd &lt;- 10\nsample_size &lt;- 25\nn_simulations &lt;- 100\n\nsample_means &lt;- numeric(n_simulations)\n\nfor(i in 1:n_simulations){\n  x &lt;- rnorm(sample_size, mean = pop_mean, sd = pop_sd)\n  sample_means[i] &lt;- mean(x)\n}\n\n\nWhat is the mean of sample_means? How close is it to \\(E[\\bar{X}]\\) above?\nWhat is the sd of sample_means? How close is it to the theoretical standard error you found above?\nThe code below makes a histogram of sample_means and overlays an incorrect theoretical density curve on top. Replace pop_mean and pop_sd with the theoretical values that you found in Q1.\n\n\ndf &lt;- data.frame(\n  sample_means = sample_means\n)\n\nggplot(df, aes(x = sample_means)) + \n  geom_histogram(aes(y = stat(density)), col = \"white\", bins = 20) + \n  stat_function(fun = dnorm, args = list(mean = pop_mean, s = pop_sd), col = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n\nTry different values for sample_size and n_simulations. When does the histogram match the theoretical density most closely?\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nYou have (hopefully) just demonstrated the Central Limit Theorem in action. Nice work! The CLT tells us that, as our sample size approaches \\(\\infty\\),\n\\[F_\\bar{X}(\\bar{X}) \\to N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\]\n\n\n\n\nPart 2: Non-normal population\nWe often use the Exponential distribution to model the time until an event occurs in a process where events happen randomly and independently at a constant average rate (a Poisson process). A common example is the time between consecutive arrivals of customers, like a coffee shop, during a period where the arrival rate is relatively stable. The key parameter is the rate \\(\\lambda\\) (e.g., average customers per minute) or its reciprocal, the mean time \\(\\mu = 1/\\lambda\\) (e.g., average minutes between customers). Individual inter-arrival times will follow a skewed distribution (many short times, fewer long times), but the CLT tells us something interesting about the average of these times if we look at many samples.\n\nLet \\(X\\) be the time (in minutes) between consecutive customer arrivals at a small coffee shop during a specific hour. Suppose this time can be modeled by an Exponential distribution. We observe that, on average over a long period of time, a customer arrives every 5 minutes. This average time is the mean of the distribution, \\(\\mu = 5\\) minutes. What is the rate, \\(\\lambda\\)? What is \\(E[X]\\) and \\(\\sigma_X\\)? (You can use the distributions cheat sheet in the back of the book)\nThe code below plots a theoretical exponential distribution with \\(\\lambda=1\\). Change it to the appropriate value.\n\n\n\n\n\n\n\n\n\n\n\nAccording to the CLT, if we take a sample of n = 40 consecutive inter-arrival times, what should the approximate distribution of the average inter-arrival time (\\(\\bar{X}\\)) be across many such samples? What are its approximate mean and standard error?\nLet’s check our answers through simulation. Replace all _____ with appropriate values in the code chunk below, and then remove the line with #| eval: false.\n\n\npop_lambda &lt;- ____\nsample_size &lt;- ____\nn_simulations &lt;- 1000\n\nsample_means &lt;- numeric(n_simulations)\n\nfor(i in 1:n_simulations){\n  x &lt;- rexp(sample_size, rate = pop_lambda)\n  sample_means[i] &lt;- mean(x)\n}\n\ndf &lt;- data.frame(\n  sample_means = sample_means\n)\n\nggplot(df, aes(x = sample_means)) + \n  geom_histogram(aes(y = stat(density)), col = \"white\", bins = 20) + \n  stat_function(fun = dnorm, args = list(mean = ______, s = ______), col = \"cornflowerblue\")\n\n\nHow well does the normal curve approximate the distribution of the average inter-arrival times, even though the distribution of individual inter-arrival times is heavily skewed (Exponential)?\nWhat happens if the sample size n is much smaller (e.g., \\(n=5\\))?\n\n\n\nPart 3: Discrete data\nThe true magic of the CLT is that it applies to any probability distribution, even discrete ones!\nExample: According to the 2004 American Community Survey, 28% of adults over 25 years old in Utah have completed a bachelor’s degree. In a random sample of 64 adults over age 25 from Utah, what is the probability that at least 30 have a bachelor’s degree?\nLet \\(X_i\\) indicate whether a sampled person has a bachelor’s degree. Then, \\(X_1, ...., X_n \\sim Binom(n,p)\\).\n\nWhat is \\(n\\) and what is \\(p\\)?\n\nWe are interested in how many people have a bachelor’s degree, or \\(\\frac{\\sum X_i}{n}\\). This is also an \\(\\bar{X}\\)!\n\nUse the CLT to find \\(E[\\hat{p}]\\) and \\(s_{\\hat{p}}\\).\nUse the normal distribution to find \\(P(\\bar{X} &gt; .3)\\) (Hint: use pnorm)\nAlternatively, we can answer this question through simulation. Use sample_means to estimate this probability. How close are you to your answer from (3)?\n\n\npop_p &lt;- .28\nsample_size &lt;- 64\nn_simulations &lt;- 1000\n\nsample_means &lt;- numeric(n_simulations)\n\nfor(i in 1:n_simulations){\n  x &lt;- rbinom(1, size = sample_size, prob = pop_p)\n  sample_means[i] &lt;- x/sample_size\n}\n\n\n\nPart 4: Summary and Reflection\n\nWhat is the difference between a population distribution, sample distribution, and distribution of a sample?\nWhy is the CLT important?\nGive one benefit of finding sampling distributions theoretically with probability\nGive one benefit of finding sampling distributions computationally through simulation\nDoes your group have any lingering questions for me?"
  },
  {
    "objectID": "notes/16-bootstrap-t/16-activity.html",
    "href": "notes/16-bootstrap-t/16-activity.html",
    "title": "16: Comparing confidence interval procedures",
    "section": "",
    "text": "The {nycflights23} R package contains a dataset called flights. This dataset contains all flights in and out of NYC-area airports in 2023. Since the dataset has all flights, we can treat it as a population. We’re interested in the average departure delay of flights departing from NYC area airports.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(nycflights23)\n\nflights &lt;- flights |&gt;\n  filter(origin %in% c(\"JFK\", \"EWR\", \"LGA\")) |&gt; # filter to departing flights from NYC\n  drop_na(dep_delay) # drop observations with missing dep_delay variables\n\nglimpse(flights)\n\nRows: 424,614\nColumns: 19\n$ year           &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 1, 18, 31, 33, 36, 503, 520, 524, 537, 547, 549, 551, 5…\n$ sched_dep_time &lt;int&gt; 2038, 2300, 2344, 2140, 2048, 500, 510, 530, 520, 545, …\n$ dep_delay      &lt;dbl&gt; 203, 78, 47, 173, 228, 3, 10, -6, 17, 2, -10, -9, -7, -…\n$ arr_time       &lt;int&gt; 328, 228, 500, 238, 223, 808, 948, 645, 926, 845, 905, …\n$ sched_arr_time &lt;int&gt; 3, 135, 426, 2352, 2252, 815, 949, 710, 818, 852, 901, …\n$ arr_delay      &lt;dbl&gt; 205, 53, 34, 166, 211, -7, -1, -25, 68, -7, 4, -13, -14…\n$ carrier        &lt;chr&gt; \"UA\", \"DL\", \"B6\", \"B6\", \"UA\", \"AA\", \"B6\", \"AA\", \"UA\", \"…\n$ flight         &lt;int&gt; 628, 393, 371, 1053, 219, 499, 996, 981, 206, 225, 800,…\n$ tailnum        &lt;chr&gt; \"N25201\", \"N830DN\", \"N807JB\", \"N265JB\", \"N17730\", \"N925…\n$ origin         &lt;chr&gt; \"EWR\", \"JFK\", \"JFK\", \"JFK\", \"EWR\", \"EWR\", \"JFK\", \"EWR\",…\n$ dest           &lt;chr&gt; \"SMF\", \"ATL\", \"BQN\", \"CHS\", \"DTW\", \"MIA\", \"BQN\", \"ORD\",…\n$ air_time       &lt;dbl&gt; 367, 108, 190, 108, 80, 154, 192, 119, 258, 157, 164, 1…\n$ distance       &lt;dbl&gt; 2500, 760, 1576, 636, 488, 1085, 1576, 719, 1400, 1065,…\n$ hour           &lt;dbl&gt; 20, 23, 23, 21, 20, 5, 5, 5, 5, 5, 5, 6, 5, 6, 6, 6, 6,…\n$ minute         &lt;dbl&gt; 38, 0, 44, 40, 48, 0, 10, 30, 20, 45, 59, 0, 59, 0, 0, …\n$ time_hour      &lt;dttm&gt; 2023-01-01 20:00:00, 2023-01-01 23:00:00, 2023-01-01 2…\n\n\n\n\n\n\n\n\nQ1: EDA\n\n\n\nMake a histogram of dep_delay. This represents our population. What do you notice about the distribution? What is the true \\(\\mu\\) in this case?\n\n\nLet’s explore the performance of our different confidence interval procedures. To do so, you’ll first draw a sample from the flights dataset. You’ll treat this as your data sample throughout the rest of the activity, and compare your confidence intervals to the true value that you found in Q1.\n\n\n\n\n\n\nQ2: Draw your sample\n\n\n\nUse the code below to draw a sample of size 40 from the flights dataset. Make sure to set a seed that you think is different from everyone else in the class! This is the “sample data” that you’ll work with for the rest of this activity.\n\n\n\nset.seed(1) # Replace \"1\" with a random seed\nsample_index &lt;- sample(1:nrow(flights), size = 40)\nflights_sample &lt;- flights[sample_index,]\ndep_delay &lt;- flights_sample$dep_delay\n\n\n\n\n\n\n\nQ3: Construct your confidence intervals\n\n\n\nUsing your sample dep_delay from above, compute three confidence intervals:\n\npercentile bootstrap\nCLT-based t\nbootstrap t-based\n\ntemplate code can be found in the handout for today\n\n\nwhen you’re done, enter your lower and upper bounds in this google sheet\n\n\n\n\n\n\nQ4: (stretch goal, if time) Run coverage simulation\n\n\n\nTry to set up a “simulation of simulations”. Repeat your confidence interval procedure 100 times, on 100 different samples from flights, storing the upper and lower bounds for each confidence interval procedure as you go."
  },
  {
    "objectID": "slides/14/slides14.html#last-time",
    "href": "slides/14/slides14.html#last-time",
    "title": "CLT-based Confidence Intervals",
    "section": "Last time",
    "text": "Last time\n\n\n\nDr. Kristen Gorman and the Palmer Station, Antarctica LTER, are studying the bill dimensions of a certain species of penguin\nThey want to estimate the average bill depth and bill length (in mm)\n\n\ndata(\"penguins\", package = \"palmerpenguins\")\ngentoo &lt;- filter(penguins, species == \"Gentoo\")\n\n\n\n\n\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package: https://allisonhorst.github.io/palmerpenguins/"
  },
  {
    "objectID": "slides/14/slides14.html#last-time-1",
    "href": "slides/14/slides14.html#last-time-1",
    "title": "CLT-based Confidence Intervals",
    "section": "Last Time",
    "text": "Last Time"
  },
  {
    "objectID": "slides/14/slides14.html#what-are-plausible-values-for-theta-given-hattheta",
    "href": "slides/14/slides14.html#what-are-plausible-values-for-theta-given-hattheta",
    "title": "CLT-based Confidence Intervals",
    "section": "What are plausible values for \\(\\theta\\) given \\(\\hat\\theta\\)?",
    "text": "What are plausible values for \\(\\theta\\) given \\(\\hat\\theta\\)?\nWe want to develop an interval estimate of a population parameter\n\nExact method: Find the sampling distribution in closed form (Chapter 4). Requires knowledge of the distribution of the data!\nBootstrap method: Use the sample to approximate the population and simulate a sampling distribution (Chapter 5).\nAsymptotic method: Use large-sample theory to approximate the sampling distribution (e.g., appeal to CLT; Chapter 7)"
  },
  {
    "objectID": "slides/14/slides14.html#strategy",
    "href": "slides/14/slides14.html#strategy",
    "title": "CLT-based Confidence Intervals",
    "section": "Strategy",
    "text": "Strategy\n\nWe have an estimator \\(\\hat\\theta\\) in hand\nUse \\(\\hat\\theta\\) to find a range of plausible values for \\(\\theta\\) where\n\n\\[P(\\hat\\theta_L \\le \\theta \\le \\hat\\theta_U) = 1-\\alpha\\]"
  },
  {
    "objectID": "slides/14/slides14.html#central-limit-theorem",
    "href": "slides/14/slides14.html#central-limit-theorem",
    "title": "CLT-based Confidence Intervals",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf \\(X_1, ...., X_n\\) are iid normal random variables mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\]"
  },
  {
    "objectID": "slides/14/slides14.html#rnorm",
    "href": "slides/14/slides14.html#rnorm",
    "title": "CLT-based Confidence Intervals",
    "section": "rnorm",
    "text": "rnorm\nDraw random samples from a normal distribution\n\nrnorm(100)\n\n  [1]  0.689997495  0.243009315 -0.807401540  0.374458542 -0.915732432\n  [6]  0.178867707 -1.027265141  0.242949546 -1.842819078 -0.979080488\n [11]  0.600574438 -1.585153699 -0.264674428 -2.354315837 -1.485571171\n [16]  0.387874140 -1.512696626 -0.449453148  0.440283232  0.107874436\n [21]  0.893008371  1.306476920  1.052635158  1.248282276  0.238040548\n [26] -0.726755145 -0.921146552 -1.090287875  1.525105205  0.738510117\n [31] -0.581905367 -1.174651589 -0.647301509  1.236758361  1.329885783\n [36] -0.250956569  0.077823736  0.532409529 -0.868581185 -1.344989890\n [41] -0.055143175 -0.558497326  0.163625827  0.116062082  0.283748319\n [46] -0.185443284 -0.905502739 -0.378970109  2.444398064  1.545879204\n [51]  1.661460098 -1.765290580 -0.640270478 -0.179410439 -0.323670270\n [56]  0.818200954  0.869805697  0.583986275 -0.282794918 -0.455207421\n [61]  0.217383610 -0.023017063 -0.317057367 -0.594464928 -0.180541495\n [66]  0.902926695 -0.346319348 -1.719542799  0.375358299 -0.745854018\n [71] -0.215431321  1.442550814  0.556950688 -0.572394649  0.801313966\n [76] -0.035591859 -0.308874351  1.206091825  0.323264481  0.430756886\n [81] -0.826246153 -3.013584878  0.713907243  0.643560029 -0.323800012\n [86] -0.553170310  1.803857176  0.389636234 -0.953947150  0.075435359\n [91]  0.503398676  0.257374899 -0.009081139  0.065887151 -0.075173157\n [96] -1.128362548  1.121108578  0.987522633  0.528639124 -0.092888292\n\n\n\n\nAll distribution functions in R that start with rXXX draw random samples (rexp, rgamma, rbinom, etc)"
  },
  {
    "objectID": "slides/14/slides14.html#dnorm",
    "href": "slides/14/slides14.html#dnorm",
    "title": "CLT-based Confidence Intervals",
    "section": "dnorm",
    "text": "dnorm\nReturn the pdf evaluated at x\n\ndnorm(1)\n\n[1] 0.2419707"
  },
  {
    "objectID": "slides/14/slides14.html#pnorm",
    "href": "slides/14/slides14.html#pnorm",
    "title": "CLT-based Confidence Intervals",
    "section": "pnorm",
    "text": "pnorm\nReturn the cdf evaluated at x\n\npnorm(1)\n\n[1] 0.8413447"
  },
  {
    "objectID": "slides/14/slides14.html#qnorm",
    "href": "slides/14/slides14.html#qnorm",
    "title": "CLT-based Confidence Intervals",
    "section": "qnorm",
    "text": "qnorm\nReturn the quantile where the cdf is equal to x\n\nqnorm(.8413447)\n\n[1] 0.9999998"
  },
  {
    "objectID": "slides/14/slides14.html#try-it",
    "href": "slides/14/slides14.html#try-it",
    "title": "CLT-based Confidence Intervals",
    "section": "Try it",
    "text": "Try it\nFind the value of q that is needed for the following \\((1-\\alpha)100\\%\\) normal-based CIs:\n\n90%\n95%\n97%"
  },
  {
    "objectID": "slides/14/slides14.html#your-turn",
    "href": "slides/14/slides14.html#your-turn",
    "title": "CLT-based Confidence Intervals",
    "section": "Your turn",
    "text": "Your turn\nFind a 90% confidence interval for the mean bill length of Gentoo penguins.\nAssume that \\(\\sigma = 3.08\\)\nSample statistics:\n\\(\\bar{X}\\) = 47.5\nn = 123"
  },
  {
    "objectID": "slides/14/slides14.html#interpreting-cis-intro-stat-redux",
    "href": "slides/14/slides14.html#interpreting-cis-intro-stat-redux",
    "title": "CLT-based Confidence Intervals",
    "section": "Interpreting CI’s: intro stat redux",
    "text": "Interpreting CI’s: intro stat redux\n\nWe are \\((1-\\alpha)100\\)% confident that the true parameter of interest is between L and U"
  },
  {
    "objectID": "slides/14/slides14.html#section",
    "href": "slides/14/slides14.html#section",
    "title": "CLT-based Confidence Intervals",
    "section": "",
    "text": "(L, U) is a random interval before data are observed\nThe process by which the interval constructed is a random process\n\\((1-\\alpha)100\\)% is the long-run proportion of intervals that will capture the parameter\nIn practice, we don’t know which “type” of interval we have (good/bad)"
  },
  {
    "objectID": "slides/14/slides14.html#plug-in-principle",
    "href": "slides/14/slides14.html#plug-in-principle",
    "title": "CLT-based Confidence Intervals",
    "section": "Plug-in principle",
    "text": "Plug-in principle\nLet \\(X_1, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\).\n\n\n\\(\\dfrac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\)\nPROBLEM: \\(\\bar{x} \\pm z_{1-\\alpha/2} \\left( \\dfrac{\\sigma}{\\sqrt{n}} \\right)\\)\nIn practice both \\(\\mu\\) and \\(\\sigma^2\\) are unknown\nProposed solution: plug in the sample standard deviation \\(s\\)"
  },
  {
    "objectID": "slides/14/slides14.html#estimating-sigma-impacts-the-distribution",
    "href": "slides/14/slides14.html#estimating-sigma-impacts-the-distribution",
    "title": "CLT-based Confidence Intervals",
    "section": "Estimating \\(\\sigma\\) impacts the distribution",
    "text": "Estimating \\(\\sigma\\) impacts the distribution"
  },
  {
    "objectID": "slides/14/slides14.html#estimating-sigma-impacts-the-distribution-1",
    "href": "slides/14/slides14.html#estimating-sigma-impacts-the-distribution-1",
    "title": "CLT-based Confidence Intervals",
    "section": "Estimating \\(\\sigma\\) impacts the distribution",
    "text": "Estimating \\(\\sigma\\) impacts the distribution"
  },
  {
    "objectID": "slides/14/slides14.html#students-t-distribution",
    "href": "slides/14/slides14.html#students-t-distribution",
    "title": "CLT-based Confidence Intervals",
    "section": "(Student’s) t distribution",
    "text": "(Student’s) t distribution\nLet \\(T = \\dfrac{Z}{\\sqrt{V/df}}\\) where \\(Z \\sim N(0, 1)\\), \\(V \\sim \\chi^2_{df}\\), and \\(Z \\perp V \\Longrightarrow T \\sim t_{df}\\)"
  },
  {
    "objectID": "slides/14/slides14.html#t-distribution-properties",
    "href": "slides/14/slides14.html#t-distribution-properties",
    "title": "CLT-based Confidence Intervals",
    "section": "t distribution properties",
    "text": "t distribution properties\n\n\n\nSymmetric around 0\nFor \\(df=1\\), mean doesn’t exist (Cauchy distribution)\nFor \\(df \\ge 2\\), \\(E(T) = E(Z) E \\left(1 / \\sqrt{V/n} \\right) = 0\\)\nHeavier tails than normal distribution\n\\(t_{df} \\to N(0, 1)\\) as \\(df \\to \\infty\\)"
  },
  {
    "objectID": "slides/14/slides14.html#your-turn-finding-t-quantiles-in",
    "href": "slides/14/slides14.html#your-turn-finding-t-quantiles-in",
    "title": "CLT-based Confidence Intervals",
    "section": "Your turn: Finding t quantiles in ",
    "text": "Your turn: Finding t quantiles in \nqt(p, df) will calculate the p quantile of \\(t_{\\rm df}\\)\nFind the value of q that is needed for the following \\((1-\\alpha)100\\%\\) normal-based CIs:\n\n90%, n = 123\n95%, n = 25\n99%, n = 34\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/14/slides14.html#your-turn-1",
    "href": "slides/14/slides14.html#your-turn-1",
    "title": "CLT-based Confidence Intervals",
    "section": "Your turn",
    "text": "Your turn\nFind a 90% confidence interval for the mean bill length of Gentoo penguins.\nAssume that σ = 3.08.\nSample statistics:\n\n\\(n=123\\)\n\\(\\bar{x} = 47.5\\)\n\\(s = 3.08\\)"
  },
  {
    "objectID": "slides/14/slides14.html#underlying-validity-conditions",
    "href": "slides/14/slides14.html#underlying-validity-conditions",
    "title": "CLT-based Confidence Intervals",
    "section": "Underlying validity conditions",
    "text": "Underlying validity conditions\nWe have a random sample from a normal population distribution\n\n\nAsk Yourself…\n\nAre the observations independent?\nAre the observations approximately normal?"
  },
  {
    "objectID": "slides/14/slides14.html#checking-conditions",
    "href": "slides/14/slides14.html#checking-conditions",
    "title": "CLT-based Confidence Intervals",
    "section": "Checking conditions",
    "text": "Checking conditions\n\n\n\nAre the penguins independent?\nAre the bill lengths approximately normal?"
  },
  {
    "objectID": "slides/14/slides14.html#robustness",
    "href": "slides/14/slides14.html#robustness",
    "title": "CLT-based Confidence Intervals",
    "section": "Robustness",
    "text": "Robustness\nIf the a procedure “perform well” even if some of the assumptions under which they were developed do not hold, then they are called robust."
  },
  {
    "objectID": "slides/14/slides14.html#simulation-study",
    "href": "slides/14/slides14.html#simulation-study",
    "title": "CLT-based Confidence Intervals",
    "section": "Simulation study",
    "text": "Simulation study\nTo check whether a procedure is robust, we can use simulation:\n\nSimulate data from a variety of different probability distributions\nRun the procedure (e.g., build a one-sample t-interval)\nCompare the results of the procedure to what should have happened.\nfor a large number of CIs, approximately 95% of 95% CIs should capture the parameter value"
  },
  {
    "objectID": "slides/14/slides14.html#robustness-one-sample-t",
    "href": "slides/14/slides14.html#robustness-one-sample-t",
    "title": "CLT-based Confidence Intervals",
    "section": "Robustness: one-sample \\(t\\)",
    "text": "Robustness: one-sample \\(t\\)\n\n\nIf the population distribution is roughly symmetric and unimodal, then the procedure works well for sample sizes of at least 10–15 (just a rough guide)\nFor skewed population distributions, the t-procedure can be substantially affected, depending on the severity of the skew and the sample size.\nt-procedures are not resistant to outliers.\nIf observations are not independent, everything breaks"
  },
  {
    "objectID": "slides/00-test/00-test.html#r-code",
    "href": "slides/00-test/00-test.html#r-code",
    "title": "Test slides",
    "section": "R Code",
    "text": "R Code\n\npalmerpenguins::penguins %&gt;%\n  ggplot(aes(x = flipper_length_mm)) +\n  geom_histogram(col = \"white\")"
  },
  {
    "objectID": "slides/00-test/00-test.html#plan-for-today",
    "href": "slides/00-test/00-test.html#plan-for-today",
    "title": "Test slides",
    "section": "Plan for today",
    "text": "Plan for today\n\nSyllabus\nContext for the class\nCourse expectations"
  },
  {
    "objectID": "slides/00-test/00-test.html#a-bit-about-me",
    "href": "slides/00-test/00-test.html#a-bit-about-me",
    "title": "Test slides",
    "section": "A bit about me",
    "text": "A bit about me"
  },
  {
    "objectID": "slides/00-test/00-test.html#course-description",
    "href": "slides/00-test/00-test.html#course-description",
    "title": "Test slides",
    "section": "Course description",
    "text": "Course description\nThis course is an introduction to the mathematical theory of frequentist and Bayesian statistical inference. Topics include parameter estimation, confidence intervals and hypothesis testing, linear models, and Bayesian inference.\nStudents who analyze data, or who aspire to develop new methods for analyzing data, should be well-grounded in mathematical statistics."
  },
  {
    "objectID": "slides/00-test/00-test.html#course-objectives",
    "href": "slides/00-test/00-test.html#course-objectives",
    "title": "Test slides",
    "section": "Course Objectives",
    "text": "Course Objectives\nBy the end of this course, you should be able to:\n\nDerive estimators for parameters using maximum likelihood, the method of moments, and Bayesian techniques\nEvaluate the performance of estimators and describe their strengths and weaknesses\nDemonstrate a sophisticated understanding of the mathematics behind hypothesis tests, confidence intervals, and linear models\n\nUse the statistical package R to implement basic simulations of estimation scenarios"
  },
  {
    "objectID": "slides/00-test/00-test.html#textbook",
    "href": "slides/00-test/00-test.html#textbook",
    "title": "Test slides",
    "section": "Textbook:",
    "text": "Textbook:"
  },
  {
    "objectID": "slides/00-test/00-test.html#computing",
    "href": "slides/00-test/00-test.html#computing",
    "title": "Test slides",
    "section": "Computing:",
    "text": "Computing:"
  },
  {
    "objectID": "slides/00-test/00-test.html#assignments",
    "href": "slides/00-test/00-test.html#assignments",
    "title": "Test slides",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "slides/00-test/00-test.html#regrade-request-policy",
    "href": "slides/00-test/00-test.html#regrade-request-policy",
    "title": "Test slides",
    "section": "Regrade request policy",
    "text": "Regrade request policy\nGrading is often a tedious task, and the grading team will sometimes make mistakes. I am always happy to fix these mistakes, and gradescope makes it easy to do so. However, it takes time to read through these requests and I’ve noticed an increase in unwarranted regrade requests in recent years. This semester, I am instituting an “NFL Coaches Challenge”-style regrade request rule. Every student will start the semester with 2 regrade requests available to them. If you submit a regrade request and I agree with you, you get it back. If you submit a regrade request for something that was not a grading mistake, you lose that request."
  },
  {
    "objectID": "slides/00-test/00-test.html#exams",
    "href": "slides/00-test/00-test.html#exams",
    "title": "Test slides",
    "section": "Exams",
    "text": "Exams"
  },
  {
    "objectID": "slides/00-test/00-test.html#final-exam",
    "href": "slides/00-test/00-test.html#final-exam",
    "title": "Test slides",
    "section": "Final Exam",
    "text": "Final Exam"
  },
  {
    "objectID": "slides/00-test/00-test.html#course-project",
    "href": "slides/00-test/00-test.html#course-project",
    "title": "Test slides",
    "section": "Course project",
    "text": "Course project"
  },
  {
    "objectID": "slides/00-test/00-test.html#final-grades",
    "href": "slides/00-test/00-test.html#final-grades",
    "title": "Test slides",
    "section": "Final grades",
    "text": "Final grades"
  },
  {
    "objectID": "slides/00-test/00-test.html#advice-from-past-students",
    "href": "slides/00-test/00-test.html#advice-from-past-students",
    "title": "Test slides",
    "section": "Advice from past students:",
    "text": "Advice from past students:\n\nStart the problem sets early and go to office hours!\nWork with other people in the class, collaboration is key\nBe willing to ask lots of questions and don’t be afraid to ask for help!\nI would advise for them to attend office hours and the stat clinic since early on to get help and not be worrying about assignments at the last minute.\nI would advise future students to reach out and ask questions as soon if they have any confusion. Understanding statistical concepts and working with R can be frustrating at times, but people are here to help you along the way!\nDon’t let work snowball! Try to get help early and often and an imperfect problem set is better than no problem set.\nlots of R here! Organization is key, and you’ll do well as long as you start homeworks early, communicate with the professor and with your fellow classmates. There’s a good support system."
  },
  {
    "objectID": "slides/00-test/00-test.html#the-genius-myth",
    "href": "slides/00-test/00-test.html#the-genius-myth",
    "title": "Test slides",
    "section": "The “Genius Myth”",
    "text": "The “Genius Myth”\nIt’s sometimes easy to buy into the “genius myth” when it comes to math/stat courses: that you need to be a “math person” and have some innate mathematical ability in order to do well or become a statistics major. This could not be further from the truth! The best statisticians don’t necessarily have the “best” math or programming background, but are people that are able to formulate interesting questions and use math and programming to rigorously answer those questions. Many of the best statisticians I know became statisticians because they were initially interested in something else (biology, public health, psychology, neuroscience, physics, etc.) and realized that being able to answer important questions with data was not only valuable but fun and interesting. Being able to perform interesting statistical analyses is a skill that is learned, not an innate ability, and working hard at developing that skill is the point of this course."
  },
  {
    "objectID": "slides/00-test/00-test.html#academic-integrity",
    "href": "slides/00-test/00-test.html#academic-integrity",
    "title": "Test slides",
    "section": "Academic Integrity",
    "text": "Academic Integrity"
  },
  {
    "objectID": "slides/00-test/00-test.html#diversity-inclusion",
    "href": "slides/00-test/00-test.html#diversity-inclusion",
    "title": "Test slides",
    "section": "Diversity & Inclusion",
    "text": "Diversity & Inclusion\nWe all come to class with different backgrounds and experiences, and this diversity makes our class environment richer. We value diversity and inclusion, and are committed to a climate of mutual respect and full participation in and out of the classroom. This class strives to be a learning environment that is usable, equitable, inclusive and welcoming, regardless of race, ethnicity, religion, gender and gender identities, sexual orientation, ability, socioeconomic background, and nationality. If you anticipate or experience any barriers to learning, please discuss your concerns with me."
  },
  {
    "objectID": "slides/00-test/00-test.html#accomodations",
    "href": "slides/00-test/00-test.html#accomodations",
    "title": "Test slides",
    "section": "Accomodations",
    "text": "Accomodations"
  },
  {
    "objectID": "slides/00-test/00-test.html#stat-lab",
    "href": "slides/00-test/00-test.html#stat-lab",
    "title": "Test slides",
    "section": "Stat Lab",
    "text": "Stat Lab"
  },
  {
    "objectID": "slides/00-test/00-test.html#title-ix",
    "href": "slides/00-test/00-test.html#title-ix",
    "title": "Test slides",
    "section": "Title IX",
    "text": "Title IX\nPlease be aware that all faculty are “responsible employees”, which means that if you tell me about a situation involving sexual harassment, sexual assault, dating violence, domestic violence, or stalking, I must share that information with the Title IX Coordinator. Although I have to make this notification, you will control how your case will be handled, including whether or not you wish to meet with the Title IX coordinator or pursue a formal complaint."
  },
  {
    "objectID": "slides/00-test/00-test.html#take-care-of-yourself",
    "href": "slides/00-test/00-test.html#take-care-of-yourself",
    "title": "Test slides",
    "section": "Take care of yourself",
    "text": "Take care of yourself\nDo your best to maintain a healthy lifestyle this semester by wearing a mask if you don’t feel well, eating a vegetable every day, exercising, avoiding excessive drug and alcohol use, getting enough sleep, and taking some time to relax. Your mental health is more important than your grade in this course. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. If you are experiencing mental health symptoms as a result of coursework, please speak with me so we can address the problem together."
  },
  {
    "objectID": "slides/00-test/00-test.html#prerequisites",
    "href": "slides/00-test/00-test.html#prerequisites",
    "title": "Test slides",
    "section": "Prerequisites",
    "text": "Prerequisites"
  },
  {
    "objectID": "slides/00-test/00-test.html#history",
    "href": "slides/00-test/00-test.html#history",
    "title": "Test slides",
    "section": "History",
    "text": "History\n\nStatistics seems to be a difficult subject for mathematicians, perhaps because its elusive and wide-ranging character mitigates against the traditional theorem-proof method of presentation. It may come as some comfort then that statistics is also a difficult subject for statisticians\n\n\nControversies in the Foundations of Statistics, Bradley Efron 1978"
  },
  {
    "objectID": "slides/00-test/00-test.html#fisher-vs-neyman-pearson",
    "href": "slides/00-test/00-test.html#fisher-vs-neyman-pearson",
    "title": "Test slides",
    "section": "Fisher vs Neyman-Pearson",
    "text": "Fisher vs Neyman-Pearson"
  },
  {
    "objectID": "slides/01/slides01.html#plan-for-today",
    "href": "slides/01/slides01.html#plan-for-today",
    "title": "Welcome to Stat250!",
    "section": "Plan for today",
    "text": "Plan for today\n\nIntros\nContext for the class\nSyllabus\nSetting course expectations"
  },
  {
    "objectID": "slides/01/slides01.html#about-me",
    "href": "slides/01/slides01.html#about-me",
    "title": "Welcome to Stat250!",
    "section": "About me",
    "text": "About me\n\n\n\nFirst year at Carleton!\nTaught at Swarthmore for 5 years before moving here this fall\nPhD in Statistics & Data Science from Carnegie Mellon University\nGrew up in Minnesota, went to St Ben’s as an undergrad"
  },
  {
    "objectID": "slides/01/slides01.html#three-prongs-of-statistics",
    "href": "slides/01/slides01.html#three-prongs-of-statistics",
    "title": "Welcome to Stat250!",
    "section": "Three prongs of statistics",
    "text": "Three prongs of statistics\n\n\nDesign  The design of surveys/experiments and collection of data to more efficiently/correctly address scientific questions \n\n\n\nExploration  Understand the major features of and detect patterns in data \n\n\n\n\nInference  Account for randomness, variability, and bias in a sample in order to draw reasonable and correct conclusions about a population"
  },
  {
    "objectID": "slides/01/slides01.html#statistics-vs.-probability",
    "href": "slides/01/slides01.html#statistics-vs.-probability",
    "title": "Welcome to Stat250!",
    "section": "Statistics vs. probability",
    "text": "Statistics vs. probability\nProbability (Math 240)\nWe learned how to calculate the probability of seeing a result (data) given a specific probability model (e.g., a specific distribution)\n\n\nStatistics (Stat 250)\nWe will learn how to make statements about the underlying probability models given the data we see"
  },
  {
    "objectID": "slides/01/slides01.html#example-spies-vs.-statisticians",
    "href": "slides/01/slides01.html#example-spies-vs.-statisticians",
    "title": "Welcome to Stat250!",
    "section": "Example: Spies vs. Statisticians",
    "text": "Example: Spies vs. Statisticians\nDuring WWII, the Allies wanted to determine production rates of tanks (and airplanes, missiles, etc.)\nSpies\nGathered intelligence (intercepted messages, interrogated of prisoners, etc.) and made the following estimates:\n\nJune 1940: 1000\nJune 1941: 1550\nAugust 1942: 1550"
  },
  {
    "objectID": "slides/01/slides01.html#example-spies-vs.-statisticians-1",
    "href": "slides/01/slides01.html#example-spies-vs.-statisticians-1",
    "title": "Welcome to Stat250!",
    "section": "Example: Spies vs. Statisticians",
    "text": "Example: Spies vs. Statisticians\nStatisticians\n\n\nThe Allies had a sample of serial numbers (via capture, photography, etc.), \\(X_1, X_2, \\ldots, X_n\\), and there were \\(N\\) produced.\nAllied statisticians needed to devise an estimator to obtain \\(N\\)\nUltimately, they used \\(\\widehat{N} = X_{\\text{max}} + \\dfrac{X_{\\text{max}}}{n} - 1\\)\n\nJune 1940: 169\nJune 1941: 244\nAugust 1942: 327"
  },
  {
    "objectID": "slides/01/slides01.html#example-spies-vs.-statisticians-2",
    "href": "slides/01/slides01.html#example-spies-vs.-statisticians-2",
    "title": "Welcome to Stat250!",
    "section": "Example: Spies vs. Statisticians",
    "text": "Example: Spies vs. Statisticians\nAfter the war, the Allies discovered documents revealing the true number of tanks produced:\n\n\n\n\nMonth\nTruth\nStatisticians\nSpies\n\n\n\n\nJune 1940\n122\n169\n1000\n\n\nJune 1941\n271\n244\n1550\n\n\nAugust 1942\n342\n327\n1550"
  },
  {
    "objectID": "slides/01/slides01.html#prereqs",
    "href": "slides/01/slides01.html#prereqs",
    "title": "Welcome to Stat250!",
    "section": "Prereqs",
    "text": "Prereqs\n\nMath240 (Probability)\nStat120 or other applied intro stats course is helpful but not required\nI’ll assume some experience with R\nWe’re going to use mechanics from calculus & probability, but it’s not the focus of the course\nPart of HW1 is Calc/Prob review. refresh your memory, use resources that you need, talk to me sooner rather than later if it’s especially tough"
  },
  {
    "objectID": "slides/01/slides01.html#section",
    "href": "slides/01/slides01.html#section",
    "title": "Welcome to Stat250!",
    "section": "",
    "text": "Statistics seems to be a difficult subject for mathematicians, perhaps because its elusive and wide-ranging character mitigates against the traditional theorem-proof method of presentation. It may come as some comfort then that statistics is also a difficult subject for statisticians\n\n\nControversies in the Foundations of Statistics, Bradley Efron 1978"
  },
  {
    "objectID": "slides/01/slides01.html#statistical-models",
    "href": "slides/01/slides01.html#statistical-models",
    "title": "Welcome to Stat250!",
    "section": "Statistical models",
    "text": "Statistical models\nA statistical model consists of\n\na collection of random variables to describe observable data,\nthe possible joint distribution(s) of the random variables,\nand the parameters, \\(\\boldsymbol \\theta\\), that define those distributions\n\n\n\nMorris and DeGroot, 377"
  },
  {
    "objectID": "slides/01/slides01.html#r.a.-fisher",
    "href": "slides/01/slides01.html#r.a.-fisher",
    "title": "Welcome to Stat250!",
    "section": "R.A. Fisher",
    "text": "R.A. Fisher\n1890-1962\n\n\n\n\n\nVariance\nANOVA\nNull hypothesis\nMaximum likelihood estimation\np-value\nLots of contributions in genetics\nAlso a eugenecist"
  },
  {
    "objectID": "slides/01/slides01.html#neyman-pearson",
    "href": "slides/01/slides01.html#neyman-pearson",
    "title": "Welcome to Stat250!",
    "section": "Neyman & Pearson",
    "text": "Neyman & Pearson\n\n\n\n\n\nConfidence interval\nCorrelation\nRegression\nStandard deviation\nEffect size\n“Optimal” tests\n\\(\\alpha\\) and \\(\\beta\\)\nType I and II error"
  },
  {
    "objectID": "slides/01/slides01.html#frequentist-vs-bayesian",
    "href": "slides/01/slides01.html#frequentist-vs-bayesian",
    "title": "Welcome to Stat250!",
    "section": "Frequentist vs Bayesian",
    "text": "Frequentist vs Bayesian"
  },
  {
    "objectID": "slides/01/slides01.html#frequentist-vs-bayesian-1",
    "href": "slides/01/slides01.html#frequentist-vs-bayesian-1",
    "title": "Welcome to Stat250!",
    "section": "Frequentist vs Bayesian",
    "text": "Frequentist vs Bayesian\n\n\nFrequentist\n\n“Classical” statistics\nProbability is a long-run frequency\nType I and Type II errors\nConfidence intervals\n\n\nBayesian\n\nProbability is a subjective belief\nUpdate “prior” probabilities with data to obtain “posterior” probabilities\n“Credible” intervals\nComputationally intensive"
  },
  {
    "objectID": "slides/01/slides01.html#parametric-vs-nonparametric",
    "href": "slides/01/slides01.html#parametric-vs-nonparametric",
    "title": "Welcome to Stat250!",
    "section": "Parametric vs Nonparametric",
    "text": "Parametric vs Nonparametric\n\nNonparametric \n\nThe basic idea of nonparametric inference is to use data to infer an unknown quantity while making as few assumptions as possible. Usually, this means using statistical models that are infinite-dimensional. (Wasserman, 2006)\n\n\nParametric\n\nA parametric inference uses models that consist of a set of distributions/densities that can be parameterized by a finite number of parameters."
  },
  {
    "objectID": "slides/01/slides01.html#tentative-schedule",
    "href": "slides/01/slides01.html#tentative-schedule",
    "title": "Welcome to Stat250!",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\n\nTopic\nChapters\nApprox. Duration\n\n\n\n\nReview\n1-4\n1 week\n\n\nParametric estimation\n6\n3 weeks\n\n\nParametric & Nonparametric inference\n3-5, 7-9\n4 weeks\n\n\nModeling\n8-10\n2 weeks"
  },
  {
    "objectID": "slides/01/slides01.html#course-description",
    "href": "slides/01/slides01.html#course-description",
    "title": "Welcome to Stat250!",
    "section": "Course description",
    "text": "Course description\nThis course is an introduction to the mathematical theory of frequentist and Bayesian statistical inference. Topics include parameter estimation, confidence intervals and hypothesis testing, linear models, and Bayesian inference.\nStudents who analyze data, or who aspire to develop new methods for analyzing data, should be well-grounded in mathematical statistics."
  },
  {
    "objectID": "slides/01/slides01.html#course-objectives",
    "href": "slides/01/slides01.html#course-objectives",
    "title": "Welcome to Stat250!",
    "section": "Course Objectives",
    "text": "Course Objectives\nBy the end of this course, you should be able to:\n\nDerive estimators for parameters using maximum likelihood, the method of moments, and Bayesian techniques\nEvaluate the performance of estimators and describe their strengths and weaknesses\nDemonstrate a sophisticated understanding of the mathematics behind hypothesis tests, confidence intervals, and linear models\n\nUse the statistical package R to implement basic simulations of estimation scenarios"
  },
  {
    "objectID": "slides/01/slides01.html#textbook",
    "href": "slides/01/slides01.html#textbook",
    "title": "Welcome to Stat250!",
    "section": "Textbook:",
    "text": "Textbook:\nMathematical Statistics with Resampling and R (3rd edition) by Chihara and Hesterberg\n\nSometimes, it can help to see a second way of topics being explained. I recommend Mathematical Statistics with applications by Larsen & Marx."
  },
  {
    "objectID": "slides/01/slides01.html#computing",
    "href": "slides/01/slides01.html#computing",
    "title": "Welcome to Stat250!",
    "section": "Computing:",
    "text": "Computing:\nWe’ll be using R and RStudio throughout the course. If you’ve downloaded R to your own computer from a different class, great! If not, you can access anything you need through the maize server:\nhttps://maize.mathcs.carleton.edu/"
  },
  {
    "objectID": "slides/01/slides01.html#what-will-you-do-in-this-course",
    "href": "slides/01/slides01.html#what-will-you-do-in-this-course",
    "title": "Welcome to Stat250!",
    "section": "What will you do in this course?",
    "text": "What will you do in this course?\nEach of the following components are important for your learning and therefore part of your final grade calculation:\n\nDaily Prep (5%)\nHomework (15%)\n\nDue once per week, typically Wednesdays but sometimes Fridays\n\nGroup work and attendance (5%)\n\nmore than 5 absences \\(\\to\\) 0\n\nMidterm exams (2 x 17.5%)\nCourse Project (10%)\nFinal Exam (30%)"
  },
  {
    "objectID": "slides/01/slides01.html#what-will-a-typical-dayweek-look-like",
    "href": "slides/01/slides01.html#what-will-a-typical-dayweek-look-like",
    "title": "Welcome to Stat250!",
    "section": "What will a typical day/week look like?",
    "text": "What will a typical day/week look like?\n\n\nBefore class:\n\nRead a chapter\nCome with questions\nBe prepared to try what was covered\n\n\nIn class:\n\nMini lecture\n\nSometimes review\nSometimes new\n\nHands-on work or coding in R\n\n\nAfter class:\n\nFinish in-class exercises\nWork on homework"
  },
  {
    "objectID": "slides/01/slides01.html#office-hours-tentative",
    "href": "slides/01/slides01.html#office-hours-tentative",
    "title": "Welcome to Stat250!",
    "section": "Office hours (tentative)",
    "text": "Office hours (tentative)\n\n\n\nDay\nTime\nType\nLocation\n\n\n\n\nMonday\n4:15-5:15\nDrop-in\nCMC 307\n\n\nTuesday\n10:30-11:30\nDrop-in\nCMC 307\n\n\nWednesday\n2:15-3:15\nDrop-in\nCMC 307\n\n\nFriday\n11-12\nDrop-in\nCMC 307"
  },
  {
    "objectID": "slides/01/slides01.html#where-is-amanda-this-term",
    "href": "slides/01/slides01.html#where-is-amanda-this-term",
    "title": "Welcome to Stat250!",
    "section": "Where is Amanda this term?",
    "text": "Where is Amanda this term?"
  },
  {
    "objectID": "slides/01/slides01.html#communication",
    "href": "slides/01/slides01.html#communication",
    "title": "Welcome to Stat250!",
    "section": "Communication",
    "text": "Communication\n\n\nMoodle: assignments, note sets, and grades\nSlack: homework questions, announcements, discussion\nEmail: personal matters, time-sensitive annoucements\n\n\n\nSlack is the fastest way to reach me. I typically will respond to messages 3x per weekday. I try to respond to emails within 48 hours. I’m online sporadically on evenings and weekends to devote time to family and rest – I hope you also use this time to reset and recharge!"
  },
  {
    "objectID": "slides/01/slides01.html#advice-from-past-students",
    "href": "slides/01/slides01.html#advice-from-past-students",
    "title": "Welcome to Stat250!",
    "section": "Advice from past students:",
    "text": "Advice from past students:\n\nStart the problem sets early and go to office hours!\nWork with other people in the class, collaboration is key\nBe willing to ask lots of questions and don’t be afraid to ask for help!\nI would advise for them to attend office hours and the stat clinic since early on to get help and not be worrying about assignments at the last minute.\nI would advise future students to reach out and ask questions as soon if they have any confusion. Understanding statistical concepts and working with R can be frustrating at times, but people are here to help you along the way!\nDon’t let work snowball! Try to get help early and often and an imperfect problem set is better than no problem set.\nStart the homework early! Give yourself time to get things done, to understand, and to pause. Don’t feel afraid to ask questions; she’s so accessible.\nSpend some time reviewing handouts before doing homework."
  },
  {
    "objectID": "slides/01/slides01.html#the-genius-myth",
    "href": "slides/01/slides01.html#the-genius-myth",
    "title": "Welcome to Stat250!",
    "section": "The “Genius Myth”",
    "text": "The “Genius Myth”\nIt’s sometimes easy to buy into the “genius myth” when it comes to math/stat courses: that you need to be a “math person” and have some innate mathematical ability in order to do well or become a statistics major. This could not be further from the truth! The best statisticians don’t necessarily have the “best” math or programming background, but are people that are able to formulate interesting questions and use math and programming to rigorously answer those questions. Many of the best statisticians I know became statisticians because they were initially interested in something else (biology, public health, psychology, neuroscience, physics, etc.) and realized that being able to answer important questions with data was not only valuable but fun and interesting. Being able to perform interesting statistical analyses is a skill that is learned, not an innate ability, and working hard at developing that skill is the point of this course."
  },
  {
    "objectID": "slides/01/slides01.html#academic-integrity",
    "href": "slides/01/slides01.html#academic-integrity",
    "title": "Welcome to Stat250!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nYou are expected to follow Carleton’s policies regarding academic integrity. I encourage you to discuss the homework problems with others and use the resources available to you to try to figure out tough problems. You should code and write up your solutions on your own. Exams must be done by yourself without communicating with others; all work must be your own. The use of textbook solution manuals (physical or online), course materials from other students, or materials from previous versions of this course are not allowed. Copying, paraphrasing, summarizing, or submitting work generated by anyone but yourself without proper attribution is considered academic dishonesty (this includes output from LLMs).\nPlease ask if you are unsure of whether or not your actions are complying with the assignment/exam/project instructions. Always default to acknowledging any help received. Cases of suspected academic dishonesty are handled by the Provost’s Office and I am obligated to report any suspected violations of this policy."
  },
  {
    "objectID": "slides/01/slides01.html#more-on-ai",
    "href": "slides/01/slides01.html#more-on-ai",
    "title": "Welcome to Stat250!",
    "section": "More on “AI”",
    "text": "More on “AI”\nLarge-language models (e.g. ChatGPT, Gemini, etc.) should only be used for help interpreting R’s error messages. You should not copy and paste course material into or out of an AI text generator.\nI also have a few rules in place to protect my intellectual property. You may not record my lectures using tools such as Otter.ai or upload any video or audio recordings to generate transcripts or study notes. You may not upload my course materials (slides, assignment prompts, note sets, etc.) into AI tools or homework help sites (such as chegg).\n“AI” tools are new for all of us and it’s OK to have questions about what is and isn’t appropriate!"
  },
  {
    "objectID": "slides/01/slides01.html#diversity-inclusion",
    "href": "slides/01/slides01.html#diversity-inclusion",
    "title": "Welcome to Stat250!",
    "section": "Diversity & Inclusion",
    "text": "Diversity & Inclusion\nWe all come to class with different backgrounds and experiences, and this diversity makes our class environment richer. We value diversity and inclusion, and are committed to a climate of mutual respect and full participation in and out of the classroom. This class strives to be a learning environment that is usable, equitable, inclusive and welcoming, regardless of race, ethnicity, religion, gender and gender identities, sexual orientation, ability, socioeconomic background, and nationality. If you anticipate or experience any barriers to learning, please discuss your concerns with me."
  },
  {
    "objectID": "slides/01/slides01.html#accomodations",
    "href": "slides/01/slides01.html#accomodations",
    "title": "Welcome to Stat250!",
    "section": "Accomodations",
    "text": "Accomodations\nCarleton College is committed to providing equitable access to learning opportunities for all students. The Office of Accessibility Resources (Henry House, 107 Union Street) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations. If you have, or think you may have, a disability, please contact OAR@carleton.edu to arrange a confidential discussion regarding equitable access and reasonable accommodations. You are also welcome to contact me privately to discuss your academic needs. However, all disability-related accommodations must be arranged, in advance, through OAR."
  },
  {
    "objectID": "slides/01/slides01.html#stat-lab",
    "href": "slides/01/slides01.html#stat-lab",
    "title": "Welcome to Stat250!",
    "section": "Stat Lab",
    "text": "Stat Lab\nThe Stats Lab (CMC 304) offers drop-in help R/RStudio help sessions run by friendly and knowledgeable lab assistants on most weekday evenings and some weekend times. The Stat Lab is primarily meant to serve Stat120 students, but many of the lab assistants can also help with Stat250."
  },
  {
    "objectID": "slides/01/slides01.html#title-ix",
    "href": "slides/01/slides01.html#title-ix",
    "title": "Welcome to Stat250!",
    "section": "Title IX",
    "text": "Title IX\nPlease be aware that all faculty are “responsible employees”, which means that if you tell me about a situation involving sexual harassment, sexual assault, dating violence, domestic violence, or stalking, I must share that information with the Title IX Coordinator. Although I have to make this notification, you will control how your case will be handled, including whether or not you wish to meet with the Title IX coordinator or pursue a formal complaint."
  },
  {
    "objectID": "slides/01/slides01.html#take-care-of-yourself",
    "href": "slides/01/slides01.html#take-care-of-yourself",
    "title": "Welcome to Stat250!",
    "section": "Take care of yourself",
    "text": "Take care of yourself\nDo your best to maintain a healthy lifestyle this semester by wearing a mask if you don’t feel well, eating a vegetable every day, exercising, avoiding excessive drug and alcohol use, getting enough sleep, and taking some time to relax. Your mental health is more important than your grade in this course. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. If you are experiencing mental health symptoms as a result of coursework, please speak with me so we can address the problem together."
  },
  {
    "objectID": "slides/09/slides09.html#recap",
    "href": "slides/09/slides09.html#recap",
    "title": "Efficiency & CRLB",
    "section": "Recap",
    "text": "Recap\n\nObserve data \\(X_1, ..., X_n \\sim F_x(x|\\theta)\\), where \\(\\theta\\) is unknown\nGoal: Estimate \\(\\theta\\) based on the values of \\(X_i\\) by formulating an estimator \\(\\widehat \\theta\\)\nOne technique is to use the maximum likelihood estimator\nA second technique is to use the method of moments estimator\nWe can compare estimators by comparing their bias, variance, and mean squared error.\nToday: is there a way to know if we’ve found an “optimal” estimator?"
  },
  {
    "objectID": "slides/09/slides09.html#last-time-unif0-theta-distribution",
    "href": "slides/09/slides09.html#last-time-unif0-theta-distribution",
    "title": "Efficiency & CRLB",
    "section": "Last time: Unif(0, \\(\\theta\\)) distribution1",
    "text": "Last time: Unif(0, \\(\\theta\\)) distribution1\nYour task is to compare the estimators\n\\[\\widehat{\\theta}_{MLE} = X_\\max \\hspace{1in} \\widehat{\\theta}_{MoM} = 2\\bar{X}\\]\n\n\nWhat is the bias of each estimator?2\nWhat is the SE of each estimator?\nWhat is the MSE of each estimator?\nWhen does \\(\\widehat{\\theta}_{MLE}\\) “beat” \\(\\widehat{\\theta}_{MoM}\\) in terms of MSE?\n\n\nSo \\(0 \\le x \\le \\theta\\), \\(f_x = \\frac{1}{\\theta}\\), \\(F_x = \\frac{x}{\\theta}\\), \\(E(X) = \\frac{\\theta}{2}\\), and \\(V(X) = \\frac{\\theta^2}{12}\\)A helpful fact is that \\(f_{X_{max}}(x) = n[F(x)]^{n-1}f_X(x)\\)"
  },
  {
    "objectID": "slides/09/slides09.html#section",
    "href": "slides/09/slides09.html#section",
    "title": "Efficiency & CRLB",
    "section": "",
    "text": "(d) When does \\(\\widehat{\\theta}_{MLE}\\) “beat” \\(\\widehat{\\theta}_{MoM}\\) in terms of MSE?\nMSE = “MSE factor” \\(\\times \\theta^2\\)"
  },
  {
    "objectID": "slides/09/slides09.html#example-uniform0theta",
    "href": "slides/09/slides09.html#example-uniform0theta",
    "title": "Efficiency & CRLB",
    "section": "Example: Uniform(\\(0,\\theta\\))",
    "text": "Example: Uniform(\\(0,\\theta\\))\nFrom last time: \\(E(\\widehat \\theta_{MLE}) = \\frac{n}{n+1} \\theta\\) and \\(V(\\widehat \\theta_{MLE}) = [\\frac{n}{(n+2)(n+1)^2}]\\)\n\n\nSince \\(\\hat \\theta_{MLE}\\) beats \\(\\hat \\theta_{MoM}\\) in terms of MSE but is biased, can we “unbias” the MLE? Call this third estimator \\(\\hat\\theta_3\\)\nDoes \\(\\hat \\theta_3\\) ever “beat” \\(\\hat \\theta_{MLE}\\) in terms of MSE?"
  },
  {
    "objectID": "slides/09/slides09.html#section-1",
    "href": "slides/09/slides09.html#section-1",
    "title": "Efficiency & CRLB",
    "section": "",
    "text": "(f) Does \\(\\hat \\theta_3\\) ever “beat” \\(\\hat \\theta_{MLE}\\) in terms of MSE?\nMSE = “MSE factor” \\(\\times \\theta^2\\)"
  },
  {
    "objectID": "slides/09/slides09.html#moral-of-the-story",
    "href": "slides/09/slides09.html#moral-of-the-story",
    "title": "Efficiency & CRLB",
    "section": "Moral of the story",
    "text": "Moral of the story\nWe can “fix” bias, but it’s harder to fix variance"
  },
  {
    "objectID": "slides/09/slides09.html#comparing-unbiased-estimators-efficiency",
    "href": "slides/09/slides09.html#comparing-unbiased-estimators-efficiency",
    "title": "Efficiency & CRLB",
    "section": "Comparing unbiased estimators: efficiency",
    "text": "Comparing unbiased estimators: efficiency\n\n\n\n\n\n\nEfficiency\n\n\nFor two unbiased estimators, \\(\\widehat \\theta_1\\) is more efficient than \\(\\widehat \\theta_2\\) if\n\\[\\text{Var}(\\widehat \\theta_1) &lt; \\text{Var}(\\widehat \\theta_2) \\iff \\text{SE}(\\widehat \\theta_1) &lt; \\text{SE}(\\widehat \\theta_2)\\]"
  },
  {
    "objectID": "slides/09/slides09.html#example-why-efficiency-matters",
    "href": "slides/09/slides09.html#example-why-efficiency-matters",
    "title": "Efficiency & CRLB",
    "section": "Example: why efficiency matters",
    "text": "Example: why efficiency matters\nWe now have two unbiased estimators: \\[\\widehat \\theta_{MoM}$\\widehat \\theta_{3} = \\frac{n+1}{n} X_\\max\\]\n\nIf \\(n=10\\) what is the expectation and variance of each of these estimators?"
  },
  {
    "objectID": "slides/09/slides09.html#example-why-efficiency-matters-1",
    "href": "slides/09/slides09.html#example-why-efficiency-matters-1",
    "title": "Efficiency & CRLB",
    "section": "Example: why efficiency matters",
    "text": "Example: why efficiency matters\n\nWhat \\(n\\) would we need for \\(\\widehat\\theta_{MOM}\\) to reach the variance of \\(\\widehat\\theta_3\\)?"
  },
  {
    "objectID": "slides/09/slides09.html#comparing-unbiased-estimators-cramer-rao-lower-bound-crlb",
    "href": "slides/09/slides09.html#comparing-unbiased-estimators-cramer-rao-lower-bound-crlb",
    "title": "Efficiency & CRLB",
    "section": "Comparing unbiased estimators: Cramer-Rao Lower Bound (CRLB)",
    "text": "Comparing unbiased estimators: Cramer-Rao Lower Bound (CRLB)\n\n\n\n\n\n\n\nCRLB\n\n\nIf \\(X_1, ..., X_n\\) are an iid sample from a distribution with pdf \\(f(x|\\theta)\\), then any unbiased estimator \\(\\hat\\theta\\) of \\(\\theta\\) satisfies:\n\\[V(\\hat{\\theta}) \\ge \\frac{1}{n I(\\theta)}\\]\nwhere \\(I(\\theta)\\) is the Fisher Information of \\(X_i\\)\n\n\n\n\n\nIf the variance of an estimator is equal to the CRLB, then there is no other unbiased estimator with more precision"
  },
  {
    "objectID": "slides/09/slides09.html#fisher-information",
    "href": "slides/09/slides09.html#fisher-information",
    "title": "Efficiency & CRLB",
    "section": "Fisher Information",
    "text": "Fisher Information\n\n\n\n\n\n\nFisher Information\n\n\nThe Fisher Information of a random variable \\(X\\) is \\[I(\\theta) = E[(\\frac{d}{d\\theta} \\ln f(x|\\theta))^2] = -   E(l''(\\theta))\\]\n*provided the domain of \\(X\\) does not depend on \\(\\theta\\) (and a few other regularity conditions)"
  },
  {
    "objectID": "slides/09/slides09.html#intuition-fisher-information",
    "href": "slides/09/slides09.html#intuition-fisher-information",
    "title": "Efficiency & CRLB",
    "section": "Intuition: Fisher information",
    "text": "Intuition: Fisher information\n\\[I(\\theta) = E_X[(\\frac{d}{d\\theta} \\ln f(x|\\theta))^2] = -   E_X(l''(\\theta))\\]\n\n\\(l(\\theta)\\) is the log-likelihood function\n\\(l'(\\theta)\\) gives the slope of the log-likelihood at any point, \\(l''(\\theta)\\) gives the curvature at any given point\nThe Fisher information “averages out” over \\(X\\) and summarizes the overall curvature of the log-likelihood function"
  },
  {
    "objectID": "slides/09/slides09.html#intuition-fisher-information-1",
    "href": "slides/09/slides09.html#intuition-fisher-information-1",
    "title": "Efficiency & CRLB",
    "section": "Intuition: Fisher information",
    "text": "Intuition: Fisher information\n\n\nHigher Fisher Information\n\n\n\n\n\n\n\n\n\n\nLower Fisher Information\n\n\n\n\n\n\n\n\n\n\n\nDistributions with a “pointier” log-likelihood have higher Fisher information than distributions with lower Fisher information\n\n\nLower Fisher information \\(\\to\\) flat log-likelihood \\(\\to\\) lots of estimators “close” to the MLE"
  },
  {
    "objectID": "slides/09/slides09.html#practice-fisher-information",
    "href": "slides/09/slides09.html#practice-fisher-information",
    "title": "Efficiency & CRLB",
    "section": "Practice: Fisher information",
    "text": "Practice: Fisher information\nFind the Fisher information for \\(Y\\), where \\(X \\sim Exp(\\lambda)\\)"
  },
  {
    "objectID": "slides/09/slides09.html#practice-finding-crlb",
    "href": "slides/09/slides09.html#practice-finding-crlb",
    "title": "Efficiency & CRLB",
    "section": "Practice: finding CRLB",
    "text": "Practice: finding CRLB\nLet \\(Y_1, ..., Y_n\\) be \\(n\\) Exp(\\(\\lambda\\)) random variables Let \\(\\widehat\\lambda = \\frac{n}{\\sum Y_i}\\). How does Var(\\(\\widehat\\lambda\\)) compare with the CRLB?"
  },
  {
    "objectID": "slides/09/slides09.html#section-2",
    "href": "slides/09/slides09.html#section-2",
    "title": "Efficiency & CRLB",
    "section": "",
    "text": "So far, we’ve found the bias, variance, and MSE analytically (with formal mathematical formulas). In practice, we often can’t do this.\n\nFor example: The median \\(m\\) of an Exponential(\\(\\lambda\\)) distribution satisfies P(X ≤ m) = 0.5. Solving \\(1 - e^(-\\lambda m) = 0.5\\) gives \\(m = ln(2) / \\lambda\\).\n\n\nThis suggests an estimator for \\(\\lambda\\) based on the median: \\(\\widehat{\\lambda} = \\frac{ln(2)}{m}\\)\n\n\nFinding the analytical variance of \\(\\widehat \\lambda\\) is complicated. Finding the sampling distribution of \\(m\\) is complicated, and finding the non-linear transformation is also complicated."
  },
  {
    "objectID": "slides/09/slides09.html#your-turn-simulation",
    "href": "slides/09/slides09.html#your-turn-simulation",
    "title": "Efficiency & CRLB",
    "section": "Your turn: simulation",
    "text": "Your turn: simulation\nUse simulation to see whether \\(\\frac{ln(2)}{m}\\) (a) is unbiased and (b) achieves the CRLB"
  },
  {
    "objectID": "slides/16/slides16.html#goal-develop-an-interval-estimate-of-a-population-parameter",
    "href": "slides/16/slides16.html#goal-develop-an-interval-estimate-of-a-population-parameter",
    "title": "CLT-based Confidence Intervals",
    "section": "Goal: develop an interval estimate of a population parameter",
    "text": "Goal: develop an interval estimate of a population parameter"
  },
  {
    "objectID": "slides/16/slides16.html#section",
    "href": "slides/16/slides16.html#section",
    "title": "CLT-based Confidence Intervals",
    "section": "",
    "text": "Example: Suppose we observe \\(X_1, ..., X_n\\) , which represent the error rates of \\(n\\) different forensic examiners. I am interested in finding a confidence interval for \\(\\mu\\), the average error rate in the population. We have a sample of size 30 shown below:\n\n\n\n\n\n\nmean\nsd\nn\n\n\n\n\n0.0325\n0.0343\n30"
  },
  {
    "objectID": "slides/16/slides16.html#lets-review-our-confidence-interval-toolkit-so-far",
    "href": "slides/16/slides16.html#lets-review-our-confidence-interval-toolkit-so-far",
    "title": "CLT-based Confidence Intervals",
    "section": "Let’s review our confidence interval toolkit so far:",
    "text": "Let’s review our confidence interval toolkit so far:\n\nConstruct a bootstrap distribution and find a percentile-based confidence interval\nEither (a) assume a normal population or (b) use the CLT to construct a formula t-based confidence interval\nAssume a different population distribution and construct a pivot-based confidence interval"
  },
  {
    "objectID": "slides/16/slides16.html#bootstrap-percentile-interval",
    "href": "slides/16/slides16.html#bootstrap-percentile-interval",
    "title": "CLT-based Confidence Intervals",
    "section": "Bootstrap percentile interval",
    "text": "Bootstrap percentile interval\n\nerror_rates &lt;- examiner_error_rates$error_rate\nn = length(error_rates)\nN = 10^4\nboot_means = numeric(N)\n\nfor(i in 1:N){\n  x &lt;- sample(error_rates, size = n, replace = TRUE)\n  boot_means[i] &lt;- mean(x, na.rm = TRUE)\n}\n\nquantile(boot_means, probs = c(.025, .975))\n\n      2.5%      97.5% \n0.02147176 0.04576585"
  },
  {
    "objectID": "slides/16/slides16.html#clt-based-t-confidence-interval",
    "href": "slides/16/slides16.html#clt-based-t-confidence-interval",
    "title": "CLT-based Confidence Intervals",
    "section": "CLT-based t confidence interval",
    "text": "CLT-based t confidence interval\n\nqt(.975, df = 29)\n\n[1] 2.04523"
  },
  {
    "objectID": "slides/16/slides16.html#clt-based-t-confidence-interval-1",
    "href": "slides/16/slides16.html#clt-based-t-confidence-interval-1",
    "title": "CLT-based Confidence Intervals",
    "section": "CLT-based t confidence interval",
    "text": "CLT-based t confidence interval\nCan also use a built-in R function:\n\nt.test(error_rates, conf.level = .95)$conf\n\n[1] 0.01969500 0.04531505\nattr(,\"conf.level\")\n[1] 0.95\n\n\nWhen do we trust this confidence interval?"
  },
  {
    "objectID": "slides/16/slides16.html#look-at-bootstrap-t",
    "href": "slides/16/slides16.html#look-at-bootstrap-t",
    "title": "CLT-based Confidence Intervals",
    "section": "Look at bootstrap T*",
    "text": "Look at bootstrap T*\n\nt_star &lt;- numeric(N)\n\nfor(i in 1:N){\n  x &lt;- sample(error_rates, size = n, replace = TRUE)\n  t_star[i] &lt;- (mean(x) - mean(error_rates))/(sd(x)/sqrt(n))\n}"
  },
  {
    "objectID": "slides/16/slides16.html#issues",
    "href": "slides/16/slides16.html#issues",
    "title": "CLT-based Confidence Intervals",
    "section": "Issues",
    "text": "Issues\n\nDistribution of \\(T*\\) is slightly left-skewed\nTails of \\(T*\\) don’t seem to match the theoretical t"
  },
  {
    "objectID": "slides/16/slides16.html#why-does-this-happen",
    "href": "slides/16/slides16.html#why-does-this-happen",
    "title": "CLT-based Confidence Intervals",
    "section": "Why does this happen?",
    "text": "Why does this happen?"
  },
  {
    "objectID": "slides/16/slides16.html#bootstrap-t-confidence-interval",
    "href": "slides/16/slides16.html#bootstrap-t-confidence-interval",
    "title": "CLT-based Confidence Intervals",
    "section": "Bootstrap t confidence interval",
    "text": "Bootstrap t confidence interval\nThe CLT-based confidence interval relies on:\n\\[1-\\alpha = P(t_{\\alpha/2} &lt; \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} &lt; t_{1-\\alpha/2})\\]\nIdea: generate “better” quantiles using the bootstrap distribution:"
  },
  {
    "objectID": "slides/16/slides16.html#then-solve-for-mu",
    "href": "slides/16/slides16.html#then-solve-for-mu",
    "title": "CLT-based Confidence Intervals",
    "section": "Then solve for \\(\\mu\\):",
    "text": "Then solve for \\(\\mu\\):"
  },
  {
    "objectID": "slides/16/slides16.html#to-obtain-the-bootstrap-t-confidence-interval",
    "href": "slides/16/slides16.html#to-obtain-the-bootstrap-t-confidence-interval",
    "title": "CLT-based Confidence Intervals",
    "section": "to obtain the bootstrap \\(t\\) confidence interval:",
    "text": "to obtain the bootstrap \\(t\\) confidence interval:\n\n\n\n\n\n\n\n\nWatch out!\n\n\nLower bound is computed from the upper percentile and upper bound is computed from the lower percentile"
  },
  {
    "objectID": "slides/16/slides16.html#bootstrap-t-algorithm",
    "href": "slides/16/slides16.html#bootstrap-t-algorithm",
    "title": "CLT-based Confidence Intervals",
    "section": "Bootstrap T algorithm",
    "text": "Bootstrap T algorithm\n\n\n\n\n\n\nBootstrap T algorithm\n\n\n\nRepeat the bootstrap procedure many times:\n\nTake bootstrap sample\nCompute mean (\\(\\bar{X}^*\\)) and sd (\\(s^*\\)) of each bootstrap sample\nCompute t-statistic for each bootstrap sample: \\(T^* = \\frac{\\bar{X}^* - \\bar{X}}{s^*/\\sqrt{n}}\\)\n\nFind quantiles \\(Q_{\\alpha/2}^*\\) and \\(Q_{1-\\alpha/2}^*\\) using distribution of \\(T^*\\)\nCompute confidence interval using the bootstrap t quantiles: \\[(\\bar{x} - Q^*_{1-\\alpha/2}\\frac{s}{\\sqrt{n}}, \\bar{x} - Q^*_{\\alpha/2}\\frac{s}{\\sqrt{n}})\\]"
  },
  {
    "objectID": "slides/16/slides16.html#example-bootstrap-t-for-error-rates",
    "href": "slides/16/slides16.html#example-bootstrap-t-for-error-rates",
    "title": "CLT-based Confidence Intervals",
    "section": "Example: bootstrap t for error rates",
    "text": "Example: bootstrap t for error rates\n\nn = length(error_rates)\nN = 10^4\nboot_means &lt;- numeric(N)\nboot_tstar &lt;- numeric(N)\nfor(i in 1:N){\n  x &lt;- sample(error_rates, size = n, replace = TRUE)\n  boot_means[i] &lt;- mean(x)\n  t_star[i] &lt;- (mean(x) - mean(error_rates))/(sd(x)/sqrt(n))\n}\n\nquantile(t_star, probs = c(.025, .975))\n\n     2.5%     97.5% \n-2.934144  1.717921 \n\nmean(error_rates) - quantile(t_star, probs = c(.025, .975))*sd(error_rates)/sqrt(n)\n\n      2.5%      97.5% \n0.05088264 0.02174506"
  },
  {
    "objectID": "slides/16/slides16.html#summary-of-results",
    "href": "slides/16/slides16.html#summary-of-results",
    "title": "CLT-based Confidence Intervals",
    "section": "Summary of results",
    "text": "Summary of results\n\n\n\n\nLower\nUpper\n\n\n\n\nPercentile Bootstrap\n.021\n.0458\n\n\nCLT t-based\n.020\n.0453\n\n\nBootstrap t\n.022\n.0509"
  },
  {
    "objectID": "slides/16/slides16.html#r-activity",
    "href": "slides/16/slides16.html#r-activity",
    "title": "CLT-based Confidence Intervals",
    "section": "R Activity",
    "text": "R Activity\nThe {nycflights23} R package contains a dataset called flights. This dataset contains all flights in and out of NYC-area airports in 2023. Since the dataset has all flights, we can treat it as a population. We’re interested in the average departure delay of flights departing from NYC area airports.\nWe’ll explore the performance of our different confidence interval procedures. To do so, you’ll first draw a sample from the flights dataset. You’ll treat this as your data sample throughout the rest of the activity, and compare your confidence intervals to the true value from the whole dataset."
  },
  {
    "objectID": "slides/16/slides16.html#results",
    "href": "slides/16/slides16.html#results",
    "title": "CLT-based Confidence Intervals",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/05/slides05.html#section",
    "href": "slides/05/slides05.html#section",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "You are testing seeds from a new plant variety. You plant 10 seeds (n=10) and observe that 3 of them successfully germinate (k=3). Consider two hypotheses about the true germination probability (p) for this variety:\n\nHypothesis A: \\(p = 0.1\\) (10% germination rate)\nHypothesis B: \\(p = 0.4\\) (40% germination rate)\n\n\nGiven that you observed 3 germinations out of 10 seeds, which hypothesis (A or B) is more supported by the data?\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/05/slides05.html#evaluating-intuitively",
    "href": "slides/05/slides05.html#evaluating-intuitively",
    "title": "Maximum Likelihood Estimation",
    "section": "Evaluating Intuitively",
    "text": "Evaluating Intuitively"
  },
  {
    "objectID": "slides/05/slides05.html#evaluating-formally-problem-set-up",
    "href": "slides/05/slides05.html#evaluating-formally-problem-set-up",
    "title": "Maximum Likelihood Estimation",
    "section": "Evaluating Formally: Problem Set Up",
    "text": "Evaluating Formally: Problem Set Up\n\nLet \\(X\\) be the number of plants that successfully germinate\n\\(X \\sim \\text{Binom}(10, p)\\) and we observed \\(X=3\\)\n\n\n\n\n\\(P(X=3)\\) if \\(p=.1\\)\n\n\\(P(X=3)\\) if \\(p=.4\\)"
  },
  {
    "objectID": "slides/05/slides05.html#section-1",
    "href": "slides/05/slides05.html#section-1",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "In general, the likelihood of seeing \\(X=3\\) given \\(p\\) is:\n\\[{10 \\choose 3} p^3 (1-p)^7\\]\n\nWhat value of \\(p\\) maximizes this likelihood?"
  },
  {
    "objectID": "slides/05/slides05.html#methods-of-maximizing",
    "href": "slides/05/slides05.html#methods-of-maximizing",
    "title": "Maximum Likelihood Estimation",
    "section": "Methods of maximizing",
    "text": "Methods of maximizing\n\nApproximate the solution graphically\nFind a numerical approximation\nUse calculus"
  },
  {
    "objectID": "slides/05/slides05.html#approximating-with-a-graph",
    "href": "slides/05/slides05.html#approximating-with-a-graph",
    "title": "Maximum Likelihood Estimation",
    "section": "Approximating with a graph",
    "text": "Approximating with a graph"
  },
  {
    "objectID": "slides/05/slides05.html#approximating-with-a-graph-code",
    "href": "slides/05/slides05.html#approximating-with-a-graph-code",
    "title": "Maximum Likelihood Estimation",
    "section": "Approximating with a graph: code",
    "text": "Approximating with a graph: code\n\ngerm_function &lt;- function(p) {choose(10,3) * p^3 * (1-p)^7} # define function\nggplot() + \n  geom_function(fun = germ_function) +\n  xlim(c(0,1))"
  },
  {
    "objectID": "slides/05/slides05.html#approximating-numerically",
    "href": "slides/05/slides05.html#approximating-numerically",
    "title": "Maximum Likelihood Estimation",
    "section": "Approximating numerically",
    "text": "Approximating numerically\n\noptimize(germ_function, interval = c(0,1), maximum = TRUE)\n\n\n\n$maximum\n[1] 0.3000157\n\n$objective\n[1] 0.2668279"
  },
  {
    "objectID": "slides/05/slides05.html#with-calculus",
    "href": "slides/05/slides05.html#with-calculus",
    "title": "Maximum Likelihood Estimation",
    "section": "With calculus",
    "text": "With calculus"
  },
  {
    "objectID": "slides/05/slides05.html#a-trick",
    "href": "slides/05/slides05.html#a-trick",
    "title": "Maximum Likelihood Estimation",
    "section": "A “trick”",
    "text": "A “trick”\nProbability functions often are tricky to differentiate because of the product rule. Lucky for us, we can maximize the log instead."
  },
  {
    "objectID": "slides/05/slides05.html#can-we-generalize-to-any-data",
    "href": "slides/05/slides05.html#can-we-generalize-to-any-data",
    "title": "Maximum Likelihood Estimation",
    "section": "Can we generalize to any data?",
    "text": "Can we generalize to any data?"
  },
  {
    "objectID": "slides/05/slides05.html#section-2",
    "href": "slides/05/slides05.html#section-2",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Likelihood function\n\n\nLet \\(f(x; \\theta)\\) denote the probability mass function for a discrete distribution with associated parameter \\(\\theta\\). Suppose \\(X_1, X_2, ... , X_n\\) are a random sample from this distribution and \\(x_1, x_2, ... , x_n\\) are the actual observed values. Then, the likelihood function of \\(\\theta\\) is:\n  \n\n\n\n\n\n\n\n\n\n\nMaximum likelihood estimate\n\n\nA maximum likelihood estimate (MLE), \\(\\hat{\\theta}_{MLE}\\) is the value of \\(\\theta\\) that maximizes the likelihood function, or equivalently, that maximizes the log-likelihood \\(\\ln L(\\theta)\\)"
  },
  {
    "objectID": "slides/05/slides05.html#bernoulli-data",
    "href": "slides/05/slides05.html#bernoulli-data",
    "title": "Maximum Likelihood Estimation",
    "section": "Bernoulli data",
    "text": "Bernoulli data\nAn alternative way to express this example is \\(X_1, ..., X_n \\sim \\text{Bernoulli}(\\theta)\\)"
  },
  {
    "objectID": "slides/05/slides05.html#is-the-likelihood-also-a-pdf",
    "href": "slides/05/slides05.html#is-the-likelihood-also-a-pdf",
    "title": "Maximum Likelihood Estimation",
    "section": "Is the likelihood also a PDF?",
    "text": "Is the likelihood also a PDF?\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/05/slides05.html#aside-notation",
    "href": "slides/05/slides05.html#aside-notation",
    "title": "Maximum Likelihood Estimation",
    "section": "Aside: notation",
    "text": "Aside: notation\n\n\n\n\n\n\nEstimator\n\n\n  \n\n\n\n\n\n\n\n\n\nEstimate"
  },
  {
    "objectID": "slides/05/slides05.html#section-3",
    "href": "slides/05/slides05.html#section-3",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Let \\(X_1, ..., X_n\\) be an iid random sample from a distribution with PDF\n\\[f(x|\\theta)= (\\theta + 1)x^\\theta, 0 \\le x \\le 1\\]\n\n\nFind the maximum likelihood estimator for a random sample of size \\(n\\) using calculus.\nSuppose we observe a sample of size 5: {.83, .49, .72, .57, .66}. Find the maximum likelihood estimate and verify with a graph or numerical approximation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat250-S25",
    "section": "",
    "text": "Landing page for Stat250 course materials in Spring 2025\n\nIn-Class Materials\nDay02: slides handout r code\nDay03: handout activity qmd\nDay04: slides | handout\nDay05: slides | handout\nDay06: slides | handout\nDay07: slides | handout\nDay08: slides | handout\nDay09: slides | handout\nDay10: handout | R activity\nDay11: handout\nDay12: Exam I corrections info\nDay13: slides | handout | R activity\nDay14: slides | handout\nDay15: handout\nDay16: handout | R activity | slides\n\n\nOther Resources and Readings\n\nR Basics\nHow to do homework in R in Stat250"
  },
  {
    "objectID": "slides/04/slides04.html#plan-for-today",
    "href": "slides/04/slides04.html#plan-for-today",
    "title": "Sampling Distributions + Probability Review",
    "section": "Plan for today:",
    "text": "Plan for today:\n\nHW00 probability review\nMore on sampling distributions\nIntro to Estimation"
  },
  {
    "objectID": "slides/04/slides04.html#overview-of-hw00-topics",
    "href": "slides/04/slides04.html#overview-of-hw00-topics",
    "title": "Sampling Distributions + Probability Review",
    "section": "Overview of HW00 Topics",
    "text": "Overview of HW00 Topics\n\n\nWorking with a PDF\nWorking with a CDF\nCalculating Moments\nMoments from PDF\nDeriving and Using MGFs\nRecognizing distribution from MGF\nTransformation of a Random Variable\nJoint PDF Calculations"
  },
  {
    "objectID": "slides/04/slides04.html#generally-went-ok",
    "href": "slides/04/slides04.html#generally-went-ok",
    "title": "Sampling Distributions + Probability Review",
    "section": "Generally went OK",
    "text": "Generally went OK\n\n\nWorking with a PDF\nWorking with a CDF\nCalculating Moments\nMoments from PDF\nDeriving and Using MGFs\nRecognizing distribution from MGF\nTransformation of a Random Variable\nJoint PDF Calculations"
  },
  {
    "objectID": "slides/04/slides04.html#big-mechanics-to-know-1-variable",
    "href": "slides/04/slides04.html#big-mechanics-to-know-1-variable",
    "title": "Sampling Distributions + Probability Review",
    "section": "Big mechanics to know (1 variable)",
    "text": "Big mechanics to know (1 variable)\n\nPDF \\(f_X(x)\\) gives probabilities by integrating\nCDF \\(F_X(x)\\) gives probabilities directly\nFind \\(E[X]\\) with pdf by \\(\\int_\\infty x f_X(x) dx\\)\nLoTUS: \\(E[g(X)] = \\int_\\infty g(x) f_X(x) dx\\)\n\\(V[X] = E[(X-E[X])^2] = E[X^2] - E[X]^2\\)"
  },
  {
    "objectID": "slides/04/slides04.html#big-mechanics-to-know-2-variable",
    "href": "slides/04/slides04.html#big-mechanics-to-know-2-variable",
    "title": "Sampling Distributions + Probability Review",
    "section": "Big mechanics to know (2 variable)",
    "text": "Big mechanics to know (2 variable)\n\nPDF \\(f_{X,Y}(x,y)\\) gives probabilities by integrating\nCDF \\(F_{X, Y}(x,y)\\) gives probabilities directly\nMarginal pdf: \\(f_X(x) = \\int_\\infty y f_{X,Y} dy\\)\nConditional pdf \\(f_{X|Y}(x,y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\\)\nTwo variables are independent if \\(f_{X,Y} = f_{X} f_{Y}\\) or \\(f_X = f_{X|Y}\\)"
  },
  {
    "objectID": "slides/04/slides04.html#working-with-moments",
    "href": "slides/04/slides04.html#working-with-moments",
    "title": "Sampling Distributions + Probability Review",
    "section": "Working with moments",
    "text": "Working with moments\nFor any two variables X and Y:\n\n\\(E[aX + b] = a E[X] + b\\)\n\\(V[aX + b] = a^2 V[X]\\)\n\\(E[X + Y] = E[X] + E[Y]\\)\n\\(E[aX + bY + c] = a E[X] + b E[Y] + c\\)\n\\(V[X + Y] = V[X] + V[Y] + 2 Cov(X,Y)\\)\n\\(Cov(X,Y) = E[(X - E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]\\)\n\n\nIf X and Y are independent:\n\n\\(Cov(X,Y) = 0\\)\n\\(V[X + Y] = V[X] + V[Y]\\)\n\\(V[aX + bY + c] = a^2 V[X] + b^2 V[Y]\\)"
  },
  {
    "objectID": "slides/04/slides04.html#some-issues",
    "href": "slides/04/slides04.html#some-issues",
    "title": "Sampling Distributions + Probability Review",
    "section": "Some issues",
    "text": "Some issues\n\n\nWorking with a PDF\nWorking with a CDF\nCalculating Moments\nMoments from PDF\nDeriving and Using MGFs\nRecognizing distribution from MGF\nTransformation of a Random Variable\nJoint PDF Calculations"
  },
  {
    "objectID": "slides/04/slides04.html#recap-from-friday",
    "href": "slides/04/slides04.html#recap-from-friday",
    "title": "Sampling Distributions + Probability Review",
    "section": "Recap from Friday",
    "text": "Recap from Friday\n\n\n\n\n\n\nCentral Limit Theorem\n\n\nSuppose we have an iid sample \\(X_1, ..., X_n \\sim F_x\\). The CLT tells us that, as our sample size approaches \\(\\infty\\),\n\\[F_\\bar{X}(\\bar{X}) \\to N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\]\n\n\n\n\nSampling distribution is centered at population mean\nAs \\(n \\to \\infty\\), \\(\\sigma_\\bar{X} \\to 0\\)\nIt doesn’t matter what shape \\(X_i\\) is!"
  },
  {
    "objectID": "slides/04/slides04.html#example-binomial-data",
    "href": "slides/04/slides04.html#example-binomial-data",
    "title": "Sampling Distributions + Probability Review",
    "section": "Example: Binomial data",
    "text": "Example: Binomial data\nAccording to the 2004 American Community Survey, 28% of adults over 25 years old in Utah have completed a bachelor’s degree. In a random sample of 64 adults over age 25 from Utah, what is the probability that at least 30 have a bachelor’s degree?\nLet \\(X_i\\) indicate whether a sampled person has a bachelor’s degree. Then, \\(X_1, ...., X_{64} \\sim \\text{Binom}(n=1,p=.28)\\).\n\nOr, \\(X = \\sum X_i \\sim \\text{Binom}(64, p = .28)\\)\n\n\nUsing the CLT, \\(\\bar{X} \\sim N(p, SE_\\bar{X}) \\sim N(0.28, \\sqrt{\\frac{.28(1-.28)}{64}} = .056)\\)"
  },
  {
    "objectID": "slides/04/slides04.html#example-binomial-data-1",
    "href": "slides/04/slides04.html#example-binomial-data-1",
    "title": "Sampling Distributions + Probability Review",
    "section": "Example: Binomial data",
    "text": "Example: Binomial data\n\\(\\bar{X} \\sim N(0.28, .056)\\). To find the probability that at least 30% of people in the sample have a bachelor’s degree,\n\\[P(\\hat{p} \\ge 0.30) = P(\\bar{X} \\ge 0.30)\\]\n\n\npnorm(0.30, mean = 0.28, sd = .056, lower.tail = FALSE)\n\n[1] 0.3604924\n\n\n\n\n\nNote: It’s really important to be careful about when you’re working with variance and when you’re working with standard error"
  },
  {
    "objectID": "slides/04/slides04.html#issue",
    "href": "slides/04/slides04.html#issue",
    "title": "Sampling Distributions + Probability Review",
    "section": "Issue",
    "text": "Issue\nIn this case, the CLT uses a continuous density to approximate a discrete random variable. 30% of 64 people is 19.2 – we can’t actually have 19.2 answer “yes”!\n\n\\[P(X \\ge 19.2) = P(X \\ge 20)\\]\n\n\n\\[\\sum_{k=20}^{64} \\binom{64}{k} .28^k .72^{64-k}\\]\n\n\n\n1 - pbinom(19, 64, .28)\n\n[1] 0.3242167\n\n\n\n\nThe CLT overestimates this probability by .04!"
  },
  {
    "objectID": "slides/04/slides04.html#continuity-correction",
    "href": "slides/04/slides04.html#continuity-correction",
    "title": "Sampling Distributions + Probability Review",
    "section": "Continuity Correction",
    "text": "Continuity Correction\nWhen using the CLT with discrete data, split the difference between 19 and 20:\n\\[P(X \\ge 19.2) \\approx P(X \\ge 19.5)\\]"
  },
  {
    "objectID": "slides/02/slides02.html#goals-for-today",
    "href": "slides/02/slides02.html#goals-for-today",
    "title": "Intro to Permutation Tests",
    "section": "Goals for today:",
    "text": "Goals for today:\n\nReview (or introduce!) hypothesis testing\nPractice with notation\nAsk questions and talk to each other\nGet comfier in R"
  },
  {
    "objectID": "slides/02/slides02.html#example",
    "href": "slides/02/slides02.html#example",
    "title": "Intro to Permutation Tests",
    "section": "Example",
    "text": "Example\nEvidence suggests that reward systems may operate in the opposite way from what is intended:\n\nRanking systems may decrease productivity;\nRewards may not stimulate learning"
  },
  {
    "objectID": "slides/02/slides02.html#experiment",
    "href": "slides/02/slides02.html#experiment",
    "title": "Intro to Permutation Tests",
    "section": "Experiment",
    "text": "Experiment\n\n47 subjects with considerable experience in creativity were recruited\nRandomly assigned to either intrinsic- or extrinsic-motivation group\nSubjects completed a questionnaire related to either intrinsic or extrinsic reasons for writing\nAll subjects were asked to write a Haiku about laughter\nPoems were scored by a panel of 12 poets, evaluated on 40-point creativity scale\n\n\n\nSource: Amabile, T. M. (1985). Motivation and creativity: Effects of motivational orientation on creative writers. Journal of Personality and Social Psychology, 48(2), 393."
  },
  {
    "objectID": "slides/02/slides02.html#results",
    "href": "slides/02/slides02.html#results",
    "title": "Intro to Permutation Tests",
    "section": "Results",
    "text": "Results\n\n\n\n\nTreatment\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\nExtrinsic\n5\n12.2\n17.2\n19.0\n24.0\n15.7\n5.3\n23\n0\n\n\nIntrinsic\n12\n17.4\n20.4\n22.3\n29.7\n19.9\n4.4\n24\n0"
  },
  {
    "objectID": "slides/02/slides02.html#the-logic-of-hypothesis-testing",
    "href": "slides/02/slides02.html#the-logic-of-hypothesis-testing",
    "title": "Intro to Permutation Tests",
    "section": "The logic of hypothesis testing",
    "text": "The logic of hypothesis testing\n\nFormulate two competing hypotheses about the population\nCalculate a test statistic summarizing the relevant information to the claims\nLook at the behavior of the test statistic assuming that the initial claim is true\nCompare the observed test statistic to the expected behavior (the distribution created in step 3).\nHow likely is it that the observed test statistic occurred by chance alone?"
  },
  {
    "objectID": "slides/02/slides02.html#permutation-resampling",
    "href": "slides/02/slides02.html#permutation-resampling",
    "title": "Intro to Permutation Tests",
    "section": "Permutation resampling",
    "text": "Permutation resampling\n\n\n\n\nIntrinsic\n\n\n12\n16.6\n19.1\n20.5\n22.1\n24\n\n\n12\n17.2\n19.3\n20.6\n22.2\n24.3\n\n\n12.9\n17.5\n19.8\n21.3\n22.6\n26.7\n\n\n13.6\n18.2\n20.3\n21.6\n23.1\n29.7\n\n\n\n\n\n\n\nExtrinsic\n\n\n5\n11.8\n15\n17.4\n18.7\n21.2\n\n\n5.4\n12\n16.8\n17.5\n19.2\n22.1\n\n\n6.1\n12.3\n17.2\n18.5\n19.5\n24\n\n\n10.9\n14.8\n17.2\n18.7\n20.7"
  },
  {
    "objectID": "slides/02/slides02.html#permutation-resampling-1",
    "href": "slides/02/slides02.html#permutation-resampling-1",
    "title": "Intro to Permutation Tests",
    "section": "Permutation resampling",
    "text": "Permutation resampling\n\n\n\n\nIntrinsic\n\n\n5\n12.9\n17.2\n18.2\n20.3\n21.3\n\n\n10.9\n13.6\n17.2\n18.7\n20.6\n22.2\n\n\n12\n14.8\n17.2\n19.2\n20.7\n23.1\n\n\n12.3\n16.8\n17.5\n19.5\n21.2\n26.7\n\n\n\n\n\n\n\nExtrinsic\n\n\n5.4\n12\n17.5\n19.3\n22.1\n24\n\n\n6.1\n15\n18.5\n19.8\n22.1\n24.3\n\n\n11.8\n16.6\n18.7\n20.5\n22.6\n29.7\n\n\n12\n17.4\n19.1\n21.6\n24"
  },
  {
    "objectID": "slides/02/slides02.html#permutation-resampling-2",
    "href": "slides/02/slides02.html#permutation-resampling-2",
    "title": "Intro to Permutation Tests",
    "section": "Permutation resampling",
    "text": "Permutation resampling\n\n\n\n\nIntrinsic\n\n\n5\n12\n17.2\n19.2\n21.3\n22.6\n\n\n6.1\n12.9\n17.5\n19.8\n21.6\n23.1\n\n\n10.9\n14.8\n17.5\n20.7\n22.1\n26.7\n\n\n12\n15\n18.7\n21.2\n22.2\n29.7\n\n\n\n\n\n\n\nExtrinsic\n\n\n5.4\n13.6\n17.2\n18.7\n20.3\n24\n\n\n11.8\n16.6\n17.4\n19.1\n20.5\n24\n\n\n12\n16.8\n18.2\n19.3\n20.6\n24.3\n\n\n12.3\n17.2\n18.5\n19.5\n22.1"
  },
  {
    "objectID": "slides/02/slides02.html#your-turn-1",
    "href": "slides/02/slides02.html#your-turn-1",
    "title": "Intro to Permutation Tests",
    "section": "Your turn",
    "text": "Your turn\nHow likely is it that the observed results occurred by chance alone?\nWhat does this say about the null hypothesis?"
  },
  {
    "objectID": "slides/02/slides02.html#your-turn-2",
    "href": "slides/02/slides02.html#your-turn-2",
    "title": "Intro to Permutation Tests",
    "section": "Your turn",
    "text": "Your turn\nHow likely is it that the observed test statistic occurred by chance alone?\nWhat does this say about the null hypothesis?"
  },
  {
    "objectID": "slides/02/slides02.html#p-value",
    "href": "slides/02/slides02.html#p-value",
    "title": "Intro to Permutation Tests",
    "section": "p-value",
    "text": "p-value\nDefinition: fraction of times the random test statistic exceeds the original test statistic\n\n\n\n\n\n\n\n\n\n\n\n\nObserved test statistic:  \\(\\overline{x}_{int} - \\overline{x}_{ext} \\approx 4.2\\)\n10 of the random test statistics exceed 4.2"
  },
  {
    "objectID": "slides/02/slides02.html#strength-of-evidence",
    "href": "slides/02/slides02.html#strength-of-evidence",
    "title": "Intro to Permutation Tests",
    "section": "Strength of evidence",
    "text": "Strength of evidence\np-values provide a continuous measurement of the strength of evidence against the null hypothesis"
  },
  {
    "objectID": "slides/02/slides02.html#permutation-test-algorithm",
    "href": "slides/02/slides02.html#permutation-test-algorithm",
    "title": "Intro to Permutation Tests",
    "section": "Permutation test algorithm",
    "text": "Permutation test algorithm\n\n\nPool the \\(m+n\\) data values\nDraw a resample of size \\(m\\) without replacement, assign these values to group 1. Assign the remaining \\(n\\) values to group 2.\nCalculate the test statistic comparing the samples from the resampled groups.\nRepeat steps 2 and 3 until we have enough samples.\nEstimate the p-value as the proportion of times the observed test statistic exceeds the original (observed) test statistic\n\\(p\\text{-value}=\\frac{\\text{# statistics that exceed the original} + 1}{\\text{# of statistics in the distribution} + 1}\\)"
  },
  {
    "objectID": "slides/02/slides02.html#where-were-going",
    "href": "slides/02/slides02.html#where-were-going",
    "title": "Intro to Permutation Tests",
    "section": "Where we’re going",
    "text": "Where we’re going\nInstead of a permutation test, we can also use facts about the sampling distribution from probability to compute p-values with math."
  },
  {
    "objectID": "slides/07/slides07.html#recap",
    "href": "slides/07/slides07.html#recap",
    "title": "Method of Moments",
    "section": "Recap",
    "text": "Recap\n\nObserve data \\(X_1, ..., X_n \\sim F_x(x|\\theta)\\), where \\(\\theta\\) is unknown\nGoal: Estimate \\(\\theta\\) based on the values of \\(X_i\\) by formulating an estimator \\(\\hat \\theta\\)\nOne technique is to use the maximum likelihood estimator, which finds the value of \\(\\theta\\) that maximizes the joint probability \\(\\prod_{i=1}^n f_x(x_i | \\theta)\\)\nToday, we’ll talk about another technique for estimating \\(\\theta\\)"
  },
  {
    "objectID": "slides/07/slides07.html#which-population-is-more-likely",
    "href": "slides/07/slides07.html#which-population-is-more-likely",
    "title": "Method of Moments",
    "section": "Which population is more likely?",
    "text": "Which population is more likely?\n\n\n\nSample\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n   mean   var\n  &lt;dbl&gt; &lt;dbl&gt;\n1  20.7  16.5\n\n\n\n\n\n\n\n\\(N(10, 5)\\)\\(N(20, 5)\\)"
  },
  {
    "objectID": "slides/07/slides07.html#which-population-is-more-likely-1",
    "href": "slides/07/slides07.html#which-population-is-more-likely-1",
    "title": "Method of Moments",
    "section": "Which population is more likely?",
    "text": "Which population is more likely?\n\n\n\nSample\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n   mean   var\n  &lt;dbl&gt; &lt;dbl&gt;\n1  20.7  16.5\n\n\n\n\n\n\n\n\\(N(20, 1.5^2)\\)\\(N(20, 4^2)\\)"
  },
  {
    "objectID": "slides/07/slides07.html#method-of-moments-mom",
    "href": "slides/07/slides07.html#method-of-moments-mom",
    "title": "Method of Moments",
    "section": "Method of Moments (MoM)",
    "text": "Method of Moments (MoM)\n\nMean of sample data should match mean of theoretical distribution\nVariance of sample data should match variance of theoretical distribution\nSkewness of sample data should match skewness of theoretical distribution\n\\(\\vdots\\)\n(for as many unknown parameters as we have)"
  },
  {
    "objectID": "slides/08/slides08.html#recap",
    "href": "slides/08/slides08.html#recap",
    "title": "Evaluating Estimators",
    "section": "Recap",
    "text": "Recap\n\nObserve data \\(X_1, ..., X_n \\sim F_x(x|\\theta)\\), where \\(\\theta\\) is unknown\nGoal: Estimate \\(\\theta\\) based on the values of \\(X_i\\) by formulating an estimator \\(\\widehat \\theta\\)\nOne technique is to use the maximum likelihood estimator, which finds the value of \\(\\theta\\) that maximizes the joint probability \\(\\prod_{i=1}^n f_x(x_i | \\theta)\\)\nA second technique is to use the method of moments estimator, which finds the value of \\(\\theta\\) that make the theoretical moments equal to the sample moments\nToday: if we have multiple estimators, how do we decide which is better?"
  },
  {
    "objectID": "slides/08/slides08.html#warm-up",
    "href": "slides/08/slides08.html#warm-up",
    "title": "Evaluating Estimators",
    "section": "Warm up",
    "text": "Warm up\nSuppose we take a random sample of size 50 from an exponential distribution with rate \\(\\lambda = 1/10\\).\nWhat is \\(\\mu = E(Y)\\)?\nIf we want to design an estimator for the mean, \\(\\widehat \\mu\\), what are some intuitive estimators?"
  },
  {
    "objectID": "slides/08/slides08.html#estimators-for-the-mean",
    "href": "slides/08/slides08.html#estimators-for-the-mean",
    "title": "Evaluating Estimators",
    "section": "Estimators for the mean",
    "text": "Estimators for the mean\nSuppose we take a random sample of size 50 from an exponential distribution with mean 10 (rate is \\(\\lambda = 1/10\\)).\nConsider three estimators of \\(\\mu\\):\n\n\\(\\widehat{\\mu_1} = \\bar{X}\\)\n\\(\\widehat{\\mu_2} = \\widetilde{X}\\)\n\\(\\widehat{\\mu_3} = \\frac{X_{\\max} - X_{\\min}}{2}\\)"
  },
  {
    "objectID": "slides/08/slides08.html#aside-estimators-are-also-random-variables",
    "href": "slides/08/slides08.html#aside-estimators-are-also-random-variables",
    "title": "Evaluating Estimators",
    "section": "Aside: estimators are also random variables",
    "text": "Aside: estimators are also random variables\n\\(X_1, ..., X_n \\sim F_x(\\theta)\\) and each \\(\\widehat{\\theta} = g(X_1, ..., X_n)\\) is a function of the data\n\nThis means that each \\(\\widehat\\theta\\) is itself a random variable, and so it has:\n\na pdf \\(f_\\widehat{\\theta}\\)\nan expectation \\(E(\\widehat\\theta)\\)\na variance \\(\\text{Var}(\\widehat{\\theta})\\)"
  },
  {
    "objectID": "slides/08/slides08.html#comparing-estimators-simulation",
    "href": "slides/08/slides08.html#comparing-estimators-simulation",
    "title": "Evaluating Estimators",
    "section": "Comparing Estimators: Simulation",
    "text": "Comparing Estimators: Simulation\nSuppose we take a random sample of size 50 from an exponential distribution with mean 10 (rate is \\(\\lambda = 1/10\\)).\n\n\nConsider three estimators of \\(\\mu\\):\n\n\\(\\hat{\\mu_1} = \\bar{X}\\)\n\\(\\hat{\\mu_2} = \\tilde{X}\\)\n\\(\\hat{\\mu_3} = \\frac{X_{\\max} - X_{\\min}}{2}\\)\n\n\n\nn &lt;- 50\nN_sims &lt;- 100000\nest1 &lt;- numeric(N_sims)\nest2 &lt;- numeric(N_sims)\nest3 &lt;- numeric(N_sims)\nfor(i in 1:N_sims){\n  x &lt;- rexp(n, rate = .1)\n  est1[i] &lt;- mean(x)\n  est2[i] &lt;- median(x)\n  est3[i] &lt;- (max(x) - min(x))/2\n}"
  },
  {
    "objectID": "slides/08/slides08.html#sampling-distribution-of-the-estimators",
    "href": "slides/08/slides08.html#sampling-distribution-of-the-estimators",
    "title": "Evaluating Estimators",
    "section": "Sampling Distribution of the Estimators",
    "text": "Sampling Distribution of the Estimators\n\nwhich estimator is “best”?"
  },
  {
    "objectID": "slides/08/slides08.html#properties-of-estimators",
    "href": "slides/08/slides08.html#properties-of-estimators",
    "title": "Evaluating Estimators",
    "section": "Properties of Estimators",
    "text": "Properties of Estimators"
  },
  {
    "objectID": "slides/08/slides08.html#estimator-properties-bias",
    "href": "slides/08/slides08.html#estimator-properties-bias",
    "title": "Evaluating Estimators",
    "section": "Estimator properties: bias",
    "text": "Estimator properties: bias\nHow accurate is the estimator?\n\n\n\n\n\n\n\nBias of an estimator\n\n\nIf \\(\\widehat{\\theta}(X)\\) is an estimator of \\(\\theta\\), then the bias of the estimator is equal to\n\\[\\text{Bias}(\\widehat\\theta) = E[\\widehat \\theta(X)] - \\theta\\]\nNote: the expected value is computed from the sampling distribution of \\(\\widehat{\\theta}(X)\\)"
  },
  {
    "objectID": "slides/08/slides08.html#estimator-properties-variance",
    "href": "slides/08/slides08.html#estimator-properties-variance",
    "title": "Evaluating Estimators",
    "section": "Estimator properties: variance",
    "text": "Estimator properties: variance\nHow much variability does the estimator have around its mean?\n\n\n\n\n\n\nVariance of an estimator\n\n\nThe variance of an estimator is\n\\[\\text{Var}(\\widehat{\\theta}) = E[(\\widehat{\\theta} - E[\\widehat{\\theta}])^2]\\]\nNote: the expected value is computed from the sampling distribution of \\(\\widehat{\\theta}(X)\\)"
  },
  {
    "objectID": "slides/08/slides08.html#estimator-properties-mean-square-error-mse",
    "href": "slides/08/slides08.html#estimator-properties-mean-square-error-mse",
    "title": "Evaluating Estimators",
    "section": "Estimator properties: Mean Square Error (MSE)",
    "text": "Estimator properties: Mean Square Error (MSE)\nHow much variability does the estimator have around \\(\\theta\\)?\n\n\n\n\n\n\nMSE\n\n\nThe MSE of an estimator is\n\\[MSE(\\widehat\\theta) = E[(\\widehat\\theta - \\theta)^2] = \\text{Var}(\\widehat{\\theta}) + \\text{Bias}(\\widehat\\theta)^2\\]"
  },
  {
    "objectID": "slides/08/slides08.html#comparing-estimators-simulation-1",
    "href": "slides/08/slides08.html#comparing-estimators-simulation-1",
    "title": "Evaluating Estimators",
    "section": "Comparing Estimators: simulation",
    "text": "Comparing Estimators: simulation\nRecall that \\(E(Y) = \\mu = 1/\\lambda = 10\\) is the true value\n\nsims |&gt;\n  group_by(estimator) |&gt;\n  summarize(\n    bias = mean(value) - 10,\n    var = var(value),\n    MSE = var + bias^2\n  ) \n\n# A tibble: 3 × 4\n  estimator     bias   var    MSE\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 mean       0.00123  1.99   1.99\n2 median    -2.97     1.96  10.8 \n3 midpoint  12.4     40.7  195."
  },
  {
    "objectID": "slides/08/slides08.html#practice-unif0-theta-distribution",
    "href": "slides/08/slides08.html#practice-unif0-theta-distribution",
    "title": "Evaluating Estimators",
    "section": "Practice: Unif(0, \\(\\theta\\)) distribution1",
    "text": "Practice: Unif(0, \\(\\theta\\)) distribution1\nYour task is to compare the estimators\n\\[\\widehat{\\theta}_{MLE} = X_\\max \\hspace{1in} \\widehat{\\theta}_{MoM} = 2\\bar{X}\\]\n\n\nWhat is the bias of each estimator?2\nWhat is the SE of each estimator?\nWhat is the MSE of each estimator?\nWhen does \\(\\widehat{\\theta}_{MLE}\\) “beat” \\(\\widehat{\\theta}_{MoM}\\) in terms of MSE?\n\n\nSo \\(0 \\le x \\le \\theta\\), \\(f_x = \\frac{1}{\\theta}\\), \\(F_x = \\frac{x}{\\theta}\\), \\(E(X) = \\frac{\\theta}{2}\\), and \\(V(X) = \\frac{\\theta^2}{12}\\)A helpful fact is that \\(f_{X_{max}}(x) = n[F(x)]^{n-1}f_X(x)\\)"
  },
  {
    "objectID": "slides/08/slides08.html#section",
    "href": "slides/08/slides08.html#section",
    "title": "Evaluating Estimators",
    "section": "",
    "text": "(d) When does \\(\\widehat{\\theta}_{MLE}\\) “beat” \\(\\widehat{\\theta}_{MoM}\\) in terms of MSE?\nMSE = “MSE factor” \\(\\times \\theta^2\\)"
  },
  {
    "objectID": "slides/08/slides08.html#example-uniform0theta",
    "href": "slides/08/slides08.html#example-uniform0theta",
    "title": "Evaluating Estimators",
    "section": "Example: Uniform(\\(0,\\theta\\))",
    "text": "Example: Uniform(\\(0,\\theta\\))\n\n\nSince \\(\\hat \\theta_{MLE}\\) beats \\(\\hat \\theta_{MoM}\\) in terms of MSE but is biased, can we “unbias” the MLE? Call this third estimator \\(\\hat\\theta_3\\)\nDoes \\(\\hat \\theta_3\\) ever “beat” \\(\\hat \\theta_{MLE}\\) in terms of MSE?"
  },
  {
    "objectID": "slides/08/slides08.html#section-1",
    "href": "slides/08/slides08.html#section-1",
    "title": "Evaluating Estimators",
    "section": "",
    "text": "(f) Does \\(\\hat \\theta_3\\) ever “beat” \\(\\hat \\theta_{MLE}\\) in terms of MSE?\nMSE = “MSE factor” \\(\\times \\theta^2\\)"
  },
  {
    "objectID": "slides/08/slides08.html#comparing-unbiased-estimators-efficiency",
    "href": "slides/08/slides08.html#comparing-unbiased-estimators-efficiency",
    "title": "Evaluating Estimators",
    "section": "Comparing unbiased estimators: efficiency",
    "text": "Comparing unbiased estimators: efficiency\n\n\n\n\n\n\nEfficiency\n\n\nFor two unbiased estimators, \\(\\widehat \\theta_1\\) is more efficient than \\(\\widehat \\theta_2\\) if\n\\[\\text{Var}(\\widehat \\theta_1) &lt; \\text{Var}(\\widehat \\theta_2) \\iff \\text{SE}(\\widehat \\theta_1) &lt; \\text{SE}(\\widehat \\theta_2)\\]"
  },
  {
    "objectID": "slides/08/slides08.html#comparing-unbiased-estimators-cramer-rao-lower-bound-crlb",
    "href": "slides/08/slides08.html#comparing-unbiased-estimators-cramer-rao-lower-bound-crlb",
    "title": "Evaluating Estimators",
    "section": "Comparing unbiased estimators: Cramer-Rao Lower Bound (CRLB)",
    "text": "Comparing unbiased estimators: Cramer-Rao Lower Bound (CRLB)\n\n\n\n\n\n\n\nCRLB\n\n\nIf \\(X_1, ..., X_n\\) are an iid sample from a distribution with pdf \\(f(x|\\theta)\\), then any unbiased estimator \\(\\hat\\theta\\) of \\(\\theta\\) satisfies:\n\\[V(\\hat{\\theta}) \\ge \\frac{1}{n I(\\theta)}\\]\nwhere \\(I(\\theta)\\) is the Fisher Information of \\(X_i\\)"
  },
  {
    "objectID": "slides/08/slides08.html#fisher-information",
    "href": "slides/08/slides08.html#fisher-information",
    "title": "Evaluating Estimators",
    "section": "Fisher Information",
    "text": "Fisher Information\n\n\n\n\n\n\nFisher Information\n\n\nThe Fisher Information of a random variable \\(X\\) is \\[I(\\theta) = E[(\\frac{d}{d\\theta} \\ln f(x|\\theta))^2] = -   E(l''(\\theta))\\]\n*provided certain regularity conditions are met"
  },
  {
    "objectID": "slides/06/slides06.html#big-idea",
    "href": "slides/06/slides06.html#big-idea",
    "title": "Maximum Likelihood Estimation II",
    "section": "Big Idea",
    "text": "Big Idea\nWe’re interested in the proportion of Carleton students who have had some sort of research experience. We have the resources to randomly sample 10 students.\n\n\nProbability:\n\nStatistics:"
  },
  {
    "objectID": "slides/06/slides06.html#example",
    "href": "slides/06/slides06.html#example",
    "title": "Maximum Likelihood Estimation II",
    "section": "Example",
    "text": "Example\nRecall that the likelihood function for \\(n\\) iid Bernoulli(\\(\\theta\\)) random variable is \\(L(\\theta) = \\theta^{\\sum x_i} (1-\\theta)^{n - \\sum x_i}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nScenario 1: 0 yes responses\n\n\n\n\\(\\theta\\)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\n\n\\(L(\\theta)\\)\n\n\n\n\n\n\n\n\n\nScenario 2: 6 yes responses\n\n\n\n\\(\\theta\\)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\n\n\\(L(\\theta)\\)"
  },
  {
    "objectID": "slides/06/slides06.html#exercise",
    "href": "slides/06/slides06.html#exercise",
    "title": "Maximum Likelihood Estimation II",
    "section": "Exercise",
    "text": "Exercise\nIs the likelihood function a probability distribution? Why or why not?"
  },
  {
    "objectID": "slides/06/slides06.html#exercise-from-last-class",
    "href": "slides/06/slides06.html#exercise-from-last-class",
    "title": "Maximum Likelihood Estimation II",
    "section": "Exercise: from last class",
    "text": "Exercise: from last class\nLet \\(X_1, ..., X_n\\) be an iid random sample from a distribution with PDF\n\\[f(x|\\theta)= (\\theta + 1)x^\\theta, 0 \\le x \\le 1\\]\n\\[L(\\theta) =\\] \\[\\hat\\theta_{MLE}=\\]\nSuppose we observe a sample of size 5: {.83, .49, .72, .57, .66}. Find the maximum likelihood estimate and verify with a graph or numerical approximation"
  },
  {
    "objectID": "slides/06/slides06.html#example-uniform-disribution",
    "href": "slides/06/slides06.html#example-uniform-disribution",
    "title": "Maximum Likelihood Estimation II",
    "section": "Example: Uniform disribution",
    "text": "Example: Uniform disribution\nFind the MLE for \\(Y_1, ..., Y_n \\sim \\text{Unif}(0, \\theta)\\)"
  },
  {
    "objectID": "slides/06/slides06.html#finding-the-mle-when-more-than-one-parameter-is-unknown",
    "href": "slides/06/slides06.html#finding-the-mle-when-more-than-one-parameter-is-unknown",
    "title": "Maximum Likelihood Estimation II",
    "section": "Finding the MLE when more than one parameter is unknown",
    "text": "Finding the MLE when more than one parameter is unknown\nIf the pdf or pmf that we’re using has two or more parameters, say \\(\\theta_1\\) and \\(\\theta_2\\), finding MLEs for the \\(\\theta_i\\)’s requires the solution of a sest of simultaneous equations\n\n\\[\\frac{\\partial}{\\partial \\theta_1} \\ln L(\\theta_1, \\theta_2) = 0\\]\n\n\n\\[\\frac{\\partial}{\\partial \\theta_2} \\ln L(\\theta_1, \\theta_2) = 0\\]"
  },
  {
    "objectID": "slides/06/slides06.html#example-nmu-sigma2-distribution",
    "href": "slides/06/slides06.html#example-nmu-sigma2-distribution",
    "title": "Maximum Likelihood Estimation II",
    "section": "Example: \\(N(\\mu, \\sigma^2)\\) distribution",
    "text": "Example: \\(N(\\mu, \\sigma^2)\\) distribution\nSuppose a random sample of size \\(n\\) is drawn from the two parameter normal pdf:\n\\(f_y(y|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp(-(\\frac{y-\\mu}{\\sigma})^2)\\)\nfind the MLEs \\(\\hat{\\mu}\\) and \\(\\hat\\sigma^2\\)"
  },
  {
    "objectID": "slides/13/slides13.html#roadmap",
    "href": "slides/13/slides13.html#roadmap",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Roadmap",
    "text": "Roadmap\n\n\nUnit 1: Estimation\n\nStatistics vs Parameters\nDeveloping an estimator\nEvaluating the estimator’s behavior:\n\nsampling distribution\nbias, variance, consistency\n\n\n\nUnit 2: Inference\n\nOnce we have an estimator, what does it say about the population parameter?"
  },
  {
    "objectID": "slides/13/slides13.html#roadmap-1",
    "href": "slides/13/slides13.html#roadmap-1",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Roadmap",
    "text": "Roadmap\nObserve \\(X_1, ..., X_n \\sim F(\\theta)\\) with \\(\\theta\\) unknown. Estimate \\(\\hat\\theta = g(X_i)\\).\n\nDo we expect \\(\\hat\\theta\\) to be exactly equal to \\(\\theta\\)?\n\n\nWhat are plausible values for \\(\\theta\\) given an observed \\(\\hat\\theta\\)?"
  },
  {
    "objectID": "slides/13/slides13.html#roadmap-2",
    "href": "slides/13/slides13.html#roadmap-2",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Roadmap",
    "text": "Roadmap\nWe want to develop an interval estimate of a population parameter\n\nExact method: Find the sampling distribution in closed form (Chapter 4). Requires knowledge of the distribution of the data!\nBootstrap method: Use the sample to approximate the population and simulate a sampling distribution (Chapter 5).\nAsymptotic method: Use large-sample theory to approximate the sampling distribution (e.g., appeal to CLT; Chapter 7)"
  },
  {
    "objectID": "slides/13/slides13.html#example",
    "href": "slides/13/slides13.html#example",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Example",
    "text": "Example\n\n\n\nDr. Kristen Gorman and the Palmer Station, Antarctica LTER, are studying the bill dimensions of a certain species of penguin\nThey want to estimate the average bill depth and bill length (in mm)\n\n\ndata(\"penguins\", package = \"palmerpenguins\")\ngentoo &lt;- filter(penguins, species == \"Gentoo\")\n\n\n\n\n\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package: https://allisonhorst.github.io/palmerpenguins/"
  },
  {
    "objectID": "slides/13/slides13.html#bill-length",
    "href": "slides/13/slides13.html#bill-length",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Bill length",
    "text": "Bill length\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n40.9\n45.3\n47.3\n49.55\n59.6\n47.5\n3.08\n123\n1"
  },
  {
    "objectID": "slides/13/slides13.html#how-can-we-use-our-one-sample-to-estimate-the-population-mean-bill-length-for-all-gentoo-penguins",
    "href": "slides/13/slides13.html#how-can-we-use-our-one-sample-to-estimate-the-population-mean-bill-length-for-all-gentoo-penguins",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "How can we use our one sample to estimate the population mean bill length for all Gentoo penguins?",
    "text": "How can we use our one sample to estimate the population mean bill length for all Gentoo penguins?\n\n(Theoretical) sampling distribution\nBootstrap distribution"
  },
  {
    "objectID": "slides/13/slides13.html#bootstrap-distribution",
    "href": "slides/13/slides13.html#bootstrap-distribution",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Bootstrap distribution",
    "text": "Bootstrap distribution"
  },
  {
    "objectID": "slides/13/slides13.html#bootstrap-percentile-interval",
    "href": "slides/13/slides13.html#bootstrap-percentile-interval",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Bootstrap percentile interval",
    "text": "Bootstrap percentile interval\nA 95% confidence interval can be constructed from the 2.5 and 97.5th percentiles of the bootstrap distribution"
  },
  {
    "objectID": "slides/13/slides13.html#the-one-sample-bootstrap-algorithm",
    "href": "slides/13/slides13.html#the-one-sample-bootstrap-algorithm",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "The one-sample bootstrap algorithm",
    "text": "The one-sample bootstrap algorithm\nGiven a sample of size n from a population,\n\nDraw a resample of size n, with replacement, from the sample.\nCompute the statistic of interest.\nRepeat this resampling process (steps 1-2) many times, say 10,000.\nConstruct the bootstrap distribution of the statistic."
  },
  {
    "objectID": "slides/13/slides13.html#your-turn",
    "href": "slides/13/slides13.html#your-turn",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Your turn",
    "text": "Your turn\nA sample consists of the following values: 8, 4, 11, 3, 7.\nWhich of the following are possible bootstrap samples from this sample? Why?\n\n8, 3, 7, 11\n4, 11, 4, 3, 3\n3, 4, 5, 7, 8\n7, 8, 8, 3, 4"
  },
  {
    "objectID": "slides/13/slides13.html#population",
    "href": "slides/13/slides13.html#population",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Population",
    "text": "Population\nConsider a Gamma(2, 2) population distribution\n\n\\[E(X) = 1 \\qquad SD(X) = 1/2\\]"
  },
  {
    "objectID": "slides/13/slides13.html#sample",
    "href": "slides/13/slides13.html#sample",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Sample",
    "text": "Sample\nSuppose we draw a random sample of size \\(n=50\\)\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\n\n\n\n\n0.061\n0.445\n0.757\n1.065\n5.088\n0.925\n0.824\n50.000"
  },
  {
    "objectID": "slides/13/slides13.html#bootstrap-distribution-1",
    "href": "slides/13/slides13.html#bootstrap-distribution-1",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Bootstrap distribution",
    "text": "Bootstrap distribution\nWe can bootstrap our sample and obtain the bootstrap distribution\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\n\n\n\n\n0.590\n0.845\n0.918\n0.998\n1.481\n0.926\n0.115\n10,000.000"
  },
  {
    "objectID": "slides/13/slides13.html#sampling-distribution",
    "href": "slides/13/slides13.html#sampling-distribution",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nWe could also draw many different samples and obtain the sampling distribution\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\n\n\n\n\n0.632\n0.933\n0.997\n1.065\n1.443\n1.001\n0.100\n10,000.000"
  },
  {
    "objectID": "slides/13/slides13.html#key-comparisons",
    "href": "slides/13/slides13.html#key-comparisons",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Key comparisons",
    "text": "Key comparisons\n\n\n\n\n\n\n\n\n\n\nMean\nSD\nBias\n\n\n\n\nPopulation\n1\n0.5\n\n\n\nSample\n0.925\n0.925\n\n\n\nSampling distribution\n1.001\n0.1\n0.001\n\n\nBootstrap distribution\n0.926\n0.115\n0.001"
  },
  {
    "objectID": "slides/13/slides13.html#implementation",
    "href": "slides/13/slides13.html#implementation",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": " implementation",
    "text": "implementation\n\n\n# Subsetting to get only one species\ngentoo &lt;- dplyr::filter(penguins, species == \"Gentoo\")\n\n# Bookkeeping\ny &lt;- gentoo$bill_length_mm\nn &lt;- nrow(gentoo)        # sample size\nN &lt;- 10^4                # desired no. resamples\nboot_means &lt;- numeric(N) # a place to store the bootstrap stats\n\n# Resampling from the sample\nfor (i in 1:N) {\n  x &lt;- sample(y, size = n, replace = TRUE)\n  boot_means[i] &lt;- mean(x, na.rm = TRUE)  # you can choose other statistics\n}\n# Calculate a 95% percentile interval\nquantile(boot_means, probs = c(0.025, 0.975))"
  },
  {
    "objectID": "slides/13/slides13.html#section",
    "href": "slides/13/slides13.html#section",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "",
    "text": "First, recall the definition of the CDF:\n\\[F_x(x_0) = P(X \\le x_0)\\]\n\nIn other words, \\(F_x\\) is the probability of the event \\(\\{X \\le x_0\\}\\). If we observe a sample of \\(X_1, ..., X_n \\sim F_x\\), a natural estimator for this probability is the observed proportion of observations where \\(\\{X_i \\le x_0\\}\\).\n\n\n\\[\\widehat F_n = \\frac{\\sum \\mathbb{I}(X_i \\le x_0)}{n}\\] so \\(\\widehat{F}_n\\) is an estimator for \\(F\\)."
  },
  {
    "objectID": "slides/13/slides13.html#how-does-widehat-f-relate-to-the-bootstrap",
    "href": "slides/13/slides13.html#how-does-widehat-f-relate-to-the-bootstrap",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "How does \\(\\widehat F\\) relate to the bootstrap?",
    "text": "How does \\(\\widehat F\\) relate to the bootstrap?"
  },
  {
    "objectID": "slides/13/slides13.html#how-does-widehat-f-relate-to-the-bootstrap-1",
    "href": "slides/13/slides13.html#how-does-widehat-f-relate-to-the-bootstrap-1",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "How does \\(\\widehat F\\) relate to the bootstrap?",
    "text": "How does \\(\\widehat F\\) relate to the bootstrap?\nEach bootstrap sample is drawn from \\(\\widehat F\\):\n\\[\nX_1^{*(1)}, X_2^{*(1)}, ... X_n^{*(1)} \\sim \\widehat F_n\n\\]\n\\[\nX_1^{*(2)}, X_2^{*(2)}, ... X_n^{*(2)} \\sim \\widehat F_n\n\\]\n\\[\nX_1^{*(3)}, X_2^{*(3)}, ... X_n^{*(3)} \\sim \\widehat F_n\n\\]\n\\[\\vdots\\]\n\\[\nX_1^{*(B)}, X_2^{*(B)}, ... X_n^{*(B} \\sim \\widehat F_n\n\\]"
  },
  {
    "objectID": "slides/13/slides13.html#as-n-increases-widehatf_n-gets-closer-to-true-f",
    "href": "slides/13/slides13.html#as-n-increases-widehatf_n-gets-closer-to-true-f",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "As n increases, \\(\\widehat{F}_n\\) gets closer to true \\(F\\)",
    "text": "As n increases, \\(\\widehat{F}_n\\) gets closer to true \\(F\\)"
  },
  {
    "objectID": "slides/13/slides13.html#section-1",
    "href": "slides/13/slides13.html#section-1",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "",
    "text": "It turns out that \\(\\widehat{F}_n\\) is an unbiased and consistent estimator for \\(F\\)!\n\nWhen \\(n\\) is large, \\(\\widehat{F}_n\\) is very close to \\(F\\)\nSo any statistic that is based on \\(\\widehat{F}_n\\) is very similar to the same statistic based on \\(F\\)\nRe-sampling from our original sample results in a sampling distribution that is very similar to the theoretical sampling distribution\nThis is true even if we don’t know what the theoretical sampling distribution is!"
  },
  {
    "objectID": "slides/13/slides13.html#your-turn-theoretical-example",
    "href": "slides/13/slides13.html#your-turn-theoretical-example",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Your turn: theoretical example",
    "text": "Your turn: theoretical example\nConsider a population that has a gamma distribution with parameters r=5, 𝜆=1∕4.\n\n\nUse simulation (with n = 200) to generate an approximate sampling distribution of the mean; plot and describe the distribution.\nNow, draw one random sample of size 200 from this population. Create a histogram of your sample and find the mean and standard deviation.\nCompute the bootstrap distribution of the mean for your sample, plot it, and note the bootstrap mean and standard error.\nCompare the bootstrap distribution to the approximate theoretical sampling distribution by creating a table like slide 17\n\n\nRepeat (a)–(d) for sample sizes of n = 50 and n = 10. Describe carefully your observations about the effects of sample size on the bootstrap distribution."
  },
  {
    "objectID": "slides/13/slides13.html#your-turn-data-example",
    "href": "slides/13/slides13.html#your-turn-data-example",
    "title": "Intro to Confidence Intervals & the bootstrap",
    "section": "Your turn: data example",
    "text": "Your turn: data example\nThe Bangladesh data set contains information about arsenic, cobalt, and chlorine concentrations from a sample of 271 water wells in Bangladesh.\n\nlibrary(resampledata3)\nlibrary(tidyverse)\n\n\n\nConduct EDA on the chlorine concentrations and describe the salient features.\nFind the bootstrap distribution of the mean.\nFind and interpret the 95% bootstrap percentile confidence interval.\nWhat is the bootstrap estimate of the bias? What fraction of the bootstrap standard error does it represent?"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html",
    "href": "notes/10-asymptotics/activity10-sols.html",
    "title": "10: Asymptotic Properties",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html#example-1",
    "href": "notes/10-asymptotics/activity10-sols.html#example-1",
    "title": "10: Asymptotic Properties",
    "section": "Example 1:",
    "text": "Example 1:\nLet \\(Y_1, ..., Y_n\\) be \\(n\\) Exp(\\(\\lambda\\)) random variables Let \\(\\widehat\\lambda = \\frac{n}{\\sum Y_i}\\). How does Var(\\(\\widehat\\lambda\\)) compare with the CRLB?\n\nlambda &lt;- .5\nn &lt;- 20\nn_sims &lt;- 10000\nlambda_hat &lt;- numeric(n_sims)\n\nfor(i in 1:n_sims){\n  sample &lt;- rexp(n, rate = lambda)\n  lambda_hat[i] &lt;- n/sum(sample)\n}\n\nvar(lambda_hat)\n\n[1] 0.0154848\n\nlambda^2/n\n\n[1] 0.0125"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html#example-1-1",
    "href": "notes/10-asymptotics/activity10-sols.html#example-1-1",
    "title": "10: Asymptotic Properties",
    "section": "Example 1:",
    "text": "Example 1:\nExample: The median \\(m\\) of an Exponential(\\(\\lambda\\)) distribution satisfies P(X ≤ m) = 0.5. Solving \\(1 - e^{-\\lambda m} = 0.5\\) gives \\(m = ln(2) / \\lambda\\). This suggests an estimator for \\(\\lambda\\) based on the median: \\(\\widehat{\\lambda} = \\frac{ln(2)}{m}\\). Finding the analytical variance of \\(\\widehat \\lambda\\) is complicated. Finding the sampling distribution of \\(m\\) is complicated, and finding the non-linear transformation is also complicated.\nUse simulation to see whether \\(\\frac{ln(2)}{m}\\) (a) is unbiased and (b) achieves the CRLB\n\nlambda &lt;- .5\nn &lt;- 200\nn_sims &lt;- 10000\nlambda_hat &lt;- numeric(n_sims)\n\nfor(i in 1:n_sims){\n  sample &lt;- rexp(n, rate = lambda)\n  lambda_hat[i] &lt;- log(2)/median(sample)\n}\n\nmean(lambda_hat)\n\n[1] 0.5023425\n\nlambda\n\n[1] 0.5\n\nvar(lambda_hat)\n\n[1] 0.002668319\n\nlambda^2/n\n\n[1] 0.00125"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html#running-means",
    "href": "notes/10-asymptotics/activity10-sols.html#running-means",
    "title": "10: Asymptotic Properties",
    "section": "Running Means",
    "text": "Running Means\nThe code below creates the “running mean” graph from the textbook for the Cauchy distribution. In your groups, talk through the steps and ask if you have questions. (It’s a little different than other simulations that we’ve seen!) Run the code chunk a couple of times to get a sense of the behavior of the mean of a Cauchy distribution.\n\nn &lt;- 10000\nrunning_mean &lt;- numeric(n)\nsample &lt;- rcauchy(n)\n\nfor(i in 1:n){\n  running_mean[i] &lt;- mean(sample[1:i])\n}\n\nggplot() + \n  geom_line(aes(x = 1:n, y = running_mean)) \n\n\n\n\n\n\n\n\nNow, use the code chunk below to do the same “running mean” example for a Normal(0,1) distribution. What do you notice? How is it the same/different from the Cauchy example above?\n\nn &lt;- 10000\nrunning_mean &lt;- numeric(n)\nsample &lt;- rnorm(n)\n\nfor(i in 1:n){\n  running_mean[i] &lt;- mean(sample[1:i])\n}\n\nggplot() + \n  geom_line(aes(x = 1:n, y = running_mean))"
  },
  {
    "objectID": "notes/10-asymptotics/activity10-sols.html#variance-of-sample-means",
    "href": "notes/10-asymptotics/activity10-sols.html#variance-of-sample-means",
    "title": "10: Asymptotic Properties",
    "section": "Variance of sample means",
    "text": "Variance of sample means\nThe code chunk below runs a simulation comparing the variance of the sample mean of a Normal(0,1) distribution to the variance of the sample mean for a Cauchy distribution. Run it a few times to get a sense of the behavior, then try larger values of n. What do you notice? What does this mean about the consistency of the estimators?\n\nn &lt;- 20\nn_sims &lt;- 1000\nsample_mean_norm &lt;- numeric(n)\nsample_mean_cauchy &lt;- numeric(n)\n\nfor(i in 1:n_sims){\n  samp1 &lt;- rnorm(n)\n  samp2 &lt;- rcauchy(n)\n  sample_mean_norm[i] = mean(samp1)\n  sample_mean_cauchy[i] = mean(samp2)\n}\n\nvar(sample_mean_norm)\n\n[1] 0.05207371\n\nvar(sample_mean_cauchy)\n\n[1] 297.7243"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html",
    "href": "notes/10-asymptotics/activity10.html",
    "title": "10: Asymptotic Properties",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html#example-1",
    "href": "notes/10-asymptotics/activity10.html#example-1",
    "title": "10: Asymptotic Properties",
    "section": "Example 1:",
    "text": "Example 1:\nLet \\(Y_1, ..., Y_n\\) be \\(n\\) Exp(\\(\\lambda\\)) random variables Let \\(\\widehat\\lambda = \\frac{n}{\\sum Y_i}\\). How does Var(\\(\\widehat\\lambda\\)) compare with the CRLB?\n\nlambda &lt;- .5\nn &lt;- 20\nn_sims &lt;- 10000\nlambda_hat &lt;- numeric(n_sims)\n\nfor(i in 1:n_sims){\n  sample &lt;- rexp(n, rate = lambda)\n  lambda_hat[i] &lt;- n/sum(sample)\n}\n\nvar(lambda_hat)\n\n[1] 0.01488985\n\nlambda^2/n\n\n[1] 0.0125"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html#example-1-1",
    "href": "notes/10-asymptotics/activity10.html#example-1-1",
    "title": "10: Asymptotic Properties",
    "section": "Example 1:",
    "text": "Example 1:\nExample: The median \\(m\\) of an Exponential(\\(\\lambda\\)) distribution satisfies P(X ≤ m) = 0.5. Solving \\(1 - e^{-\\lambda m} = 0.5\\) gives \\(m = ln(2) / \\lambda\\). This suggests an estimator for \\(\\lambda\\) based on the median: \\(\\widehat{\\lambda} = \\frac{ln(2)}{m}\\). Finding the analytical variance of \\(\\widehat \\lambda\\) is complicated. Finding the sampling distribution of \\(m\\) is complicated, and finding the non-linear transformation is also complicated.\nUse simulation to see whether \\(\\frac{ln(2)}{m}\\) (a) is unbiased and (b) achieves the CRLB\n\n# your code here"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html#running-means",
    "href": "notes/10-asymptotics/activity10.html#running-means",
    "title": "10: Asymptotic Properties",
    "section": "Running Means",
    "text": "Running Means\nThe code below creates the “running mean” graph from the textbook for the Cauchy distribution. In your groups, talk through the steps and ask if you have questions. (It’s a little different than other simulations that we’ve seen!) Run the code chunk a couple of times to get a sense of the behavior of the mean of a Cauchy distribution.\n\nn &lt;- 10000\nrunning_mean &lt;- numeric(n)\nsample &lt;- rcauchy(n)\n\nfor(i in 1:n){\n  running_mean[i] &lt;- mean(sample[1:i])\n}\n\nggplot() + \n  geom_line(aes(x = 1:n, y = running_mean)) \n\n\n\n\n\n\n\n\nNow, use the code chunk below to do the same “running mean” example for a Normal(0,1) distribution. What do you notice? How is it the same/different from the Cauchy example above?\n\n# your code here"
  },
  {
    "objectID": "notes/10-asymptotics/activity10.html#variance-of-sample-means",
    "href": "notes/10-asymptotics/activity10.html#variance-of-sample-means",
    "title": "10: Asymptotic Properties",
    "section": "Variance of sample means",
    "text": "Variance of sample means\nThe code chunk below runs a simulation comparing the variance of the sample mean of a Normal(0,1) distribution to the variance of the sample mean for a Cauchy distribution. Run it a few times to get a sense of the behavior, then try larger values of n. What do you notice? What does this mean about the consistency of the estimators?\n\nn &lt;- 20\nn_sims &lt;- 1000\nsample_mean_norm &lt;- numeric(n)\nsample_mean_cauchy &lt;- numeric(n)\n\nfor(i in 1:n_sims){\n  samp1 &lt;- rnorm(n)\n  samp2 &lt;- rcauchy(n)\n  sample_mean_norm[i] = mean(samp1)\n  sample_mean_cauchy[i] = mean(samp2)\n}\n\nvar(sample_mean_norm)\n\n[1] 0.05104321\n\nvar(sample_mean_cauchy)\n\n[1] 939.2958"
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html",
    "href": "notes/13-intro-ci/activity13.html",
    "title": "13: Bootstrap",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)"
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html#theoretical-example",
    "href": "notes/13-intro-ci/activity13.html#theoretical-example",
    "title": "13: Bootstrap",
    "section": "Theoretical example",
    "text": "Theoretical example\n\nset.seed(1234)\nsample_data &lt;- rgamma(50, 2, 2)\nmean(sample_data)\n\n[1] 0.9245653\n\nsd(sample_data)\n\n[1] 0.8237878\n\n\n\nBootstrap distribution\n\nn_sims &lt;- 10^5\nboot_dsn &lt;- numeric(n_sims)\nfor(i in 1:n_sims){\n  boot_dsn[i] &lt;- mean(sample(sample_data, length(sample_data), replace = TRUE))\n}\n\nmean(boot_dsn)\n\n[1] 0.924705\n\nsd(boot_dsn)\n\n[1] 0.1157962\n\n\n\n\nSampling distribution\n\nn_sims &lt;- 10^5\nsampling_dsn &lt;- numeric(n_sims)\nfor(i in 1:n_sims){\n  sampling_dsn[i] &lt;- mean(rgamma(50, 2, 2))\n}\n\nmean(sampling_dsn)\n\n[1] 0.9998301\n\nsd(sampling_dsn)\n\n[1] 0.09989004"
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html#data-example",
    "href": "notes/13-intro-ci/activity13.html#data-example",
    "title": "13: Bootstrap",
    "section": "Data example",
    "text": "Data example\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\n# Subsetting to get only one species\ngentoo &lt;- dplyr::filter(penguins, species == \"Gentoo\")\n\n# Bookkeeping\ny &lt;- gentoo$bill_length_mm\nn &lt;- nrow(gentoo)        # sample size\nN &lt;- 10^4                # desired no. resamples\nboot_means &lt;- numeric(N) # a place to store the bootstrap stats\n\n# Resampling from the sample\nfor (i in 1:N) {\n  x &lt;- sample(y, size = n, replace = TRUE)\n  boot_means[i] &lt;- mean(x, na.rm = TRUE)  # you can choose other statistics\n}\n# Calculate a 95% percentile interval\nquantile(boot_means, probs = c(0.025, 0.975))"
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html#your-turn-theoretical-example",
    "href": "notes/13-intro-ci/activity13.html#your-turn-theoretical-example",
    "title": "13: Bootstrap",
    "section": "Your turn: theoretical example",
    "text": "Your turn: theoretical example\nConsider a population that has a gamma distribution with parameters r=5, 𝜆=1∕4.\n\nUse simulation (with n = 200) to generate an approximate sampling distribution of the mean; plot and describe the distribution.\nNow, draw one random sample of size 200 from this population. Create a histogram of your sample and find the mean and standard deviation.\nCompute the bootstrap distribution of the mean for your sample, plot it, and note the bootstrap mean and standard error.\nCompare the bootstrap distribution to the approximate theoretical sampling distribution by creating a table like slide 17\n\nRepeat (a)–(e) for sample sizes of n = 50 and n = 10. Describe carefully your observations about the effects of sample size on the bootstrap distribution."
  },
  {
    "objectID": "notes/13-intro-ci/activity13.html#your-turn-data-example",
    "href": "notes/13-intro-ci/activity13.html#your-turn-data-example",
    "title": "13: Bootstrap",
    "section": "Your turn: data example",
    "text": "Your turn: data example\nThe Bangladesh data set contains information about arsenic, cobalt, and chlorine concentrations from a sample of 271 water wells in Bangladesh.\n\nlibrary(resampledata3)\nlibrary(tidyverse)\n\n\nConduct EDA on the chlorine concentrations and describe the salient features.\nFind the bootstrap distribution of the mean.\nFind and interpret the 95% bootstrap percentile confidence interval.\nWhat is the bootstrap estimate of the bias? What fraction of the bootstrap standard error does it represent?"
  },
  {
    "objectID": "notes/02-permutation/activity02.html",
    "href": "notes/02-permutation/activity02.html",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "The data for this example lives in the {Sleuth3} R package. This chunk of code loads in the data package, and the {tidyverse} package, which provides data wrangling and visualization code.\n\nlibrary(Sleuth3)\nlibrary(tidyverse)\n\nThe dataset is called case0101. We can print the first 10 rows of the dataset with the following:\n\ncase0101 |&gt;\n  slice_head(n = 10)\n\n   Score Treatment\n1    5.0 Extrinsic\n2    5.4 Extrinsic\n3    6.1 Extrinsic\n4   10.9 Extrinsic\n5   11.8 Extrinsic\n6   12.0 Extrinsic\n7   12.3 Extrinsic\n8   14.8 Extrinsic\n9   15.0 Extrinsic\n10  16.8 Extrinsic\n\n\nThe |&gt; is called a “pipe”, and tells R to take the output of the first line of code and “pipe” it into the second.\n\n\n\nThe next thing we should do is Eploratory Data Analysis, or EDA. In this class, that typically means (1) create a visualization and (2) compute summary statistics.\nWe’ll make the side-by-side boxplots from the slides:\n\ncase0101 |&gt;\n  ggplot(aes(x = Score, y = Treatment, fill = Treatment)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nYou can also use the {esquisse} package for this class.\nTo compute summary statistics, we can either use summary():\n\nsummary(case0101)\n\n     Score           Treatment \n Min.   : 5.00   Extrinsic:23  \n 1st Qu.:14.90   Intrinsic:24  \n Median :18.70                 \n Mean   :17.86                 \n 3rd Qu.:21.25                 \n Max.   :29.70                 \n\n\nor the favstats() function from the {mosaic} package. This function uses formula syntax, which says to group “Score” based on the “Treatment” variable.\n\nlibrary(mosaic) \nfavstats(Score ~ Treatment, data = case0101)\n\n  Treatment min     Q1 median    Q3  max     mean       sd  n missing\n1 Extrinsic   5 12.150   17.2 18.95 24.0 15.73913 5.252596 23       0\n2 Intrinsic  12 17.425   20.4 22.30 29.7 19.88333 4.439513 24       0\n\n\n\n\n\nWe’ll follow the code in the book to create two vectors, one for the “Extrinsic” group and one for the “Intrinsic” group. We can check that we’ve done this correctly using summary() and comparing it to the results from favstats()\n\nScore &lt;- case0101 |&gt;\n  pull(Score)\n\nScore_Extrinsic &lt;- case0101 |&gt;\n  filter(Treatment == \"Extrinsic\") |&gt;\n  pull(Score)\n\nsummary(Score_Extrinsic)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.00   12.15   17.20   15.74   18.95   24.00 \n\n\n\nScore_Intrinsic &lt;- case0101 |&gt;\n  filter(Treatment == \"Intrinsic\") |&gt;\n  pull(Score)\n\nsummary(Score_Intrinsic)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   17.43   20.40   19.88   22.30   29.70 \n\n\nSince our test statistic is the difference between the two means, we will compute the difference in means between Score_Extrinsic and Score_Intrinsic and save it to a vector called observed\n\nobserved &lt;- mean(Score_Intrinsic) - mean(Score_Extrinsic)\nobserved\n\n[1] 4.144203\n\n\n\n\n\nTo conduct the permutation test, we want to do a large number of permutations (called N in the code chunk below). The code in the for loop is run N times. Each time, we randomly sample observations in our dataset and assign them to the “Intrinsic” group. The observations that were not sampled are assigned to the “Extrinsic” group. In each simulation, we compute the difference in the means between the two groups and save it to our result vector.\n\nN &lt;- 10^4 - 1 # Number of permutations to do\nsample_size &lt;- nrow(case0101) # Sample size for each permutation (same as data)\n\nresult &lt;- numeric(N) # Create an empty vector to store results\nfor (i in 1:N){\n  index &lt;- sample(sample_size, 24, replace = FALSE) # Sample indices for group 1\n  result[i] &lt;- mean(Score[index]) - mean(Score[-index]) # Compute differences between groups\n}\n\n\n\n\nWe can visualize the results of the permutation distribution using {ggplot2}. I’ve also overlayed a red line that shows what our observed test statistic was.\n\nggplot() + \n  geom_histogram(aes(x = result), col = \"white\") + \n  geom_vline(xintercept = observed, linetype = \"dashed\", col = \"darkred\")\n\n\n\n\n\n\n\n\n\n\n\nThe following code chunk computes the fraction of simulations which resulted in a test statistic that was larger than the observed difference:\n\n(sum(result &gt;= observed) + 1)/(N+1)\n\n[1] 0.003"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#load-data",
    "href": "notes/02-permutation/activity02.html#load-data",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "The data for this example lives in the {Sleuth3} R package. This chunk of code loads in the data package, and the {tidyverse} package, which provides data wrangling and visualization code.\n\nlibrary(Sleuth3)\nlibrary(tidyverse)\n\nThe dataset is called case0101. We can print the first 10 rows of the dataset with the following:\n\ncase0101 |&gt;\n  slice_head(n = 10)\n\n   Score Treatment\n1    5.0 Extrinsic\n2    5.4 Extrinsic\n3    6.1 Extrinsic\n4   10.9 Extrinsic\n5   11.8 Extrinsic\n6   12.0 Extrinsic\n7   12.3 Extrinsic\n8   14.8 Extrinsic\n9   15.0 Extrinsic\n10  16.8 Extrinsic\n\n\nThe |&gt; is called a “pipe”, and tells R to take the output of the first line of code and “pipe” it into the second."
  },
  {
    "objectID": "notes/02-permutation/activity02.html#eda",
    "href": "notes/02-permutation/activity02.html#eda",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "The next thing we should do is Eploratory Data Analysis, or EDA. In this class, that typically means (1) create a visualization and (2) compute summary statistics.\nWe’ll make the side-by-side boxplots from the slides:\n\ncase0101 |&gt;\n  ggplot(aes(x = Score, y = Treatment, fill = Treatment)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nYou can also use the {esquisse} package for this class.\nTo compute summary statistics, we can either use summary():\n\nsummary(case0101)\n\n     Score           Treatment \n Min.   : 5.00   Extrinsic:23  \n 1st Qu.:14.90   Intrinsic:24  \n Median :18.70                 \n Mean   :17.86                 \n 3rd Qu.:21.25                 \n Max.   :29.70                 \n\n\nor the favstats() function from the {mosaic} package. This function uses formula syntax, which says to group “Score” based on the “Treatment” variable.\n\nlibrary(mosaic) \nfavstats(Score ~ Treatment, data = case0101)\n\n  Treatment min     Q1 median    Q3  max     mean       sd  n missing\n1 Extrinsic   5 12.150   17.2 18.95 24.0 15.73913 5.252596 23       0\n2 Intrinsic  12 17.425   20.4 22.30 29.7 19.88333 4.439513 24       0"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#compute-test-statistic",
    "href": "notes/02-permutation/activity02.html#compute-test-statistic",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "We’ll follow the code in the book to create two vectors, one for the “Extrinsic” group and one for the “Intrinsic” group. We can check that we’ve done this correctly using summary() and comparing it to the results from favstats()\n\nScore &lt;- case0101 |&gt;\n  pull(Score)\n\nScore_Extrinsic &lt;- case0101 |&gt;\n  filter(Treatment == \"Extrinsic\") |&gt;\n  pull(Score)\n\nsummary(Score_Extrinsic)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.00   12.15   17.20   15.74   18.95   24.00 \n\n\n\nScore_Intrinsic &lt;- case0101 |&gt;\n  filter(Treatment == \"Intrinsic\") |&gt;\n  pull(Score)\n\nsummary(Score_Intrinsic)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   17.43   20.40   19.88   22.30   29.70 \n\n\nSince our test statistic is the difference between the two means, we will compute the difference in means between Score_Extrinsic and Score_Intrinsic and save it to a vector called observed\n\nobserved &lt;- mean(Score_Intrinsic) - mean(Score_Extrinsic)\nobserved\n\n[1] 4.144203"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#conduct-permutations",
    "href": "notes/02-permutation/activity02.html#conduct-permutations",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "To conduct the permutation test, we want to do a large number of permutations (called N in the code chunk below). The code in the for loop is run N times. Each time, we randomly sample observations in our dataset and assign them to the “Intrinsic” group. The observations that were not sampled are assigned to the “Extrinsic” group. In each simulation, we compute the difference in the means between the two groups and save it to our result vector.\n\nN &lt;- 10^4 - 1 # Number of permutations to do\nsample_size &lt;- nrow(case0101) # Sample size for each permutation (same as data)\n\nresult &lt;- numeric(N) # Create an empty vector to store results\nfor (i in 1:N){\n  index &lt;- sample(sample_size, 24, replace = FALSE) # Sample indices for group 1\n  result[i] &lt;- mean(Score[index]) - mean(Score[-index]) # Compute differences between groups\n}"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#make-plot-of-test-statistics",
    "href": "notes/02-permutation/activity02.html#make-plot-of-test-statistics",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "We can visualize the results of the permutation distribution using {ggplot2}. I’ve also overlayed a red line that shows what our observed test statistic was.\n\nggplot() + \n  geom_histogram(aes(x = result), col = \"white\") + \n  geom_vline(xintercept = observed, linetype = \"dashed\", col = \"darkred\")"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#compute-p-value",
    "href": "notes/02-permutation/activity02.html#compute-p-value",
    "title": "02: Permutation Tests",
    "section": "",
    "text": "The following code chunk computes the fraction of simulations which resulted in a test statistic that was larger than the observed difference:\n\n(sum(result &gt;= observed) + 1)/(N+1)\n\n[1] 0.003"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#eda-1",
    "href": "notes/02-permutation/activity02.html#eda-1",
    "title": "02: Permutation Tests",
    "section": "EDA",
    "text": "EDA\nCreate an appropriate EDA\n\n# your graph code here\n\n\n# your summary code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#test-statistic",
    "href": "notes/02-permutation/activity02.html#test-statistic",
    "title": "02: Permutation Tests",
    "section": "Test statistic",
    "text": "Test statistic\nCompute the appropriate test statistic\n\n# your code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#conduct-permutations-1",
    "href": "notes/02-permutation/activity02.html#conduct-permutations-1",
    "title": "02: Permutation Tests",
    "section": "Conduct Permutations",
    "text": "Conduct Permutations\n\n# your permutation code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#make-plot-of-test-statistics-1",
    "href": "notes/02-permutation/activity02.html#make-plot-of-test-statistics-1",
    "title": "02: Permutation Tests",
    "section": "Make plot of test statistics",
    "text": "Make plot of test statistics\n\n# your code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#compute-p-value-1",
    "href": "notes/02-permutation/activity02.html#compute-p-value-1",
    "title": "02: Permutation Tests",
    "section": "Compute p-value",
    "text": "Compute p-value\n\n# your code here"
  },
  {
    "objectID": "notes/02-permutation/activity02.html#what-do-you-conclude",
    "href": "notes/02-permutation/activity02.html#what-do-you-conclude",
    "title": "02: Permutation Tests",
    "section": "What do you conclude?",
    "text": "What do you conclude?"
  },
  {
    "objectID": "readings/r-homework-stat250.html",
    "href": "readings/r-homework-stat250.html",
    "title": "Using R for Homework in Stat250",
    "section": "",
    "text": "In Stat250, I will distribute homework assignment templates in either .rmd or .qmd format."
  },
  {
    "objectID": "readings/r-homework-stat250.html#uploading-files-to-maize",
    "href": "readings/r-homework-stat250.html#uploading-files-to-maize",
    "title": "Using R for Homework in Stat250",
    "section": "Uploading files to maize",
    "text": "Uploading files to maize\n\nClick the “upload” button in the files pane\nSelect the file from your local computer (likely in your Downloads folder if you downloaded from moodle)\nChoose the “Target Directory” – this is the folder on your maize account where the file will be saved. I recommend creating a “Stat250” folder for this class"
  },
  {
    "objectID": "readings/r-homework-stat250.html#accessing-maize-from-off-campus",
    "href": "readings/r-homework-stat250.html#accessing-maize-from-off-campus",
    "title": "Using R for Homework in Stat250",
    "section": "Accessing maize from off campus",
    "text": "Accessing maize from off campus\nIf you are using the maize server, you should be able to access it on campus with only your Carleton ID and password. If you plan to use the maize server and you plan to do any work off campus this term (e.g., while on a field trip, travel for athletics, or just sitting in Little Joy) you need to install Carleton’s VPN to have access.\nTo install the GlobalProtect VPN follow directions provided by ITS."
  },
  {
    "objectID": "readings/r-homework-stat250.html#installing-latex-not-needed-if-you-are-using-the-maize-server",
    "href": "readings/r-homework-stat250.html#installing-latex-not-needed-if-you-are-using-the-maize-server",
    "title": "Using R for Homework in Stat250",
    "section": "Installing LaTeX (not needed if you are using the maize server)",
    "text": "Installing LaTeX (not needed if you are using the maize server)\nIf you don’t already have a tex package installed on your computer, the easiest option to create pdf’s is to use the tinytex R package. This can be installed with the following R commands:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()  # install TinyTeX\n\nIf you’d like a standalone LaTeX package that will work with programs other than RStudio, you could install the basic installations of either:\nIf you’d like a stand alone LaTeX package, you could install the basic installations of either:\n\nMacTeX for Mac (3.2GB!)\nMiKTeX for Windows (190MB)"
  }
]