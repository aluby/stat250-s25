---
title: "Maximum Likelihood Estimation"
subtitle: "Day 05"
title-slide-attributes:
  data-background-gradient: "linear-gradient(to right, #46337e, #440154)"
  data-slide-number: none
author: "Prof Amanda Luby"
format: 
  revealjs:
    incremental: true
---

```{r setup}
#| include: false

library(tidyverse)
library(countdown)

library(ggformula)
library(openintro)
library(patchwork)
library(fivethirtyeight)
library(dplyr)
library(tidyr)
library(fontawesome)

library(gridExtra)
library(tidyverse)
library(knitr)
library(mosaic)
library(infer)
library(kableExtra)

library(plotly)

# #440154
knitr::opts_chunk$set(echo = TRUE,
                  message = FALSE,
                  warning = FALSE)

slides_theme = theme_minimal(base_family = "serif") +
  theme(plot.background = element_rect(fill = "#f0f1eb", colour = NA))
  

theme_set(slides_theme)
```

# Warm up: daily prep example

## 

You are testing seeds from a new plant variety. You plant 10 seeds (n=10) and observe that 3 of them successfully germinate (k=3). Consider two hypotheses about the true germination probability (p) for this variety:

. . . 

Hypothesis A: $p = 0.1$ (10% germination rate)

Hypothesis B: $p = 0.4$ (40% germination rate)

. . . 

Given that you observed 3 germinations out of 10 seeds, which hypothesis (A or B) is more supported by the data?

```{r}
#| echo: false
countdown::countdown(2)
```

## Evaluating Intuitively

## Evaluating Formally: Problem Set Up

- Let $X$ be the number of plants that successfully germinate
- $X \sim \text{Binom}(10, p)$ and we observed $X=3$

. . . 

:::: columns
::: {.column width="50%"}
$P(X=3)$ if $p=.1$

:::

::: {.column .fragment width="50%"}
$P(X=3)$ if $p=.4$

:::
::::

## 

In general, the *likelihood* of seeing $X=3$ given $p$ is: 

$${10 \choose 3} p^3 (1-p)^7$$

. . . 

What value of $p$ *maximizes* this likelihood? 

## Methods of maximizing

1. Approximate the solution graphically 

2. Find a numerical approximation

3. Use calculus

## Approximating with a graph

```{r}
#| echo: false

p1 <- tibble(
  p = seq(0, 1, by = .01),
  prob_p = choose(10,3)*p^3*(1-p)^7
) |>
  ggplot(aes(x = p, y = prob_p)) + 
  geom_line() + 
  geom_vline(xintercept = .3, linetype = "dashed", color = "darkred") 

plotly::ggplotly(p1)
```

## Approximating with a graph: code

```{r}
#| code-line-numbers: "1|2|3|4"
#| output-location: fragment
germ_function <- function(p) {choose(10,3) * p^3 * (1-p)^7} # define function
ggplot() + 
  geom_function(fun = germ_function) +
  xlim(c(0,1))
```

## Approximating numerically 

```{r}
#| output-location: fragment
optimize(germ_function, interval = c(0,1), maximum = TRUE)
```

## With calculus

## A "trick" 

Probability functions often are tricky to differentiate because of the product rule. Lucky for us, we can maximize the log instead. 

. . . 

```{r}
#| echo: false


p2 <- tibble(
  p = seq(0, 1, by = .01),
  prob_p = choose(10,3)*p^3*(1-p)^7
) |>
  ggplot(aes(x = p, y = log(prob_p))) + 
  geom_line() + 
  geom_vline(xintercept = .3, linetype = "dashed", color = "darkred") 

subplot(ggplotly(p1), ggplotly(p2), nrows=1)
```

## Can we generalize to *any* data? 

# Maximum Likelihood Estimation

## 

::: callout-note
## Likelihood function

Let $f(x; \theta)$ denote the probability mass function for a discrete distribution with associated parameter $\theta$. Suppose $X_1, X_2, ... , X_n$ are a random sample from this distribution and $x_1, x_2, ... , x_n$ are the actual observed values. Then, the *likelihood function* of $\theta$ is:

<br>
<br>
<br>
:::

. . . 

::: callout-note
## Maximum likelihood estimate

A maximum likelihood estimate (MLE), $\hat{\theta}_{MLE}$ is the value of $\theta$ that maximizes the likelihood function, or equivalently, that maximizes the log-likelihood $\ln L(\theta)$

:::

## Bernoulli data

An alternative way to express this example is $X_1, ..., X_n \sim \text{Bernoulli}(\theta)$

## Is the likelihood also a PDF?

```{r}
#| echo: false


countdown::countdown(2)
```

## Aside: notation

::: callout-note
## Estimator

<br>
<br>
<br>
:::

::: callout-note
## Estimate

<br>
<br>
<br>
:::

## 

Let $X_1, ..., X_n$ be an iid random sample from a distribution with PDF 

$$f(x|\theta)= (\theta + 1)x^\theta, 0 \le x \le 1$$

::: nonincremental
1. Find the maximum likelihood estimator for a random sample of size $n$ using calculus. 
2. Suppose we observe a sample of size 5: {.83, .49, .72, .57, .66}. Find the maximum likelihood estimate and verify with a graph or numerical approximation
:::
