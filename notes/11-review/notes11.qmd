---
title: "11: Exam 1 Review"
author: "Prof Amanda Luby"
subtitle: "Stat250 S25"
callout-appearance: minimal
knitr:
    opts_chunk: 
      dev: "ragg_png"
      echo: false
      warning: false
      message: false
format:
  pdf:
    include-in-header: 
       - "../preamble.tex"
    toc: false
    number-sections: true
    colorlinks: true
    geometry:
      - top=1in
      - left=1in
      - right=1in
      - bottom=1in
      - heightrounded
    mainfont: "Linux Libertine"
    sansfont: "Linux Libertine"
    mathfont: "Libertinus Math"
    monofont: "Monaco"
fontsize: 11pt
---
\vspace{-.5in}
# General Topics

### Terminology {-}

\vspace{-.25in}

- Parameter vs Estimator
- Estimator vs Estimate
- pdf vs likelihood
- sample vs population



### Sampling distributions {-}

\vspace{-.25in}

- Conceptual understanding of what they are
- Central Limit Theorem for sample means
- Relationship to estimator



### Estimation {-}

\vspace{-.25in}

- Finding the MLE
- Finding the MoM



### Evaluating estimators {-}

\vspace{-.25in}

- Given a pdf, find the expected value and variance
- Recognize named distributions and use expected value and variance from cheat sheet
- Use properties of expected value and variance
- Comment on bias and efficiency of an estimator: 
    - using the theoretical expected value and variance
    - using results from a simulation
- Determine the Cramer-Rao lower bound for a probability distribution
- Compare two estimators' efficiency in terms of sample size
- Determine if an estimator is consistent 
- Use large-sample properties of the MLE

### Math techniques {-}

\vspace{-.25in}
- Integration: polynomials, $e^x$, $\ln x$, and less-involved composite functions (ie $\int \ln 2x$)
- Derivatives: same as integration
\vspace{-.25in}

### R techniques {-}
\vspace{-.25in}

- Given simulation code and output, draw conclusions
- Given a graph and a description, draw conclusions


# Example problems

*Note:* this is not an exhaustive problem list, nor is it representative of the length of the exam. Make sure to review daily prep questions, in-class examples and exercises, and homework problems.

Let $X_1, ..., X_n$ be an iid sample from a distribution with pdf $f(x|\theta) = \theta x^{\theta-1}$ for $0 < x < 1$ and $\theta >0$. 

1. Show that the likelihood function is $\theta^n \prod X_i^{\theta-1}$
2. Show that the maximum likelihood estimator is $\hat{\theta} = \frac{-n}{\sum \ln X_i}$
3. It can be shown that $E[\hat\theta_{MLE}] = \frac{n}{n-1} \theta$. Use this information to construct an unbiased estimator for $\theta$, $\hat\theta_2$, that is based on $\hat \theta_{MLE}$
4. Find the method of moments estimator for $\theta$
5. Find $I(\theta)$
6. Find the Cramer-Rao Lower Bound
7. What does the Cramer-Rao Lower Bound in (6) tell you about $Var(\hat\theta_{MLE})$? What about  $Var(\hat\theta_{2})$? 
8. Below is a simulation comparing the performance of $\hat{\theta_{MLE}}$ to $\hat{\theta_{MoM}}$. Which output corresponds to the bias of each estimator? Which corresponds to the efficiency? 

```{r}
#| echo: true
theta <- 2
n <- 20
n_sims <- 10000
theta_mle <- numeric(n_sims)
theta_mom <- numeric(n_sims)

for(i in 1:n_sims){
  sample <- rbeta(n, theta, 1)
  theta_mle[i] <- -n/sum(log(sample))
  theta_mom[i] <- mean(sample)/(1-mean(sample))
}

mean(theta_mle)
var(theta_mle)
mean(theta_mom)
var(theta_mom)
```

Are the following statements true? If not, can you correct them?

1. The variance of an estimator quantifies how close the estimator is expected to be to the true parameter value on average.

2. The CramÃ©r-Rao Lower Bound provides an upper limit on the variance that any unbiased estimator can achieve.

3. For a given statistical model, the Method of Moments estimator is generally more statistically efficient than the Maximum Likelihood Estimator, especially for large sample sizes.

4. If an estimator is biased, it cannot be a consistent estimator.

5. Efficiency is primarily concerned with minimizing the bias of an estimator.

6. A biased estimator can sometimes have a smaller Mean Squared Error (MSE) than an unbiased estimator.

Use the graphs below for the next few questions: 

(A) Sketch a sampling distribution for an estimator that is **unbiased**
(B) Sketch a sampling distribution for an estimator that is **more efficient** than the one that I sketched
(C) Sketch a sampling distribution for an estimator with **lower MSE** than the one that I sketched

```{r}
#| fig-width: 6
#| fig-height: 2
library(tidyverse)
library(patchwork)
p1 <- ggplot() +
  ylim(c(0,1)) + 
  geom_vline(xintercept = .5) + 
  annotate("text", x = .65, y = 0, label = expression(theta)) +
  labs(
    x = "",
    y = ""
  ) +
  coord_cartesian(clip = "off", xlim = c(-2,2)) + 
  theme_bw() +
  theme(axis.text = element_text(color = "white"))

p2 <- ggplot() +
  ylim(c(0,1)) + 
  geom_vline(xintercept = .5) + 
  annotate("text", x = .65, y = 0, label = expression(theta)) +
  labs(
    x = "",
    y = ""
  ) +
  coord_cartesian(clip = "off", xlim = c(-2,2)) + 
  theme_bw() +
  theme(axis.text = element_text(color = "white"))

p3 <- ggplot() +
  ylim(c(0,1)) + 
  geom_vline(xintercept = .5) + 
  annotate("text", x = .65, y = 0, label = expression(theta)) +
  labs(
    x = "",
    y = ""
  ) +
  coord_cartesian(clip = "off", xlim = c(-2,2)) + 
  theme_bw() +
  theme(axis.text = element_text(color = "white"))

p1 + p2 + p3 + plot_annotation(tag_levels = "A")
```


