---
title: "Stat250 Exam I Topics"
format: html
---

# General Topics

## Terminology 

- Parameter vs Estimator
- Estimator vs Estimate
- pdf vs likelihood
- sample vs population

## Sampling distributions


## Estimation

- Finding the MLE
- Finding the MoM

## Evaluating estimators

- Given a pdf, find the expected value and variance
- Recognize named distributions
- Use properties of expected value and variance
- Comment on bias and efficiency using the theoretical expected value and variance or using results from a simulation
- Determine the Cramer-Rao lower bound
- Compare two estimators' efficiency in terms of sample size
- Determine if an estimator is consistent 
- Use large-sample properties of the MLE

## Math skills

- Integration: polynomials, $e^x$, $\ln x$, and less-involved composite functions (ie $\int \ln 2x$)
- Derivatives: same as integration

## R skills

- Given simulation code and output, draw conclusions
- Given a graph and a description, draw conclusions

# Example problems

*Note:* this is not an exhaustive problem list, nor is it representative of the length of the exam. Make sure to review daily prep questions, in-class examples and exercises, and homework problems.

Let $X_1, ..., X_n$ be an iid sample from a distribution with pdf $f(x|\theta) = \theta x^{\theta-1}$ for $0 < x < 1$ and $\theta >0$. 

1. Show that the likelihood function is $\theta^n \prod X_i^{\theta-1}$
2. Show that the maximum likelihood estimator is $\hat{\theta} = \frac{-n}{\sum \ln X_i}$
3. It can be shown that $E[\hat\theta_{MLE}] = \frac{n}{n-1} \theta$. Use this information to construct an unbiased estimator for $\theta$, $\hat\theta_2$, that is based on $\hat \theta_{MLE}$
4. Find the method of moments estimator for $\theta$
5. Find $I(\theta)$
6. Find the Cramer-Rao Lower Bound


T/F questions

1. The variance of an estimator quantifies how close the estimator is expected to be to the true parameter value on average.

2. The Cram√©r-Rao Lower Bound provides an upper limit on the variance that any unbiased estimator can achieve.

3. For a given statistical model, the Method of Moments estimator is generally more statistically efficient than the Maximum Likelihood Estimator, especially for large sample sizes.

4. If an estimator is biased, it cannot be a consistent estimator.

5. Efficiency is primarily concerned with minimizing the bias of an estimator.

6. A biased estimator can sometimes have a smaller Mean Squared Error (MSE) than an unbiased estimator.
