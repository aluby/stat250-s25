---
title: "08: Bias and Efficiency"
author: "Prof Amanda Luby"
subtitle: "Stat250 S25"
callout-appearance: minimal
knitr:
    opts_chunk: 
      dev: "ragg_png"
      echo: false
      warning: false
      message: false
format:
  pdf:
    include-in-header: 
       - "../preamble.tex"
    toc: false
    number-sections: true
    colorlinks: true
    geometry:
      - top=1in
      - left=1in
      - right=1in
      - bottom=1in
      - heightrounded
    mainfont: "Linux Libertine"
    sansfont: "Linux Libertine"
    mathfont: "Libertinus Math"
    monofont: "Monaco"
fontsize: 11pt
---


```{r, echo = FALSE}
library(tidyverse)
library(patchwork)
library(Sleuth3)
library(mosaic)
library(infer)
```

# Recap

- Observe data $X_1, ..., X_n \sim F_x(x|\theta)$, where $\theta$ is unknown

- Goal: *Estimate* $\theta$ based on the values of $X_i$ by formulating an *estimator* $\widehat \theta$

- One technique is to use the *maximum likelihood estimator*, which finds the value of $\theta$ that maximizes the joint probability $\prod_{i=1}^n f_x(x_i | \theta)$

- A second technique is to use the *method of moments estimator*, which finds the value of $\theta$ that make the theoretical moments equal to the sample moments

- Today: if we have multiple estimators, how do we decide which is better?

# Example: exponential distribution

**Warm up:** Suppose we take a random sample of size 50 from an exponential distribution with mean 10 (rate is $\lambda = 1/10$). What is $\mu = E(Y)$? If we want to design an estimator for the mean, $\widehat \mu$, what are some intuitive estimators? 

\vspace{1in}

**Example:** Suppose we take a random sample of size 50 from an exponential distribution with mean 10 (rate is $\lambda = 1/10$). Consider three estimators of $\mu$: 

$$\hat{\mu_1} =$$
$$\hat{\mu_2} =$$
$$\hat{\mu_3} =$$

::: callout-note
*Aside:* estimators are also random variables. $X_1, ..., X_n \sim F_x(\theta)$ and each $\hat{\theta} = g(X_1, ..., X_n)$ is a function of the data, so each $\hat{\theta}$ is itself a random variable and has: 
\vspace{.75in}
:::


# Comparing estimators: simulation

:::: {layout-ncol="2"}
::: {}
```
n <- 50
N_sims <- 100000
theta <- 3
est1 <- numeric(N_sims)
est2 <- numeric(N_sims)
est3 <- numeric(N_sims)
for(i in 1:N_sims){
  x <- rexp(n, rate = .1)
  est1[i] <- mean(x)
  est2[i] <- median(x)
  est3[i] <- (max(x) - min(x))/2
}
```
:::

::: {}
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 2

n <- 50
N_sims <- 100000
theta <- 3
est1 <- numeric(N_sims)
est2 <- numeric(N_sims)
est3 <- numeric(N_sims)
for(i in 1:N_sims){
  x <- rexp(n, rate = .1)
  est1[i] <- mean(x)
  est2[i] <- median(x)
  est3[i] <- (max(x) - min(x))/2
}

sims <- tibble(
  mean = est1,
  median = est2,
  midpoint = est3
) |>
  pivot_longer(everything(), names_to = "estimator", values_to = "value") 

sims |>
  ggplot(aes(x = value, col = estimator, fill = estimator)) + 
  geom_density(alpha = .3) + 
  scale_color_viridis_d(end = .8, option = "plasma") + 
  scale_fill_viridis_d(end = .8, option = "plasma") + 
  labs(
    x = "Estimator Values",
    y = "Density",
    col = "",
    fill = ""
  ) + 
  xlim(c(0, 40)) +
  theme_bw() +
  theme(legend.position = "bottom")
```
:::
::::

# Properties of Estimators

![](../img/target-bias-variance-transparent.png){width="30%"}

::: callout-note
## Bias of an estimator

If $\widehat{\theta}(X)$ is an estimator of $\theta$, then the bias of the estimator is equal to

\vspace{.5in}

*Note:* the expected value is computed from the sampling distribution of $\widehat{\theta}(X)$
:::

::: callout-note
## Variance of an estimator

The variance of an estimator is 

\vspace{.5in}

:::


::: callout-note
## MSE

The MSE of an estimator is 

\vspace{.5in}

:::

**Exercise:** Your task is to compare the estimators 

$$\widehat{\theta}_{MLE} = X_{\max} \hspace{1in} \widehat{\theta}_{MoM} = 2\bar{X}$$ 

(a) What is the bias of each estimator? (A helpful fact is that $f_{X_{max}}(x) = n[F(x)]^{n-1}f_X(x)$)
(b) What is the variance of each estimator?
(c) What is the MSE of each estimator?
(d) When does $\widehat{\theta}_{MLE}$ "beat"  $\widehat{\theta}_{MoM}$ in terms of MSE?

\pagebreak

\vspace{4in}

::: callout-note
## Efficiency

For two unbiased estimators, $\widehat \theta_1$ is more **efficient** than $\widehat \theta_2$ if 

\vspace{.5in}
:::

(e) Since $\hat \theta_{MLE}$ beats $\hat \theta_{MoM}$ in terms of MSE but is biased, can we "unbias" the MLE? Call this third estimator $\hat\theta_3$

(f) Does $\hat \theta_3$ ever "beat" $\hat \theta_{MLE}$ in terms of MSE?

\vspace{4in}

# Comparing unbiased estimators 

::: callout-note
## Cramer-Rao Lower Bound

If $X_1, ..., X_n$ are an iid sample from a distribution with pdf $f(x|\theta)$, then any unbiased estimator $\hat\theta$ of $\theta$ satisfies: 

\vspace{1in}

where $I(\theta)$ is the **Fisher Information** of $X_i$
:::



::: callout-note
## Fisher Information

The **Fisher Information** of an observation $X$ is
$$I(\theta) = E[(\frac{d}{d\theta} \ln f(x|\theta))^2] =$$
:::


(f) Does $\hat{\theta}_3$ meet the CRLB? 



