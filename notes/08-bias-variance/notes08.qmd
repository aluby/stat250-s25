---
title: "08: Bias and Efficiency"
author: "Prof Amanda Luby"
subtitle: "Stat250 S25"
callout-appearance: minimal
knitr:
    opts_chunk: 
      dev: "ragg_png"
      echo: false
      warning: false
      message: false
format:
  pdf:
    include-in-header: 
       - "../preamble.tex"
    toc: false
    number-sections: true
    colorlinks: true
    geometry:
      - top=1in
      - left=1in
      - right=1in
      - bottom=1in
      - heightrounded
fontfamily: libertine
fontsize: 11pt
---


```{r, echo = FALSE}
library(tidyverse)
library(patchwork)
library(Sleuth3)
library(mosaic)
library(infer)
```

We've seen two methods of estimating parameters: the MLE and the MoM. Both give reasonable criteria to identify estimators for unknown parameters, but they do not always yield the same answer. 

For example, on your homework, you showed that the MLE estimator for $\theta$ in a (continuous) Unif($0, \theta)$ distribution is $\hat{\theta} = 2\bar{Y}$. 

**Example:** Find the MLE for $Y_1, Y_2, ..., Y_n \sim \text{Unif}(0, \theta)$. (Recall $f_y(y) = \frac{1}{\theta}, 0 \le y \le \theta)$)

\vspace{2.5in} 

Implicit in the two estimators for the same parameters is the obvious question: which one should we use? 

There are actually an infinite number of estimators for any given parameter, and this requires that we have a principled way of evaluating the statistical properties associated with any given estimator. What qualities should a "good" estimator have? Is it possible to find a "best" $\hat{\theta}$? These are the questions we're going to begin to answer.

**Note:** Every estimator is a function of a set of random variables (ie $\hat{\theta} = g(Y_1, Y_2, ..., Y_n))$) and is itself a random variable. 

\vspace{.5in} 

::: callout-note
## Notation for $\hat{\theta}$
\vspace{1in}
:::

# Bias

*demo* w figures

Ideally, we want the overestimates to "balance out" the underestimates: $\hat{\theta}$ should not systematically err in either direction. 

```{r}
#| fig-height: 3
#| fig-width: 6
p1 <- ggplot() +
  xlim(c(-3,3)) + 
  ylim(c(0,1)) + 
  theme_bw() + 
  theme(axis.text = element_blank()) 
   

p1 + p1
```

\vspace{3in}

When the mean of the estimator $\hat{\theta}$ is equal to the true parameter $\theta$, we say that the estimator is **unbiased**. 

:::callout-note
## Definition 

Let $W_1, W_2, ..., W_n$ be a random sample from $f_w(w, \theta)$. An estimator $\hat{\theta} = g(W_1, W_2, ..., W_n)$ is said to be **unbiased for ** $\theta$ if

:::

**Example:** is the MoM for the Unif$(0, \theta)$ distribution, $\hat{\theta}_1 = 2\bar{X}$ unbiased? 

\vspace{2in}

**Example:** is the MLE $\hat{\theta}_2 = \text{max}({X_i})$ unbiased? (*Hint:* $f_{X_{max}}(x) = n[F(x)]^{n-1}f_X(x)$)

\vspace{4in} 

**Example:** Construct an estimator, $\hat{\theta}_3$ based on $\text{max}({X_i})$ that is unbiased. 

\vspace{2in}

# Efficiency

We now have two estimators, $\hat{\theta}_1$ and $\hat{\theta}_3$, that are unbiased estimators for $\theta$. Does it matter which one we choose? 

**Idea:** 

*add bullseye image*

\vspace{3in} 

We could answer this rigorously by finding Var($\hat \theta_1$) and Var($\hat{\theta_3}$). Let's explore with simulation: 

```{r}
n <- 20
N_sims <- 10000
theta <- 3
est1 <- numeric(N_sims)
est2 <- numeric(N_sims)
est3 <- numeric(N_sims)
for(i in 1:N_sims){
  x <- runif(n, min = 0, max = theta)
  est1[i] <- max(x)
  est2[i] <- 2*mean(x)
  est3[i] <- ((n+1)/n)*max(x)
}
```

```{r}
#| echo: false
tibble(
  `estimator 1` = est1,
  `estimator 2` = est2,
  `estimator 3` = est3
) |>
  pivot_longer(everything()) |>
  ggplot(aes(x = value, col = name)) + 
  geom_density() +
  theme_bw()
```

:::callout-note
## Definition 

Let $\hat{\theta}_1$ and $\hat{\theta}_2$ be two unbiased estimators for a parameter $\theta$. If $\text{Var}(\hat{\theta}_1) < \text{Var}(\hat{\theta}_2)$, we say that 
\vspace{.5in}
:::

# The Bias-Variance Tradeoff

:::callout-note
## Mean Square Error (MSE) 

$$MSE(\hat{\theta}) = E(\hat{\theta} - \theta)^2 = V(\hat{\theta}) + \text{bias}^2$$
:::
*Note:* We'll come back to this, and show that it is true, later in the course

**Idea:** 




